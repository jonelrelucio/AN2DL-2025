{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 100\n",
    "\n",
    "# Cross-validation [NO AFFECT, this notebook removed k-folding cross validation]\n",
    "K = 5                    # Number of splits (5 and 10 are considered good values)\n",
    "N_VAL_USERS = 5          # Number of users for validation split\n",
    "N_TEST_USERS = 5         # Number of users for test split\n",
    "\n",
    "# Training\n",
    "EPOCHS = 500             # Maximum epochs (increase to improve performance)\n",
    "PATIENCE = 50            # Early stopping patience (increase to improve performance)\n",
    "VERBOSE = 1              # Print frequency\n",
    "\n",
    "# Optimisation\n",
    "LEARNING_RATE = 0.0001   # Learning rate\n",
    "BATCH_SIZE = 512         # Batch size\n",
    "WINDOW_SIZE = 200        # Input window size\n",
    "STRIDE = 50              # Input stride\n",
    "\n",
    "# Architecture\n",
    "HIDDEN_LAYERS = 2        # Hidden layers\n",
    "HIDDEN_SIZE = 128        # Neurons per layer\n",
    "RNN_TYPE = 'LSTM'        # Type of RNN architecture ['RNN', 'LSTM', 'GRU']\n",
    "BIDIRECTIONAL = False    # Bidirectional RNN\n",
    "\n",
    "# Regularisation\n",
    "NOISE_STD_DEV = 0.03     # Data Augmentation: Added noise on training dataset\n",
    "DROPOUT_RATE = 0.3       # Dropout probability\n",
    "L1_LAMBDA = 0            # L1 penalty\n",
    "L2_LAMBDA = 0.01         # L2 penalty\n",
    "\n",
    "import torch.nn as nn\n",
    "CRITERION = nn.CrossEntropyLoss() # Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9154e1ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T22:13:54.593453Z",
     "start_time": "2025-11-06T22:13:54.589401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "72f73dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T22:13:55.305869Z",
     "start_time": "2025-11-06T22:13:54.614697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>joint_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499558e-06</td>\n",
       "      <td>1.945042e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.153299e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>...</td>\n",
       "      <td>3.976952e-07</td>\n",
       "      <td>6.765108e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4.643774e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.533820e-07</td>\n",
       "      <td>1.698525e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.424536e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006865e-05</td>\n",
       "      <td>5.511079e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5.432416e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>...</td>\n",
       "      <td>4.437265e-06</td>\n",
       "      <td>1.735459e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5.825366e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0              2              0              2   \n",
       "1             0     1              2              2              2   \n",
       "2             0     2              2              0              2   \n",
       "3             0     3              2              2              2   \n",
       "4             0     4              2              2              2   \n",
       "\n",
       "   pain_survey_4 n_legs n_hands n_eyes  joint_00  ...      joint_21  \\\n",
       "0              1    two     two    two  1.094705  ...  3.499558e-06   \n",
       "1              2    two     two    two  1.135183  ...  3.976952e-07   \n",
       "2              2    two     two    two  1.080745  ...  1.533820e-07   \n",
       "3              2    two     two    two  0.938017  ...  1.006865e-05   \n",
       "4              2    two     two    two  1.090185  ...  4.437265e-06   \n",
       "\n",
       "       joint_22  joint_23      joint_24  joint_25  joint_26  joint_27  \\\n",
       "0  1.945042e-06  0.000004  1.153299e-05  0.000004  0.017592  0.013508   \n",
       "1  6.765108e-07  0.000006  4.643774e-08  0.000000  0.013352  0.000000   \n",
       "2  1.698525e-07  0.000001  2.424536e-06  0.000003  0.016225  0.008110   \n",
       "3  5.511079e-07  0.000002  5.432416e-08  0.000000  0.011832  0.007450   \n",
       "4  1.735459e-07  0.000002  5.825366e-08  0.000007  0.005360  0.002532   \n",
       "\n",
       "   joint_28  joint_29  joint_30  \n",
       "0  0.026798  0.027815       0.5  \n",
       "1  0.013377  0.013716       0.5  \n",
       "2  0.024097  0.023105       0.5  \n",
       "3  0.028613  0.024648       0.5  \n",
       "4  0.033026  0.025328       0.5  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('pirate_pain_train.csv')\n",
    "\n",
    "float_cols = df_train.select_dtypes(include=['float64']).columns\n",
    "for col in float_cols:\n",
    "    df_train[col] = df_train[col].astype(np.float32)\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dbcbc38e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T22:13:55.331343Z",
     "start_time": "2025-11-06T22:13:55.322920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of timestamps per sample_index: 160.00\n",
      "     sample_index  n_timestamps\n",
      "0               0           160\n",
      "1               1           160\n",
      "2               2           160\n",
      "3               3           160\n",
      "4               4           160\n",
      "..            ...           ...\n",
      "656           656           160\n",
      "657           657           160\n",
      "658           658           160\n",
      "659           659           160\n",
      "660           660           160\n",
      "\n",
      "[661 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "counts = df_train.groupby('sample_index')['time'].count().reset_index(name='n_timestamps')\n",
    "counts = counts.sort_values('sample_index')  \n",
    "\n",
    "avg_ts = counts['n_timestamps'].mean()\n",
    "print(f\"Average number of timestamps per sample_index: {avg_ts:.2f}\")\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fba3eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def build_fixed_sequences(df_features, df_labels, feature_cols, joint_cols, label_map, \n",
    "                          window=200, stride=50):\n",
    "    \"\"\"\n",
    "    Builds fixed-size, overlapping sequences from the dataset.\n",
    "    This function normalizes each sample individually before windowing.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    n_features = len(feature_cols)\n",
    "\n",
    "    print(f\"Building fixed sequences with window={window}, stride={stride}...\")\n",
    "\n",
    "    # Iterate over unique sample indices\n",
    "    for sid in df_features['sample_index'].unique():\n",
    "        \n",
    "        # 1. Get features\n",
    "        temp_features = df_features[df_features['sample_index'] == sid].sort_values('time')[feature_cols].values\n",
    "        \n",
    "        # 2. Get label\n",
    "        label_str = df_labels[df_labels['sample_index'] == sid]['label'].values[0]\n",
    "        label_numeric = label_map[label_str]\n",
    "\n",
    "        # 3. Normalize\n",
    "        temp_features_tensor = torch.FloatTensor(temp_features)\n",
    "        joint_data = temp_features_tensor[:, :len(joint_cols)]\n",
    "        joint_min = joint_data.min(dim=0)[0]\n",
    "        joint_max = joint_data.max(dim=0)[0]\n",
    "        joint_range = joint_max - joint_min\n",
    "        joint_range[joint_range == 0] = 1\n",
    "        normalized_joints = 2 * ((joint_data - joint_min) / joint_range) - 1\n",
    "        temp_features_tensor[:, :len(joint_cols)] = normalized_joints\n",
    "        temp_features_normalized = temp_features_tensor.numpy()\n",
    "\n",
    "        # 4. Apply padding (from your original logic)\n",
    "        padding_len = window - len(temp_features_normalized) % window\n",
    "        \n",
    "        # --- BUG FIX: Remove the 'if padding_len == window' line ---\n",
    "        # This was the bug. The original logic was correct.\n",
    "        \n",
    "        padding = np.zeros((padding_len, n_features), dtype='float32')\n",
    "        temp_features_padded = np.concatenate((temp_features_normalized, padding))\n",
    "\n",
    "        # 5. Apply windowing logic\n",
    "        idx = 0\n",
    "        while idx + window <= len(temp_features_padded):\n",
    "            dataset.append(temp_features_padded[idx:idx + window])\n",
    "            labels.append(label_numeric)\n",
    "            idx += stride\n",
    "\n",
    "    dataset = np.array(dataset, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fa507812",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv('pirate_pain_train_labels.csv')\n",
    "df_labels.head()\n",
    "\n",
    "df_joint_00 = df_train[['sample_index', 'time', 'joint_00']].copy()\n",
    "df_joint_00 = df_joint_00.merge(df_labels, on='sample_index', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b0742322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: 35\n",
      "Label map: {'high_pain': 0, 'low_pain': 1, 'no_pain': 2}\n",
      "Building fixed sequences with window=200, stride=50...\n",
      "Original samples: 661\n",
      "Total windows created: 661\n",
      "Shape of X: (661, 200, 35)\n",
      "Shape of y: (661,)\n",
      "\n",
      "Train shapes: X=(528, 200, 35), y=(528,)\n",
      "Val shapes  : X=(133, 200, 35), y=(133,)\n",
      "Noise level (std): 0.03\n",
      "Original X_train[0] first joint, first 5 steps:\n",
      " [-0.09461689  0.01702583  0.6403575  -0.35634392 -0.44900668]\n",
      "\n",
      "Noisy batch[0] first joint, first 5 steps:\n",
      " tensor([-0.1291, -0.1497, -0.4234,  0.0291,  0.1719])\n",
      "Original samples: 661\n",
      "Total windows created: 661\n",
      "Shape of X: (661, 200, 35)\n",
      "Shape of y: (661,)\n",
      "\n",
      "Train shapes: X=(528, 200, 35), y=(528,)\n",
      "Val shapes  : X=(133, 200, 35), y=(133,)\n",
      "Noise level (std): 0.03\n",
      "Original X_train[0] first joint, first 5 steps:\n",
      " [-0.09461689  0.01702583  0.6403575  -0.35634392 -0.44900668]\n",
      "\n",
      "Noisy batch[0] first joint, first 5 steps:\n",
      " tensor([-0.1291, -0.1497, -0.4234,  0.0291,  0.1719])\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- 1. Define feature columns and label map ---\n",
    "joint_cols = [c for c in df_train.columns if c.startswith('joint_')]\n",
    "pain_cols = [c for c in df_train.columns if c.startswith('pain_survey_')]\n",
    "feature_cols = joint_cols + pain_cols\n",
    "input_size = len(feature_cols)\n",
    "\n",
    "# Create label map\n",
    "unique_labels = sorted(df_labels['label'].unique())\n",
    "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "num_classes = len(label_map)\n",
    "\n",
    "print(f\"Feature columns: {input_size}\")\n",
    "print(f\"Label map: {label_map}\")\n",
    "\n",
    "# --- 2. Build all sequences ---\n",
    "# We use the constants defined in your notebook (WINDOW_SIZE=200, STRIDE=50)\n",
    "X_all_windows, y_all_windows = build_fixed_sequences(\n",
    "    df_train, df_labels, feature_cols, joint_cols, label_map,\n",
    "    window=WINDOW_SIZE,\n",
    "    stride=STRIDE\n",
    ")\n",
    "\n",
    "print(f\"Original samples: {len(df_labels)}\")\n",
    "print(f\"Total windows created: {len(X_all_windows)}\")\n",
    "print(f\"Shape of X: {X_all_windows.shape}\") # (num_windows, WINDOW_SIZE, num_features)\n",
    "print(f\"Shape of y: {y_all_windows.shape}\") # (num_windows,)\n",
    "\n",
    "# --- 3. Split into train/validation sets ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_all_windows,\n",
    "    y_all_windows,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y_all_windows  # Stratify on the new window labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Val shapes  : X={X_val.shape}, y={y_val.shape}\")\n",
    "\n",
    "# %%\n",
    "# --- 4. Create new, flexible Dataset ---\n",
    "# This dataset can now add noise *only* during training\n",
    "\n",
    "class PainDataset(Dataset):\n",
    "    def __init__(self, X, y, is_train=False, n_joint_cols=0, noise_level=0.01):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        # --- New parameters for noise ---\n",
    "        self.is_train = is_train\n",
    "        self.n_joint_cols = n_joint_cols\n",
    "        self.noise_level = noise_level\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Use .clone() to ensure we don't modify the original, clean data\n",
    "        x_sample = self.X[idx].clone() \n",
    "        y_sample = self.y[idx]\n",
    "        \n",
    "        # --- Add noise *only* if it's the training set ---\n",
    "        if self.is_train and self.noise_level > 0:\n",
    "            # 1. Create noise\n",
    "            # Shape: (window_size, num_joint_features)\n",
    "            noise = torch.randn(x_sample.shape[0], self.n_joint_cols) * self.noise_level\n",
    "            \n",
    "            # 2. Add noise ONLY to the joint columns\n",
    "            x_sample[:, :self.n_joint_cols] = x_sample[:, :self.n_joint_cols] + noise.to(x_sample.device)\n",
    "            \n",
    "        return x_sample, y_sample\n",
    "\n",
    "# --- 5. Create new DataLoaders ---\n",
    "\n",
    "# Get the number of joint columns\n",
    "n_joint_cols = len(joint_cols) \n",
    "\n",
    "# Create the TRAINING dataset *with noise enabled*\n",
    "train_dataset = PainDataset(\n",
    "    X_train, y_train, \n",
    "    is_train=True, \n",
    "    n_joint_cols=n_joint_cols, \n",
    "    noise_level=NOISE_STD_DEV\n",
    ")\n",
    "\n",
    "# Create the VALIDATION dataset *with noise disabled*\n",
    "# This is critical! You want a stable, clean metric for validation.\n",
    "val_dataset = PainDataset(\n",
    "    X_val, y_val, \n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Check a batch to see the effect\n",
    "print(f\"Noise level (std): {NOISE_STD_DEV}\")\n",
    "print(f\"Original X_train[0] first joint, first 5 steps:\\n {X_train[0, :5, 0]}\")\n",
    "\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"\\nNoisy batch[0] first joint, first 5 steps:\\n {x_batch[0, :5, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bfb5ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers.\n",
    "\n",
    "    This function is designed for models whose direct children are\n",
    "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to analyze.\n",
    "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store output shapes captured by forward hooks\n",
    "    output_shapes = {}\n",
    "    # List to track hook handles for later removal\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # Handle RNN layer outputs (returns a tuple)\n",
    "            if isinstance(output, tuple):\n",
    "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "\n",
    "                # Replace batch dimension (middle position) with -1\n",
    "                shape2[1] = -1\n",
    "\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "\n",
    "            # Handle standard layer outputs (e.g., Linear)\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1  # Replace batch dimension with -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    # 1. Determine the device where model parameters reside\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
    "\n",
    "    # 2. Create a dummy input tensor with batch_size=1\n",
    "    dummy_input = torch.randn(1, *input_size).to(device)\n",
    "\n",
    "    # 3. Register forward hooks on target layers\n",
    "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            # Register the hook and store its handle for cleanup\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    # 4. Execute a dummy forward pass in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            # Clean up hooks even if an error occurs\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    # 5. Remove all registered hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # --- 6. Print the summary table ---\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    # Column headers\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    # Iterate through modules again to collect and display parameter information\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            # Count total and trainable parameters for this module\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            # Format strings for display\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9732cb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Layer (type)              Output Shape                 Param #           \n",
      "===============================================================================\n",
      "rnn (RNN)                 [[-1, 160, 128], [2, -1, 128]] 54,144         \n",
      "classifier (Linear)       [-1, 3]                      387            \n",
      "===============================================================================\n",
      "Total params: 54,531\n",
      "Trainable params: 54,531\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU).\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(input_size)\n",
    "\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Map string name to PyTorch RNN class\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        # Create the recurrent layer\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        # Calculate input size for the final classifier\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_length, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.batch_norm(x.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        # LSTM returns (h_n, c_n), we only need h_n\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Reshape to (num_layers, 2, batch_size, hidden_size)\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "\n",
    "            # Concat last fwd (hidden[-1, 0, ...]) and bwd (hidden[-1, 1, ...])\n",
    "            # Final shape: (batch_size, hidden_size * 2)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            # Take the last layer's hidden state\n",
    "            # Final shape: (batch_size, hidden_size)\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        # Get logits\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Get a sample sequence to determine max sequence length\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Create model and display architecture with parameter count\n",
    "input_size = len(feature_cols)  # Number of features (joints + pain surveys)\n",
    "num_classes = len(label_map)    # Number of unique labels\n",
    "\n",
    "# Create model with correct input size\n",
    "rnn_model = RecurrentClassifier(\n",
    "    input_size=input_size,      # Number of input features\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=0.,\n",
    "    rnn_type='RNN'\n",
    "    ).to(device)\n",
    "\n",
    "# Call recurrent_summary with correct input shape\n",
    "recurrent_summary(rnn_model, input_size=(max_seq_len, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d74d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e044d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    \"\"\"\n",
    "    Perform one complete training epoch through the entire training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): Lambda for L1 regularization\n",
    "        l2_lambda (float): Lambda for L2 regularization\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Clear gradients from previous step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision (if CUDA available)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "\n",
    "            # Check loss for NaN\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss at batch {batch_idx}\")\n",
    "                print(f\"Input range: [{inputs.min():.4f}, {inputs.max():.4f}]\")\n",
    "                print(f\"Logits range: [{logits.min():.4f}, {logits.max():.4f}]\")\n",
    "                continue\n",
    "\n",
    "            # Add L1 and L2 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2e6f797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Perform one complete validation epoch through the entire validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        criterion (nn.Module): Loss function used to calculate validation loss\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
    "\n",
    "    Note:\n",
    "        This function automatically sets the model to evaluation mode and disables\n",
    "        gradient computation for efficiency during validation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            # Move data to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision (if CUDA available)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7dc87db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data and validate on the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
    "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
    "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
    "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
    "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
    "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
    "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
    "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
    "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, training_history) - Trained model and metrics history\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "\n",
    "    # Configure early stopping if patience is set\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    # Main training loop: iterate through epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Forward pass through training data, compute gradients, update weights\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        # Evaluate model on validation data without updating weights\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Store metrics for plotting and analysis\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        # Write metrics to TensorBoard for visualization\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        # Print progress every N epochs or on first epoch\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
    "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping logic: monitor metric and save best model\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    # Restore best model weights if early stopping was used\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    # Save final model if no early stopping\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "77e52526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from itertools import chain\n",
    "\n",
    "def k_fold_cross_validation(df_train, df_labels, epochs, criterion, device,\n",
    "                            k, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate,\n",
    "                            rnn_type, bidirectional, l1_lambda=0, l2_lambda=0, patience=0, \n",
    "                            evaluation_metric=\"val_f1\", mode='max',\n",
    "                            restore_best_weights=True, writer=None, verbose=10, seed=42, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Perform K-fold cross-validation for pain classification using time series data.\n",
    "    \"\"\"\n",
    "    # Initialize containers for results across all splits\n",
    "    fold_losses = {}\n",
    "    fold_metrics = {}\n",
    "    best_scores = {}\n",
    "\n",
    "    # Get feature columns\n",
    "    joint_cols = [c for c in df_train.columns if c.startswith('joint_')]\n",
    "    pain_cols = [c for c in df_train.columns if c.startswith('pain_survey_')]\n",
    "    feature_cols = joint_cols + pain_cols\n",
    "\n",
    "    # Create global label mapping to ensure consistency across folds\n",
    "    label_map = {label: i for i, label in enumerate(sorted(df_labels['label'].unique()))}\n",
    "    num_classes = len(label_map)\n",
    "\n",
    "    # Initialize model architecture\n",
    "    model = RecurrentClassifier(\n",
    "        input_size=len(feature_cols),\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=hidden_layers,\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=dropout_rate,\n",
    "        bidirectional=bidirectional,\n",
    "        rnn_type=rnn_type\n",
    "    ).to(device)\n",
    "\n",
    "    # Store initial weights to reset model for each split\n",
    "    initial_state = model.state_dict().copy()\n",
    "\n",
    "    # Get unique sample indices for splitting\n",
    "    unique_samples = df_train['sample_index'].unique()\n",
    "    np.random.seed(seed)\n",
    "    unique_samples_shuffled = np.random.permutation(unique_samples)\n",
    "\n",
    "    # Create k folds\n",
    "    fold_size = len(unique_samples) // k\n",
    "    folds = [unique_samples_shuffled[i:i + fold_size] for i in range(0, len(unique_samples_shuffled), fold_size)]\n",
    "\n",
    "    # Iterate through K folds\n",
    "    for fold_idx in range(k):\n",
    "        if verbose > 0:\n",
    "            print(f\"\\nFold {fold_idx+1}/{k}\")\n",
    "\n",
    "        # Use current fold as validation set, rest as training set\n",
    "        val_samples = folds[fold_idx]\n",
    "        train_samples = np.array(list(chain(*[folds[i] for i in range(k) if i != fold_idx])))\n",
    "\n",
    "        # Split data into train and validation\n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        val_data = []\n",
    "        val_labels = []\n",
    "\n",
    "        # Create sequences for training set\n",
    "        for sid in train_samples:\n",
    "            seq = df_train[df_train['sample_index'] == sid].sort_values('time')[feature_cols].values\n",
    "            train_data.append(torch.FloatTensor(seq))\n",
    "            label = df_labels[df_labels['sample_index'] == sid]['label'].values[0]\n",
    "            train_labels.append(label_map[label])\n",
    "\n",
    "        # Create sequences for validation set\n",
    "        for sid in val_samples:\n",
    "            seq = df_train[df_train['sample_index'] == sid].sort_values('time')[feature_cols].values\n",
    "            val_data.append(torch.FloatTensor(seq))\n",
    "            label = df_labels[df_labels['sample_index'] == sid]['label'].values[0]\n",
    "            val_labels.append(label_map[label])\n",
    "\n",
    "        # Normalize sequences (only joint data)\n",
    "        for sequences in [train_data, val_data]:\n",
    "            for i in range(len(sequences)):\n",
    "                joint_data = sequences[i][:, :len(joint_cols)]\n",
    "                joint_min = joint_data.min(dim=0)[0]\n",
    "                joint_max = joint_data.max(dim=0)[0]\n",
    "                joint_range = joint_max - joint_min\n",
    "                joint_range[joint_range == 0] = 1\n",
    "                normalized_joints = 2 * ((joint_data - joint_min) / joint_range) - 1\n",
    "                sequences[i][:, :len(joint_cols)] = normalized_joints\n",
    "\n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = PainDataset(train_data, torch.LongTensor(train_labels))\n",
    "        val_dataset = PainDataset(val_data, torch.LongTensor(val_labels))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                              shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"  Training sequences: {len(train_data)}\")\n",
    "            print(f\"  Validation sequences: {len(val_data)}\")\n",
    "\n",
    "        # Reset model to initial weights\n",
    "        model.load_state_dict(initial_state)\n",
    "\n",
    "        # Define optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "        # Enable mixed precision training\n",
    "        fold_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "        # Create directory for model checkpoints\n",
    "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
    "\n",
    "        # Train model on current fold\n",
    "        _, training_history = fit(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=epochs,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scaler=fold_scaler,\n",
    "            device=device,\n",
    "            writer=writer,\n",
    "            patience=patience,\n",
    "            verbose=verbose,\n",
    "            l1_lambda=l1_lambda,\n",
    "            evaluation_metric=evaluation_metric,\n",
    "            mode=mode,\n",
    "            restore_best_weights=restore_best_weights,\n",
    "            experiment_name=f\"{experiment_name}/fold_{fold_idx}\"\n",
    "        )\n",
    "\n",
    "        # Store results for this fold\n",
    "        fold_losses[f\"fold_{fold_idx}\"] = training_history['val_loss']\n",
    "        fold_metrics[f\"fold_{fold_idx}\"] = training_history['val_f1']\n",
    "        best_scores[f\"fold_{fold_idx}\"] = max(training_history['val_f1'])\n",
    "\n",
    "    # Compute mean and standard deviation of best scores\n",
    "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"fold_\")])\n",
    "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"fold_\")])\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"\\nCross-validation score: {best_scores['mean']:.4f} Â± {best_scores['std']:.4f}\")\n",
    "\n",
    "    return fold_losses, fold_metrics, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ff707419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Layer (type)              Output Shape                 Param #           \n",
      "===============================================================================\n",
      "rnn (LSTM)                [[-1, 200, 128], [2, -1, 128]] 216,576        \n",
      "classifier (Linear)       [-1, 3]                      387            \n",
      "===============================================================================\n",
      "Total params: 216,963\n",
      "Trainable params: 216,963\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n",
      "Training 500 epochs...\n",
      "Epoch   1/500 | Train: Loss=7.0248, F1 Score=0.6737 | Val: Loss=1.0247, F1 Score=0.6760\n",
      "Epoch   1/500 | Train: Loss=7.0248, F1 Score=0.6737 | Val: Loss=1.0247, F1 Score=0.6760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/500 | Train: Loss=6.7222, F1 Score=0.6737 | Val: Loss=0.9848, F1 Score=0.6760\n",
      "Epoch   3/500 | Train: Loss=6.4313, F1 Score=0.6737 | Val: Loss=0.9313, F1 Score=0.6760\n",
      "Epoch   3/500 | Train: Loss=6.4313, F1 Score=0.6737 | Val: Loss=0.9313, F1 Score=0.6760\n",
      "Epoch   4/500 | Train: Loss=6.1516, F1 Score=0.6737 | Val: Loss=0.8671, F1 Score=0.6760\n",
      "Epoch   5/500 | Train: Loss=5.8777, F1 Score=0.6737 | Val: Loss=0.7990, F1 Score=0.6760\n",
      "Epoch   4/500 | Train: Loss=6.1516, F1 Score=0.6737 | Val: Loss=0.8671, F1 Score=0.6760\n",
      "Epoch   5/500 | Train: Loss=5.8777, F1 Score=0.6737 | Val: Loss=0.7990, F1 Score=0.6760\n",
      "Epoch   6/500 | Train: Loss=5.6194, F1 Score=0.6737 | Val: Loss=0.7411, F1 Score=0.6760\n",
      "Epoch   6/500 | Train: Loss=5.6194, F1 Score=0.6737 | Val: Loss=0.7411, F1 Score=0.6760\n",
      "Epoch   7/500 | Train: Loss=5.3805, F1 Score=0.6737 | Val: Loss=0.7063, F1 Score=0.6760\n",
      "Epoch   8/500 | Train: Loss=5.1524, F1 Score=0.6737 | Val: Loss=0.6895, F1 Score=0.6760\n",
      "Epoch   7/500 | Train: Loss=5.3805, F1 Score=0.6737 | Val: Loss=0.7063, F1 Score=0.6760\n",
      "Epoch   8/500 | Train: Loss=5.1524, F1 Score=0.6737 | Val: Loss=0.6895, F1 Score=0.6760\n",
      "Epoch   9/500 | Train: Loss=4.9307, F1 Score=0.6737 | Val: Loss=0.6842, F1 Score=0.6760\n",
      "Epoch   9/500 | Train: Loss=4.9307, F1 Score=0.6737 | Val: Loss=0.6842, F1 Score=0.6760\n",
      "Epoch  10/500 | Train: Loss=4.7096, F1 Score=0.6737 | Val: Loss=0.6827, F1 Score=0.6760\n",
      "Epoch  11/500 | Train: Loss=4.4967, F1 Score=0.6737 | Val: Loss=0.6822, F1 Score=0.6760\n",
      "Epoch  10/500 | Train: Loss=4.7096, F1 Score=0.6737 | Val: Loss=0.6827, F1 Score=0.6760\n",
      "Epoch  11/500 | Train: Loss=4.4967, F1 Score=0.6737 | Val: Loss=0.6822, F1 Score=0.6760\n",
      "Epoch  12/500 | Train: Loss=4.2875, F1 Score=0.6737 | Val: Loss=0.6827, F1 Score=0.6760\n",
      "Epoch  12/500 | Train: Loss=4.2875, F1 Score=0.6737 | Val: Loss=0.6827, F1 Score=0.6760\n",
      "Epoch  13/500 | Train: Loss=4.0846, F1 Score=0.6737 | Val: Loss=0.6835, F1 Score=0.6760\n",
      "Epoch  14/500 | Train: Loss=3.8821, F1 Score=0.6737 | Val: Loss=0.6849, F1 Score=0.6760\n",
      "Epoch  13/500 | Train: Loss=4.0846, F1 Score=0.6737 | Val: Loss=0.6835, F1 Score=0.6760\n",
      "Epoch  14/500 | Train: Loss=3.8821, F1 Score=0.6737 | Val: Loss=0.6849, F1 Score=0.6760\n",
      "Epoch  15/500 | Train: Loss=3.6973, F1 Score=0.6737 | Val: Loss=0.6868, F1 Score=0.6760\n",
      "Epoch  15/500 | Train: Loss=3.6973, F1 Score=0.6737 | Val: Loss=0.6868, F1 Score=0.6760\n",
      "Epoch  16/500 | Train: Loss=3.5178, F1 Score=0.6737 | Val: Loss=0.6871, F1 Score=0.6760\n",
      "Epoch  17/500 | Train: Loss=3.3478, F1 Score=0.6737 | Val: Loss=0.6859, F1 Score=0.6760\n",
      "Epoch  16/500 | Train: Loss=3.5178, F1 Score=0.6737 | Val: Loss=0.6871, F1 Score=0.6760\n",
      "Epoch  17/500 | Train: Loss=3.3478, F1 Score=0.6737 | Val: Loss=0.6859, F1 Score=0.6760\n",
      "Epoch  18/500 | Train: Loss=3.1815, F1 Score=0.6737 | Val: Loss=0.6840, F1 Score=0.6760\n",
      "Epoch  18/500 | Train: Loss=3.1815, F1 Score=0.6737 | Val: Loss=0.6840, F1 Score=0.6760\n",
      "Epoch  19/500 | Train: Loss=3.0201, F1 Score=0.6737 | Val: Loss=0.6826, F1 Score=0.6760\n",
      "Epoch  20/500 | Train: Loss=2.8688, F1 Score=0.6737 | Val: Loss=0.6827, F1 Score=0.6760\n",
      "Epoch  19/500 | Train: Loss=3.0201, F1 Score=0.6737 | Val: Loss=0.6826, F1 Score=0.6760\n",
      "Epoch  20/500 | Train: Loss=2.8688, F1 Score=0.6737 | Val: Loss=0.6827, F1 Score=0.6760\n",
      "Epoch  21/500 | Train: Loss=2.7281, F1 Score=0.6737 | Val: Loss=0.6848, F1 Score=0.6760\n",
      "Epoch  21/500 | Train: Loss=2.7281, F1 Score=0.6737 | Val: Loss=0.6848, F1 Score=0.6760\n",
      "Epoch  22/500 | Train: Loss=2.5968, F1 Score=0.6737 | Val: Loss=0.6883, F1 Score=0.6760\n",
      "Epoch  23/500 | Train: Loss=2.4771, F1 Score=0.6737 | Val: Loss=0.6916, F1 Score=0.6760\n",
      "Epoch  22/500 | Train: Loss=2.5968, F1 Score=0.6737 | Val: Loss=0.6883, F1 Score=0.6760\n",
      "Epoch  23/500 | Train: Loss=2.4771, F1 Score=0.6737 | Val: Loss=0.6916, F1 Score=0.6760\n",
      "Epoch  24/500 | Train: Loss=2.3648, F1 Score=0.6737 | Val: Loss=0.6929, F1 Score=0.6760\n",
      "Epoch  24/500 | Train: Loss=2.3648, F1 Score=0.6737 | Val: Loss=0.6929, F1 Score=0.6760\n",
      "Epoch  25/500 | Train: Loss=2.2561, F1 Score=0.6737 | Val: Loss=0.6886, F1 Score=0.6760\n",
      "Epoch  26/500 | Train: Loss=2.1479, F1 Score=0.6737 | Val: Loss=0.6831, F1 Score=0.6760\n",
      "Epoch  25/500 | Train: Loss=2.2561, F1 Score=0.6737 | Val: Loss=0.6886, F1 Score=0.6760\n",
      "Epoch  26/500 | Train: Loss=2.1479, F1 Score=0.6737 | Val: Loss=0.6831, F1 Score=0.6760\n",
      "Epoch  27/500 | Train: Loss=2.0481, F1 Score=0.6737 | Val: Loss=0.6839, F1 Score=0.6760\n",
      "Epoch  27/500 | Train: Loss=2.0481, F1 Score=0.6737 | Val: Loss=0.6839, F1 Score=0.6760\n",
      "Epoch  28/500 | Train: Loss=1.9557, F1 Score=0.6737 | Val: Loss=0.6880, F1 Score=0.6760\n",
      "Epoch  29/500 | Train: Loss=1.8714, F1 Score=0.6737 | Val: Loss=0.6914, F1 Score=0.6760\n",
      "Epoch  28/500 | Train: Loss=1.9557, F1 Score=0.6737 | Val: Loss=0.6880, F1 Score=0.6760\n",
      "Epoch  29/500 | Train: Loss=1.8714, F1 Score=0.6737 | Val: Loss=0.6914, F1 Score=0.6760\n",
      "Epoch  30/500 | Train: Loss=1.7947, F1 Score=0.6737 | Val: Loss=0.6949, F1 Score=0.6760\n",
      "Epoch  30/500 | Train: Loss=1.7947, F1 Score=0.6737 | Val: Loss=0.6949, F1 Score=0.6760\n",
      "Epoch  31/500 | Train: Loss=1.7213, F1 Score=0.6737 | Val: Loss=0.6977, F1 Score=0.6760\n",
      "Epoch  32/500 | Train: Loss=1.6548, F1 Score=0.6737 | Val: Loss=0.7010, F1 Score=0.6760\n",
      "Epoch  31/500 | Train: Loss=1.7213, F1 Score=0.6737 | Val: Loss=0.6977, F1 Score=0.6760\n",
      "Epoch  32/500 | Train: Loss=1.6548, F1 Score=0.6737 | Val: Loss=0.7010, F1 Score=0.6760\n",
      "Epoch  33/500 | Train: Loss=1.5956, F1 Score=0.6737 | Val: Loss=0.6987, F1 Score=0.6760\n",
      "Epoch  33/500 | Train: Loss=1.5956, F1 Score=0.6737 | Val: Loss=0.6987, F1 Score=0.6760\n",
      "Epoch  34/500 | Train: Loss=1.5384, F1 Score=0.6737 | Val: Loss=0.6919, F1 Score=0.6760\n",
      "Epoch  35/500 | Train: Loss=1.4790, F1 Score=0.6737 | Val: Loss=0.6853, F1 Score=0.6760\n",
      "Epoch  34/500 | Train: Loss=1.5384, F1 Score=0.6737 | Val: Loss=0.6919, F1 Score=0.6760\n",
      "Epoch  35/500 | Train: Loss=1.4790, F1 Score=0.6737 | Val: Loss=0.6853, F1 Score=0.6760\n",
      "Epoch  36/500 | Train: Loss=1.4290, F1 Score=0.6737 | Val: Loss=0.6835, F1 Score=0.6760\n",
      "Epoch  36/500 | Train: Loss=1.4290, F1 Score=0.6737 | Val: Loss=0.6835, F1 Score=0.6760\n",
      "Epoch  37/500 | Train: Loss=1.3855, F1 Score=0.6737 | Val: Loss=0.6878, F1 Score=0.6760\n",
      "Epoch  38/500 | Train: Loss=1.3512, F1 Score=0.6737 | Val: Loss=0.6899, F1 Score=0.6760\n",
      "Epoch  37/500 | Train: Loss=1.3855, F1 Score=0.6737 | Val: Loss=0.6878, F1 Score=0.6760\n",
      "Epoch  38/500 | Train: Loss=1.3512, F1 Score=0.6737 | Val: Loss=0.6899, F1 Score=0.6760\n",
      "Epoch  39/500 | Train: Loss=1.3164, F1 Score=0.6737 | Val: Loss=0.6890, F1 Score=0.6760\n",
      "Epoch  39/500 | Train: Loss=1.3164, F1 Score=0.6737 | Val: Loss=0.6890, F1 Score=0.6760\n",
      "Epoch  40/500 | Train: Loss=1.2807, F1 Score=0.6737 | Val: Loss=0.6850, F1 Score=0.6760\n",
      "Epoch  41/500 | Train: Loss=1.2418, F1 Score=0.6737 | Val: Loss=0.6828, F1 Score=0.6760\n",
      "Epoch  40/500 | Train: Loss=1.2807, F1 Score=0.6737 | Val: Loss=0.6850, F1 Score=0.6760\n",
      "Epoch  41/500 | Train: Loss=1.2418, F1 Score=0.6737 | Val: Loss=0.6828, F1 Score=0.6760\n",
      "Epoch  42/500 | Train: Loss=1.2100, F1 Score=0.6737 | Val: Loss=0.6882, F1 Score=0.6760\n",
      "Epoch  42/500 | Train: Loss=1.2100, F1 Score=0.6737 | Val: Loss=0.6882, F1 Score=0.6760\n",
      "Epoch  43/500 | Train: Loss=1.1860, F1 Score=0.6737 | Val: Loss=0.6910, F1 Score=0.6760\n",
      "Epoch  44/500 | Train: Loss=1.1646, F1 Score=0.6737 | Val: Loss=0.6881, F1 Score=0.6760\n",
      "Epoch  43/500 | Train: Loss=1.1860, F1 Score=0.6737 | Val: Loss=0.6910, F1 Score=0.6760\n",
      "Epoch  44/500 | Train: Loss=1.1646, F1 Score=0.6737 | Val: Loss=0.6881, F1 Score=0.6760\n",
      "Epoch  45/500 | Train: Loss=1.1412, F1 Score=0.6737 | Val: Loss=0.6846, F1 Score=0.6760\n",
      "Epoch  45/500 | Train: Loss=1.1412, F1 Score=0.6737 | Val: Loss=0.6846, F1 Score=0.6760\n",
      "Epoch  46/500 | Train: Loss=1.1211, F1 Score=0.6737 | Val: Loss=0.6839, F1 Score=0.6760\n",
      "Epoch  47/500 | Train: Loss=1.1046, F1 Score=0.6737 | Val: Loss=0.6842, F1 Score=0.6760\n",
      "Epoch  46/500 | Train: Loss=1.1211, F1 Score=0.6737 | Val: Loss=0.6839, F1 Score=0.6760\n",
      "Epoch  47/500 | Train: Loss=1.1046, F1 Score=0.6737 | Val: Loss=0.6842, F1 Score=0.6760\n",
      "Epoch  48/500 | Train: Loss=1.0858, F1 Score=0.6737 | Val: Loss=0.6836, F1 Score=0.6760\n",
      "Epoch  48/500 | Train: Loss=1.0858, F1 Score=0.6737 | Val: Loss=0.6836, F1 Score=0.6760\n",
      "Epoch  49/500 | Train: Loss=1.0689, F1 Score=0.6737 | Val: Loss=0.6825, F1 Score=0.6760\n",
      "Epoch  50/500 | Train: Loss=1.0570, F1 Score=0.6737 | Val: Loss=0.6827, F1 Score=0.6760\n",
      "Epoch  49/500 | Train: Loss=1.0689, F1 Score=0.6737 | Val: Loss=0.6825, F1 Score=0.6760\n",
      "Epoch  50/500 | Train: Loss=1.0570, F1 Score=0.6737 | Val: Loss=0.6827, F1 Score=0.6760\n",
      "Epoch  51/500 | Train: Loss=1.0447, F1 Score=0.6737 | Val: Loss=0.6826, F1 Score=0.6760\n",
      "Early stopping triggered after 51 epochs.\n",
      "Best model restored from epoch 1 with val_f1 0.6760\n",
      "Epoch  51/500 | Train: Loss=1.0447, F1 Score=0.6737 | Val: Loss=0.6826, F1 Score=0.6760\n",
      "Early stopping triggered after 51 epochs.\n",
      "Best model restored from epoch 1 with val_f1 0.6760\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RecurrentClassifier(\n",
    "    input_size=input_size, # Pass the number of features\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=HIDDEN_LAYERS,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    rnn_type=RNN_TYPE,\n",
    "    ).to(device)\n",
    "recurrent_summary(rnn_model, input_size=(WINDOW_SIZE, input_size))\n",
    "\n",
    "# Define optimizer with L2 regularization\n",
    "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "# Enable mixed precision training for GPU acceleration\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# Train model and track training history\n",
    "rnn_model, training_history = fit(\n",
    "    model=rnn_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=CRITERION,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    verbose=VERBOSE, \n",
    "    experiment_name=\"LSTM\",\n",
    "    patience=PATIENCE,\n",
    "    l1_lambda=L1_LAMBDA,\n",
    "    l2_lambda=L2_LAMBDA,\n",
    "    evaluation_metric=\"val_f1\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "00c4a947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Plot Hitory\n",
    "# Create a figure with two side-by-side subplots (two columns)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation accuracy on the second axis\n",
    "ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
    "ax2.set_title('F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
