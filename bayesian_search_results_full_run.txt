[I 2025-11-10 13:41:36,464] A new study created in memory with name: no-name-ffbedcd3-fdb7-4c7e-810b-f228c83fd6a1

--- Starting Bayesian Hyperparameter Optimization ---
  0%|          | 0/50 [00:00<?, ?it/s]

--- Starting Trial 0 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 20
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.0
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6729, F1=0.6633 | Val: Loss=0.4339, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2071, F1=0.9135 | Val: Loss=0.2403, F1=0.9051 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1762, F1=0.9724 | Val: Loss=0.2191, F1=0.9408 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1652, F1=0.9852 | Val: Loss=0.2674, F1=0.9274 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1721, F1=0.9780 | Val: Loss=0.2151, F1=0.9416 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1694, F1=0.9839 | Val: Loss=0.2110, F1=0.9352 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1653, F1=0.9886 | Val: Loss=0.2397, F1=0.9312 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1654, F1=0.9918 | Val: Loss=0.2306, F1=0.9182 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1712, F1=0.9880 | Val: Loss=0.2119, F1=0.9368 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1678, F1=0.9931 | Val: Loss=0.2523, F1=0.9146 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1631, F1=0.9943 | Val: Loss=0.2291, F1=0.9286 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1655, F1=0.9924 | Val: Loss=0.2286, F1=0.9428 (Per-Sample)
Early stopping triggered after 115 epochs.
Best model restored from epoch 65 with val_f1 0.9529

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6772, F1=0.6593 | Val: Loss=0.4412, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1960, F1=0.9068 | Val: Loss=0.3214, F1=0.8710 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1676, F1=0.9724 | Val: Loss=0.2821, F1=0.9115 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1713, F1=0.9780 | Val: Loss=0.2532, F1=0.9003 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1641, F1=0.9799 | Val: Loss=0.2452, F1=0.9089 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1529, F1=0.9923 | Val: Loss=0.2374, F1=0.9270 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1596, F1=0.9924 | Val: Loss=0.2734, F1=0.9053 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1577, F1=0.9918 | Val: Loss=0.2897, F1=0.9201 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1552, F1=0.9937 | Val: Loss=0.2725, F1=0.9197 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1570, F1=0.9943 | Val: Loss=0.2587, F1=0.9275 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1544, F1=0.9962 | Val: Loss=0.2533, F1=0.9445 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1527, F1=0.9975 | Val: Loss=0.2732, F1=0.9187 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1531, F1=0.9962 | Val: Loss=0.2649, F1=0.9359 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1549, F1=0.9962 | Val: Loss=0.2555, F1=0.9444 (Per-Sample)
Early stopping triggered after 139 epochs.
Best model restored from epoch 89 with val_f1 0.9689

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6088, F1=0.6846 | Val: Loss=0.4418, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1860, F1=0.9124 | Val: Loss=0.2552, F1=0.8173 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1619, F1=0.9726 | Val: Loss=0.2144, F1=0.9065 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1534, F1=0.9805 | Val: Loss=0.2352, F1=0.9103 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1592, F1=0.9802 | Val: Loss=0.2219, F1=0.9454 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1485, F1=0.9891 | Val: Loss=0.2259, F1=0.9208 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1545, F1=0.9874 | Val: Loss=0.2149, F1=0.9454 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1537, F1=0.9911 | Val: Loss=0.2280, F1=0.9084 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1492, F1=0.9892 | Val: Loss=0.1992, F1=0.9175 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1488, F1=0.9931 | Val: Loss=0.1921, F1=0.9454 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1548, F1=0.9906 | Val: Loss=0.2220, F1=0.9194 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1449, F1=0.9975 | Val: Loss=0.1971, F1=0.9435 (Per-Sample)
Early stopping triggered after 112 epochs.
Best model restored from epoch 62 with val_f1 0.9600

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5926, F1=0.6867 | Val: Loss=0.4418, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1837, F1=0.8891 | Val: Loss=0.2638, F1=0.8225 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1590, F1=0.9645 | Val: Loss=0.2362, F1=0.8734 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1486, F1=0.9804 | Val: Loss=0.2342, F1=0.8963 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1554, F1=0.9795 | Val: Loss=0.2519, F1=0.9116 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1490, F1=0.9878 | Val: Loss=0.2524, F1=0.9042 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1423, F1=0.9936 | Val: Loss=0.2265, F1=0.9187 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1461, F1=0.9898 | Val: Loss=0.2408, F1=0.9187 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1444, F1=0.9937 | Val: Loss=0.2369, F1=0.9197 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1442, F1=0.9950 | Val: Loss=0.2417, F1=0.9206 (Per-Sample)
Early stopping triggered after 97 epochs.
Best model restored from epoch 47 with val_f1 0.9284

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6437, F1=0.6531 | Val: Loss=0.3157, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1893, F1=0.9050 | Val: Loss=0.2038, F1=0.9159 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1632, F1=0.9723 | Val: Loss=0.2057, F1=0.9334 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1656, F1=0.9794 | Val: Loss=0.2165, F1=0.9357 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1563, F1=0.9859 | Val: Loss=0.2129, F1=0.9529 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1614, F1=0.9835 | Val: Loss=0.2411, F1=0.9057 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1511, F1=0.9943 | Val: Loss=0.2187, F1=0.9456 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1554, F1=0.9899 | Val: Loss=0.2259, F1=0.9456 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1558, F1=0.9892 | Val: Loss=0.2230, F1=0.9372 (Per-Sample)
Best trial: 0. Best value: -0.955795:   2%|▏         | 1/50 [02:23<1:57:35, 143.99s/it]
Early stopping triggered after 89 epochs.
Best model restored from epoch 39 with val_f1 0.9687

Cross-validation score: 0.9558 ± 0.0149
  > Trial 0 Result: Mean F1 = 0.9558
[I 2025-11-10 13:44:00,456] Trial 0 finished with value: -0.9557948628906383 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.0, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 0 with value: -0.9557948628906383.

--- Starting Trial 1 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 40
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.05
  > hidden_layers: 3
  > hidden_size: 32
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 64
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.0
  > l2_lambda: 0.0001
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5868, F1=0.6761 | Val: Loss=0.3844, F1=0.7488 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1626, F1=0.8742 | Val: Loss=0.1699, F1=0.8990 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1396, F1=0.9462 | Val: Loss=0.1603, F1=0.9311 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1173, F1=0.9749 | Val: Loss=0.1553, F1=0.9592 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1122, F1=0.9794 | Val: Loss=0.1604, F1=0.9517 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1137, F1=0.9795 | Val: Loss=0.1906, F1=0.9511 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1064, F1=0.9860 | Val: Loss=0.2122, F1=0.9308 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1042, F1=0.9912 | Val: Loss=0.1845, F1=0.9548 (Per-Sample)
Early stopping triggered after 79 epochs.
Best model restored from epoch 29 with val_f1 0.9686

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5812, F1=0.6895 | Val: Loss=0.3836, F1=0.7280 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1482, F1=0.8693 | Val: Loss=0.2359, F1=0.8279 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1255, F1=0.9409 | Val: Loss=0.2667, F1=0.8758 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1046, F1=0.9800 | Val: Loss=0.2024, F1=0.9281 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1016, F1=0.9839 | Val: Loss=0.2544, F1=0.9220 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1036, F1=0.9885 | Val: Loss=0.2540, F1=0.9029 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1121, F1=0.9827 | Val: Loss=0.2552, F1=0.8970 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1029, F1=0.9905 | Val: Loss=0.2105, F1=0.9373 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0937, F1=0.9937 | Val: Loss=0.2434, F1=0.9216 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0951, F1=0.9956 | Val: Loss=0.2815, F1=0.9075 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0900, F1=0.9975 | Val: Loss=0.2347, F1=0.9365 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0878, F1=0.9987 | Val: Loss=0.2630, F1=0.9047 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0884, F1=0.9987 | Val: Loss=0.2423, F1=0.9281 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0888, F1=0.9968 | Val: Loss=0.2558, F1=0.9227 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0896, F1=0.9968 | Val: Loss=0.2668, F1=0.9125 (Per-Sample)
Early stopping triggered after 149 epochs.
Best model restored from epoch 99 with val_f1 0.9451

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5537, F1=0.6973 | Val: Loss=0.4006, F1=0.7073 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1403, F1=0.8732 | Val: Loss=0.2212, F1=0.8109 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1116, F1=0.9635 | Val: Loss=0.1984, F1=0.8897 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1265, F1=0.9635 | Val: Loss=0.1984, F1=0.9017 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1076, F1=0.9801 | Val: Loss=0.1865, F1=0.9231 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0989, F1=0.9853 | Val: Loss=0.1824, F1=0.9388 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0881, F1=0.9924 | Val: Loss=0.1945, F1=0.9396 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0889, F1=0.9943 | Val: Loss=0.2125, F1=0.9323 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0924, F1=0.9905 | Val: Loss=0.2021, F1=0.9223 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0865, F1=0.9956 | Val: Loss=0.1894, F1=0.9313 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0887, F1=0.9930 | Val: Loss=0.1921, F1=0.9394 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0872, F1=0.9956 | Val: Loss=0.2331, F1=0.9405 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0849, F1=0.9962 | Val: Loss=0.1869, F1=0.9469 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0809, F1=1.0000 | Val: Loss=0.1974, F1=0.9313 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0856, F1=0.9968 | Val: Loss=0.2076, F1=0.9234 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0908, F1=0.9943 | Val: Loss=0.2046, F1=0.9388 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0905, F1=0.9937 | Val: Loss=0.1725, F1=0.9549 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0870, F1=0.9968 | Val: Loss=0.2081, F1=0.9469 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0831, F1=0.9975 | Val: Loss=0.1895, F1=0.9401 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0908, F1=0.9904 | Val: Loss=0.1921, F1=0.9400 (Per-Sample)
Early stopping triggered after 196 epochs.
Best model restored from epoch 146 with val_f1 0.9628

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6209, F1=0.6876 | Val: Loss=0.4310, F1=0.6530 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1509, F1=0.8777 | Val: Loss=0.2163, F1=0.8075 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1155, F1=0.9520 | Val: Loss=0.2088, F1=0.8566 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1036, F1=0.9737 | Val: Loss=0.2120, F1=0.9048 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0975, F1=0.9873 | Val: Loss=0.2258, F1=0.8910 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1022, F1=0.9848 | Val: Loss=0.2353, F1=0.8969 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0932, F1=0.9892 | Val: Loss=0.2130, F1=0.9366 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0926, F1=0.9924 | Val: Loss=0.2516, F1=0.8957 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0919, F1=0.9924 | Val: Loss=0.2562, F1=0.9067 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0857, F1=0.9949 | Val: Loss=0.2368, F1=0.9047 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0853, F1=0.9956 | Val: Loss=0.2608, F1=0.8891 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0874, F1=0.9956 | Val: Loss=0.2293, F1=0.9202 (Per-Sample)
Early stopping triggered after 110 epochs.
Best model restored from epoch 60 with val_f1 0.9366

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5920, F1=0.6778 | Val: Loss=0.2774, F1=0.7928 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1505, F1=0.8582 | Val: Loss=0.1756, F1=0.8692 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1290, F1=0.9329 | Val: Loss=0.1509, F1=0.9511 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0994, F1=0.9770 | Val: Loss=0.1663, F1=0.9346 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1173, F1=0.9677 | Val: Loss=0.1794, F1=0.9265 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0947, F1=0.9904 | Val: Loss=0.2151, F1=0.9225 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0928, F1=0.9924 | Val: Loss=0.1875, F1=0.9461 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0939, F1=0.9924 | Val: Loss=0.1699, F1=0.9261 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0914, F1=0.9930 | Val: Loss=0.1978, F1=0.9323 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0855, F1=0.9975 | Val: Loss=0.1910, F1=0.9405 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0991, F1=0.9924 | Val: Loss=0.1985, F1=0.9478 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0914, F1=0.9930 | Val: Loss=0.1756, F1=0.9378 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0959, F1=0.9918 | Val: Loss=0.1699, F1=0.9478 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0993, F1=0.9911 | Val: Loss=0.1733, F1=0.9431 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0873, F1=0.9955 | Val: Loss=0.1952, F1=0.9478 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0906, F1=0.9962 | Val: Loss=0.1813, F1=0.9470 (Per-Sample)
Epoch 160/300 | Train: Loss=0.1021, F1=0.9905 | Val: Loss=0.2137, F1=0.9237 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0856, F1=0.9975 | Val: Loss=0.2328, F1=0.9158 (Per-Sample)
Best trial: 0. Best value: -0.955795:   4%|▍         | 2/50 [06:07<2:32:43, 190.90s/it]
Early stopping triggered after 172 epochs.
Best model restored from epoch 122 with val_f1 0.9615

Cross-validation score: 0.9549 ± 0.0120
  > Trial 1 Result: Mean F1 = 0.9549
[I 2025-11-10 13:47:44,191] Trial 1 finished with value: -0.9549080348039777 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 40, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.05, 'hidden_layers': 3, 'hidden_size': 32, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.0, 'l2_lambda': 0.0001, 'noise_std_dev': 0.1}. Best is trial 0 with value: -0.9557948628906383.

--- Starting Trial 2 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 20
  > weight_ce_intensity: 0.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Best trial: 0. Best value: -0.955795:   6%|▌         | 3/50 [06:08<1:21:26, 103.97s/it]
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.0): [0. 0. 0.]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
NaN loss at batch 0
NaN loss at batch 1
NaN loss at batch 2
NaN loss at batch 3
NaN loss at batch 4
NaN loss at batch 5
NaN loss at batch 6
NaN loss at batch 7
NaN loss at batch 8
NaN loss at batch 9
NaN loss at batch 10
NaN loss at batch 11
NaN loss at batch 12
NaN loss at batch 13
NaN loss at batch 14
NaN loss at batch 15
NaN loss at batch 16
NaN loss at batch 17
NaN loss at batch 18
NaN loss at batch 19
NaN loss at batch 20
NaN loss at batch 21
NaN loss at batch 22
NaN loss at batch 23
NaN loss at batch 24
NaN loss at batch 25
NaN loss at batch 26
NaN loss at batch 27
NaN loss at batch 28
NaN loss at batch 29
NaN loss at batch 30
NaN loss at batch 31
NaN loss at batch 32
NaN loss at batch 33
NaN loss at batch 34
NaN loss at batch 35
NaN loss at batch 36
NaN loss at batch 37
NaN loss at batch 38
NaN loss at batch 39
NaN loss at batch 40
NaN loss at batch 41
NaN loss at batch 42
NaN loss at batch 43
NaN loss at batch 44
NaN loss at batch 45
NaN loss at batch 46
NaN loss at batch 47
NaN loss at batch 48
NaN loss at batch 49
--- ERROR in Trial 2 ---
Configuration: {'batch_size': 32, 'cross_entropy_weighting': True, 'l1_lambda': 0.0, 'learning_rate': 0.001, 'window_size': 80, 'stride': 20, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.001, 'noise_std_dev': 0.0}
Error: need at least one array to concatenate
  > Trial 2 Result: F1 = 0.0 (due to error)
[I 2025-11-10 13:47:44,715] Trial 2 finished with value: 0.0 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 20, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.001, 'noise_std_dev': 0.0}. Best is trial 0 with value: -0.9557948628906383.

--- Starting Trial 3 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 40
  > weight_ce_intensity: 0.0
  > label_smoothing_epsilon: 0.05
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 64
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.1
  > l2_lambda: 0.0001
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Best trial: 0. Best value: -0.955795:   8%|▊         | 4/50 [06:08<48:23, 63.13s/it]   
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.0): [0. 0. 0.]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
NaN loss at batch 0
NaN loss at batch 1
NaN loss at batch 2
NaN loss at batch 3
NaN loss at batch 4
NaN loss at batch 5
NaN loss at batch 6
NaN loss at batch 7
NaN loss at batch 8
NaN loss at batch 9
NaN loss at batch 10
NaN loss at batch 11
NaN loss at batch 12
NaN loss at batch 13
NaN loss at batch 14
NaN loss at batch 15
NaN loss at batch 16
NaN loss at batch 17
NaN loss at batch 18
NaN loss at batch 19
NaN loss at batch 20
NaN loss at batch 21
NaN loss at batch 22
NaN loss at batch 23
NaN loss at batch 24
NaN loss at batch 25
NaN loss at batch 26
NaN loss at batch 27
NaN loss at batch 28
NaN loss at batch 29
NaN loss at batch 30
NaN loss at batch 31
NaN loss at batch 32
NaN loss at batch 33
NaN loss at batch 34
NaN loss at batch 35
NaN loss at batch 36
NaN loss at batch 37
NaN loss at batch 38
NaN loss at batch 39
NaN loss at batch 40
NaN loss at batch 41
NaN loss at batch 42
NaN loss at batch 43
NaN loss at batch 44
NaN loss at batch 45
NaN loss at batch 46
NaN loss at batch 47
NaN loss at batch 48
NaN loss at batch 49
--- ERROR in Trial 3 ---
Configuration: {'batch_size': 32, 'cross_entropy_weighting': True, 'l1_lambda': 0.0, 'learning_rate': 0.001, 'window_size': 40, 'stride': 40, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.05, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.1}
Error: need at least one array to concatenate
  > Trial 3 Result: F1 = 0.0 (due to error)
[I 2025-11-10 13:47:45,238] Trial 3 finished with value: 0.0 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 40, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.05, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.1}. Best is trial 0 with value: -0.9557948628906383.

--- Starting Trial 4 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6988, F1=0.7027 | Val: Loss=0.3846, F1=0.7488 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0806, F1=0.9306 | Val: Loss=0.1426, F1=0.9435 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0445, F1=0.9775 | Val: Loss=0.2322, F1=0.9456 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0400, F1=0.9770 | Val: Loss=0.1706, F1=0.9515 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0334, F1=0.9879 | Val: Loss=0.2982, F1=0.9324 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0456, F1=0.9866 | Val: Loss=0.2614, F1=0.9524 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0750, F1=0.9931 | Val: Loss=0.3581, F1=0.9367 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0197, F1=0.9918 | Val: Loss=0.3122, F1=0.9368 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0222, F1=0.9937 | Val: Loss=0.2779, F1=0.9531 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0261, F1=0.9950 | Val: Loss=0.3173, F1=0.9482 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0194, F1=0.9975 | Val: Loss=0.3837, F1=0.9523 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0005, F1=0.9994 | Val: Loss=0.2915, F1=0.9435 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0082, F1=0.9981 | Val: Loss=0.2906, F1=0.9696 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3802, F1=0.9613 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0015, F1=0.9994 | Val: Loss=0.4217, F1=0.9374 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.4784, F1=0.9369 (Per-Sample)
Early stopping triggered after 155 epochs.
Best model restored from epoch 105 with val_f1 0.9697

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6715, F1=0.7042 | Val: Loss=0.3622, F1=0.7146 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0785, F1=0.9375 | Val: Loss=0.2095, F1=0.9089 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0572, F1=0.9737 | Val: Loss=0.2785, F1=0.9031 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0731, F1=0.9722 | Val: Loss=0.4273, F1=0.9197 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0439, F1=0.9867 | Val: Loss=0.4586, F1=0.9129 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0259, F1=0.9924 | Val: Loss=0.5567, F1=0.9189 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0294, F1=0.9943 | Val: Loss=0.5564, F1=0.9359 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0080, F1=0.9956 | Val: Loss=0.5887, F1=0.9438 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0037, F1=0.9981 | Val: Loss=0.5586, F1=0.9271 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0025, F1=0.9981 | Val: Loss=0.6364, F1=0.9271 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0089, F1=0.9962 | Val: Loss=0.6548, F1=0.9201 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0102, F1=0.9975 | Val: Loss=0.7344, F1=0.9205 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0178, F1=0.9956 | Val: Loss=0.5674, F1=0.9197 (Per-Sample)
Early stopping triggered after 127 epochs.
Best model restored from epoch 77 with val_f1 0.9442

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6765, F1=0.7172 | Val: Loss=0.3994, F1=0.6591 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0865, F1=0.9324 | Val: Loss=0.2204, F1=0.8339 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0424, F1=0.9727 | Val: Loss=0.3573, F1=0.9230 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0551, F1=0.9716 | Val: Loss=0.3331, F1=0.9313 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0384, F1=0.9859 | Val: Loss=0.4120, F1=0.9394 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0385, F1=0.9845 | Val: Loss=0.3694, F1=0.9223 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0408, F1=0.9860 | Val: Loss=0.4452, F1=0.9223 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0414, F1=0.9905 | Val: Loss=0.4786, F1=0.9313 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0123, F1=0.9949 | Val: Loss=0.3610, F1=0.9467 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0128, F1=0.9975 | Val: Loss=0.3118, F1=0.9538 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0076, F1=0.9975 | Val: Loss=0.4234, F1=0.9465 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0006, F1=0.9994 | Val: Loss=0.4198, F1=0.9456 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0190, F1=0.9956 | Val: Loss=0.5132, F1=0.9467 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0040, F1=0.9981 | Val: Loss=0.4289, F1=0.9529 (Per-Sample)
Early stopping triggered after 136 epochs.
Best model restored from epoch 86 with val_f1 0.9622

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6573, F1=0.7008 | Val: Loss=0.4013, F1=0.6517 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0713, F1=0.9195 | Val: Loss=0.1880, F1=0.9011 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0559, F1=0.9602 | Val: Loss=0.2451, F1=0.9113 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0546, F1=0.9841 | Val: Loss=0.4553, F1=0.9133 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0500, F1=0.9866 | Val: Loss=0.3484, F1=0.9202 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0399, F1=0.9893 | Val: Loss=0.3848, F1=0.9307 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0182, F1=0.9924 | Val: Loss=0.3892, F1=0.9368 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0364, F1=0.9885 | Val: Loss=0.3255, F1=0.9286 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0204, F1=0.9950 | Val: Loss=0.4373, F1=0.9213 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0138, F1=0.9950 | Val: Loss=0.5168, F1=0.9308 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0177, F1=0.9968 | Val: Loss=0.3688, F1=0.9279 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0075, F1=0.9968 | Val: Loss=0.4287, F1=0.9382 (Per-Sample)
Early stopping triggered after 118 epochs.
Best model restored from epoch 68 with val_f1 0.9457

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7066, F1=0.6684 | Val: Loss=0.2361, F1=0.7945 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1028, F1=0.9192 | Val: Loss=0.1183, F1=0.8991 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0559, F1=0.9719 | Val: Loss=0.2803, F1=0.9428 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0585, F1=0.9822 | Val: Loss=0.3466, F1=0.9308 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0157, F1=0.9930 | Val: Loss=0.3631, F1=0.9241 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0140, F1=0.9956 | Val: Loss=0.4332, F1=0.9395 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0133, F1=0.9962 | Val: Loss=0.5063, F1=0.9388 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0251, F1=0.9975 | Val: Loss=0.4327, F1=0.9460 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0200, F1=0.9968 | Val: Loss=0.4620, F1=0.9290 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0302, F1=0.9924 | Val: Loss=0.5099, F1=0.9460 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0050, F1=0.9968 | Val: Loss=0.4201, F1=0.9330 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0028, F1=0.9981 | Val: Loss=0.4754, F1=0.9378 (Per-Sample)
Best trial: 0. Best value: -0.955795:  10%|█         | 5/50 [08:38<1:10:51, 94.48s/it]
Early stopping triggered after 118 epochs.
Best model restored from epoch 68 with val_f1 0.9544

Cross-validation score: 0.9553 ± 0.0097
  > Trial 4 Result: Mean F1 = 0.9553
[I 2025-11-10 13:50:15,308] Trial 4 finished with value: -0.9552536558703197 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 0 with value: -0.9557948628906383.

--- Starting Trial 5 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.1
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5036, F1=0.7022 | Val: Loss=0.3409, F1=0.7582 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0909, F1=0.9001 | Val: Loss=0.2018, F1=0.8937 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0705, F1=0.9414 | Val: Loss=0.1044, F1=0.9426 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0485, F1=0.9716 | Val: Loss=0.2233, F1=0.9188 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0552, F1=0.9788 | Val: Loss=0.1284, F1=0.9604 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0440, F1=0.9853 | Val: Loss=0.5696, F1=0.9217 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0697, F1=0.9873 | Val: Loss=0.3584, F1=0.9442 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0602, F1=0.9880 | Val: Loss=0.6419, F1=0.9310 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0606, F1=0.9892 | Val: Loss=0.3299, F1=0.9341 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0098, F1=0.9968 | Val: Loss=0.1935, F1=0.9604 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0270, F1=0.9949 | Val: Loss=0.2345, F1=0.9693 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0154, F1=0.9981 | Val: Loss=0.4394, F1=0.9538 (Per-Sample)
Early stopping triggered after 115 epochs.
Best model restored from epoch 65 with val_f1 0.9848

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5059, F1=0.6992 | Val: Loss=0.3308, F1=0.7407 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0831, F1=0.8944 | Val: Loss=0.3098, F1=0.8735 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0507, F1=0.9447 | Val: Loss=0.3204, F1=0.8719 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0637, F1=0.9668 | Val: Loss=0.4120, F1=0.8831 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0967, F1=0.9644 | Val: Loss=0.5304, F1=0.9045 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0519, F1=0.9827 | Val: Loss=0.6488, F1=0.9080 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0311, F1=0.9911 | Val: Loss=0.8688, F1=0.9158 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0131, F1=0.9975 | Val: Loss=0.8793, F1=0.9160 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0150, F1=0.9975 | Val: Loss=0.6829, F1=0.9230 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0309, F1=0.9962 | Val: Loss=0.7847, F1=0.9207 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0078, F1=0.9968 | Val: Loss=0.7607, F1=0.9136 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0382, F1=0.9956 | Val: Loss=0.6366, F1=0.9376 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0145, F1=0.9975 | Val: Loss=0.6564, F1=0.9386 (Per-Sample)
Early stopping triggered after 121 epochs.
Best model restored from epoch 71 with val_f1 0.9394

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.4777, F1=0.7086 | Val: Loss=0.3773, F1=0.7020 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1034, F1=0.8935 | Val: Loss=0.1563, F1=0.8326 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0647, F1=0.9513 | Val: Loss=0.1516, F1=0.9107 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0386, F1=0.9715 | Val: Loss=0.4251, F1=0.9084 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0386, F1=0.9834 | Val: Loss=0.3849, F1=0.9538 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0340, F1=0.9885 | Val: Loss=0.5187, F1=0.9245 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0497, F1=0.9898 | Val: Loss=0.5214, F1=0.9388 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0079, F1=0.9981 | Val: Loss=0.4419, F1=0.9384 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0203, F1=0.9962 | Val: Loss=0.3937, F1=0.9452 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0075, F1=0.9956 | Val: Loss=0.5302, F1=0.9400 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0009, F1=0.9994 | Val: Loss=0.4379, F1=0.9539 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0265, F1=0.9968 | Val: Loss=0.3608, F1=0.9384 (Per-Sample)
Early stopping triggered after 118 epochs.
Best model restored from epoch 68 with val_f1 0.9621

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.4706, F1=0.7297 | Val: Loss=0.3820, F1=0.6530 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0673, F1=0.8952 | Val: Loss=0.1601, F1=0.8413 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0407, F1=0.9542 | Val: Loss=0.1814, F1=0.9100 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0598, F1=0.9723 | Val: Loss=0.2983, F1=0.9295 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0169, F1=0.9865 | Val: Loss=0.4298, F1=0.9037 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0453, F1=0.9801 | Val: Loss=0.5376, F1=0.8992 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0156, F1=0.9937 | Val: Loss=0.5395, F1=0.9296 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0516, F1=0.9930 | Val: Loss=0.5904, F1=0.9043 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0267, F1=0.9962 | Val: Loss=0.5082, F1=0.9119 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0007, F1=0.9994 | Val: Loss=0.4481, F1=0.9288 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0046, F1=0.9975 | Val: Loss=0.5562, F1=0.9208 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0506, F1=0.9931 | Val: Loss=0.3928, F1=0.9288 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0366, F1=0.9943 | Val: Loss=0.4192, F1=0.9379 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0016, F1=0.9987 | Val: Loss=0.5289, F1=0.9221 (Per-Sample)
Early stopping triggered after 135 epochs.
Best model restored from epoch 85 with val_f1 0.9379

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5022, F1=0.6850 | Val: Loss=0.2406, F1=0.7945 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1092, F1=0.8589 | Val: Loss=0.1620, F1=0.8692 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0419, F1=0.9483 | Val: Loss=0.1485, F1=0.9195 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0510, F1=0.9670 | Val: Loss=0.2952, F1=0.9461 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0359, F1=0.9866 | Val: Loss=0.4784, F1=0.9210 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0201, F1=0.9911 | Val: Loss=0.4961, F1=0.9195 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0301, F1=0.9937 | Val: Loss=0.3807, F1=0.9378 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0114, F1=0.9968 | Val: Loss=0.5131, F1=0.9395 (Per-Sample)
Best trial: 5. Best value: -0.956883:  12%|█▏        | 6/50 [11:32<1:28:56, 121.29s/it]
Early stopping triggered after 78 epochs.
Best model restored from epoch 28 with val_f1 0.9601

Cross-validation score: 0.9569 ± 0.0172
  > Trial 5 Result: Mean F1 = 0.9569
[I 2025-11-10 13:53:08,640] Trial 5 finished with value: -0.9568830932580417 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.1, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 5 with value: -0.9568830932580417.

--- Starting Trial 6 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 20
  > weight_ce_intensity: 0.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 64
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.3
  > l2_lambda: 0.0
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Best trial: 5. Best value: -0.956883:  14%|█▍        | 7/50 [11:32<58:37, 81.81s/it]   
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.0): [0. 0. 0.]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
NaN loss at batch 0
NaN loss at batch 1
NaN loss at batch 2
NaN loss at batch 3
NaN loss at batch 4
NaN loss at batch 5
NaN loss at batch 6
NaN loss at batch 7
NaN loss at batch 8
NaN loss at batch 9
NaN loss at batch 10
NaN loss at batch 11
NaN loss at batch 12
NaN loss at batch 13
NaN loss at batch 14
NaN loss at batch 15
NaN loss at batch 16
NaN loss at batch 17
NaN loss at batch 18
NaN loss at batch 19
NaN loss at batch 20
NaN loss at batch 21
NaN loss at batch 22
NaN loss at batch 23
NaN loss at batch 24
NaN loss at batch 25
NaN loss at batch 26
NaN loss at batch 27
NaN loss at batch 28
NaN loss at batch 29
NaN loss at batch 30
NaN loss at batch 31
NaN loss at batch 32
NaN loss at batch 33
NaN loss at batch 34
NaN loss at batch 35
NaN loss at batch 36
NaN loss at batch 37
NaN loss at batch 38
NaN loss at batch 39
NaN loss at batch 40
NaN loss at batch 41
NaN loss at batch 42
NaN loss at batch 43
NaN loss at batch 44
NaN loss at batch 45
NaN loss at batch 46
NaN loss at batch 47
NaN loss at batch 48
NaN loss at batch 49
--- ERROR in Trial 6 ---
Configuration: {'batch_size': 32, 'cross_entropy_weighting': True, 'l1_lambda': 0.0, 'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.3, 'l2_lambda': 0.0, 'noise_std_dev': 0.1}
Error: need at least one array to concatenate
  > Trial 6 Result: F1 = 0.0 (due to error)
[I 2025-11-10 13:53:09,153] Trial 6 finished with value: 0.0 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.3, 'l2_lambda': 0.0, 'noise_std_dev': 0.1}. Best is trial 5 with value: -0.9568830932580417.

--- Starting Trial 7 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 20
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 32
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.3
  > l2_lambda: 0.01
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6803, F1=0.6558 | Val: Loss=0.4031, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0909, F1=0.8678 | Val: Loss=0.1759, F1=0.8986 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1476, F1=0.9392 | Val: Loss=0.1419, F1=0.9520 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0353, F1=0.9852 | Val: Loss=0.2840, F1=0.9451 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0599, F1=0.9847 | Val: Loss=0.3682, F1=0.9381 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0256, F1=0.9904 | Val: Loss=0.4012, F1=0.9390 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0256, F1=0.9962 | Val: Loss=0.3973, F1=0.9465 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0023, F1=0.9987 | Val: Loss=0.3023, F1=0.9597 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0323, F1=0.9956 | Val: Loss=0.3248, F1=0.9614 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0321, F1=0.9937 | Val: Loss=0.3785, F1=0.9689 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0552, F1=0.9880 | Val: Loss=0.5734, F1=0.9237 (Per-Sample)
Early stopping triggered after 106 epochs.
Best model restored from epoch 56 with val_f1 0.9694

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6392, F1=0.6792 | Val: Loss=0.4373, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0853, F1=0.8720 | Val: Loss=0.3263, F1=0.8321 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0664, F1=0.9735 | Val: Loss=0.5141, F1=0.9237 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0507, F1=0.9873 | Val: Loss=0.5024, F1=0.9216 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0476, F1=0.9872 | Val: Loss=0.6596, F1=0.9149 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0273, F1=0.9924 | Val: Loss=0.6226, F1=0.9167 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0410, F1=0.9905 | Val: Loss=0.6096, F1=0.9156 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0395, F1=0.9924 | Val: Loss=0.6434, F1=0.9077 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0156, F1=0.9956 | Val: Loss=0.5752, F1=0.9245 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0143, F1=0.9968 | Val: Loss=0.5330, F1=0.9309 (Per-Sample)
Early stopping triggered after 96 epochs.
Best model restored from epoch 46 with val_f1 0.9388

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6597, F1=0.6736 | Val: Loss=0.4372, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0687, F1=0.9298 | Val: Loss=0.2646, F1=0.8583 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1127, F1=0.9632 | Val: Loss=0.3405, F1=0.9035 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0842, F1=0.9725 | Val: Loss=0.2179, F1=0.9281 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0178, F1=0.9930 | Val: Loss=0.4551, F1=0.9318 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0455, F1=0.9854 | Val: Loss=0.3377, F1=0.9471 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0230, F1=0.9943 | Val: Loss=0.2619, F1=0.9436 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0173, F1=0.9949 | Val: Loss=0.5405, F1=0.9393 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0272, F1=0.9943 | Val: Loss=0.3140, F1=0.9538 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0028, F1=0.9968 | Val: Loss=0.3890, F1=0.9475 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0093, F1=0.9981 | Val: Loss=0.3586, F1=0.9300 (Per-Sample)
Early stopping triggered after 109 epochs.
Best model restored from epoch 59 with val_f1 0.9611

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6178, F1=0.6924 | Val: Loss=0.4490, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0673, F1=0.8800 | Val: Loss=0.1646, F1=0.8071 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0579, F1=0.9803 | Val: Loss=0.4541, F1=0.9068 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0529, F1=0.9918 | Val: Loss=0.4766, F1=0.9134 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0448, F1=0.9924 | Val: Loss=0.4260, F1=0.9207 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0279, F1=0.9950 | Val: Loss=0.4700, F1=0.9121 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0427, F1=0.9911 | Val: Loss=0.4378, F1=0.9150 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0088, F1=0.9987 | Val: Loss=0.4628, F1=0.9286 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0277, F1=0.9937 | Val: Loss=0.4404, F1=0.9383 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0019, F1=0.9987 | Val: Loss=0.4296, F1=0.9217 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0166, F1=0.9975 | Val: Loss=0.4691, F1=0.9300 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0013, F1=0.9987 | Val: Loss=0.4928, F1=0.9383 (Per-Sample)
Early stopping triggered after 112 epochs.
Best model restored from epoch 62 with val_f1 0.9390

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6417, F1=0.6415 | Val: Loss=0.2715, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1158, F1=0.8571 | Val: Loss=0.2449, F1=0.8518 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0970, F1=0.9751 | Val: Loss=0.2622, F1=0.9183 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0729, F1=0.9756 | Val: Loss=0.4868, F1=0.9251 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0357, F1=0.9892 | Val: Loss=0.5336, F1=0.9238 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0339, F1=0.9924 | Val: Loss=0.4136, F1=0.9403 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0159, F1=0.9936 | Val: Loss=0.3618, F1=0.9544 (Per-Sample)
Best trial: 5. Best value: -0.956883:  16%|█▌        | 8/50 [14:22<1:16:48, 109.72s/it]
Early stopping triggered after 69 epochs.
Best model restored from epoch 19 with val_f1 0.9605

Cross-validation score: 0.9538 ± 0.0125
  > Trial 7 Result: Mean F1 = 0.9538
[I 2025-11-10 13:55:58,641] Trial 7 finished with value: -0.9537780918870566 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 32, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.3, 'l2_lambda': 0.01, 'noise_std_dev': 0.0}. Best is trial 5 with value: -0.9568830932580417.

--- Starting Trial 8 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6440, F1=0.6597 | Val: Loss=0.3696, F1=0.7651 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1093, F1=0.8529 | Val: Loss=0.1620, F1=0.9011 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1120, F1=0.9547 | Val: Loss=0.1594, F1=0.9607 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0968, F1=0.9778 | Val: Loss=0.4437, F1=0.9218 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0600, F1=0.9873 | Val: Loss=0.3539, F1=0.9463 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0426, F1=0.9943 | Val: Loss=0.4564, F1=0.9360 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0213, F1=0.9943 | Val: Loss=0.4031, F1=0.9295 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0053, F1=0.9994 | Val: Loss=0.4175, F1=0.9511 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0206, F1=0.9956 | Val: Loss=0.3802, F1=0.9421 (Per-Sample)
Early stopping triggered after 83 epochs.
Best model restored from epoch 33 with val_f1 0.9613

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5952, F1=0.6751 | Val: Loss=0.3506, F1=0.6720 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0775, F1=0.9173 | Val: Loss=0.2817, F1=0.8745 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1205, F1=0.9687 | Val: Loss=0.5530, F1=0.8982 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0937, F1=0.9728 | Val: Loss=0.7072, F1=0.8927 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0903, F1=0.9791 | Val: Loss=0.5008, F1=0.9125 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0623, F1=0.9873 | Val: Loss=0.6641, F1=0.9147 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0205, F1=0.9962 | Val: Loss=0.7496, F1=0.9059 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0249, F1=0.9950 | Val: Loss=0.7260, F1=0.9213 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0141, F1=0.9975 | Val: Loss=0.5919, F1=0.9450 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0056, F1=0.9981 | Val: Loss=0.6469, F1=0.9372 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0156, F1=0.9962 | Val: Loss=0.6179, F1=0.9524 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0049, F1=0.9981 | Val: Loss=0.7368, F1=0.9311 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0028, F1=0.9987 | Val: Loss=0.7079, F1=0.9216 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0011, F1=0.9994 | Val: Loss=0.6837, F1=0.9379 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0640, F1=0.9918 | Val: Loss=0.7219, F1=0.9310 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0449, F1=0.9937 | Val: Loss=0.7431, F1=0.9159 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0027, F1=0.9975 | Val: Loss=0.8711, F1=0.9065 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0315, F1=0.9956 | Val: Loss=0.7561, F1=0.8913 (Per-Sample)
Early stopping triggered after 172 epochs.
Best model restored from epoch 122 with val_f1 0.9554

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5460, F1=0.6903 | Val: Loss=0.3879, F1=0.6560 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1094, F1=0.8889 | Val: Loss=0.1639, F1=0.8173 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0647, F1=0.9721 | Val: Loss=0.5261, F1=0.8947 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0997, F1=0.9535 | Val: Loss=0.3487, F1=0.8689 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0609, F1=0.9821 | Val: Loss=0.3315, F1=0.9213 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0157, F1=0.9936 | Val: Loss=0.3726, F1=0.9141 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0154, F1=0.9949 | Val: Loss=0.4141, F1=0.9239 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0300, F1=0.9943 | Val: Loss=0.4223, F1=0.9228 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0037, F1=0.9981 | Val: Loss=0.4367, F1=0.9156 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0002, F1=1.0000 | Val: Loss=0.3916, F1=0.9166 (Per-Sample)
Early stopping triggered after 96 epochs.
Best model restored from epoch 46 with val_f1 0.9469

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5821, F1=0.6861 | Val: Loss=0.3868, F1=0.6387 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0807, F1=0.9046 | Val: Loss=0.2510, F1=0.8151 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0945, F1=0.9568 | Val: Loss=0.3245, F1=0.8963 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1089, F1=0.9709 | Val: Loss=0.3776, F1=0.9284 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0377, F1=0.9892 | Val: Loss=0.4285, F1=0.9461 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0204, F1=0.9924 | Val: Loss=0.5163, F1=0.9222 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0454, F1=0.9905 | Val: Loss=0.2885, F1=0.9272 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0277, F1=0.9962 | Val: Loss=0.4061, F1=0.9206 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0112, F1=0.9962 | Val: Loss=0.4717, F1=0.9061 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0155, F1=0.9968 | Val: Loss=0.4107, F1=0.9115 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0078, F1=0.9962 | Val: Loss=0.3627, F1=0.9289 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0121, F1=0.9962 | Val: Loss=0.3972, F1=0.9378 (Per-Sample)
Early stopping triggered after 115 epochs.
Best model restored from epoch 65 with val_f1 0.9537

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5812, F1=0.6649 | Val: Loss=0.2474, F1=0.8243 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0906, F1=0.9204 | Val: Loss=0.1189, F1=0.9007 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1125, F1=0.9621 | Val: Loss=0.1950, F1=0.9179 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0811, F1=0.9783 | Val: Loss=0.3070, F1=0.9197 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0803, F1=0.9868 | Val: Loss=0.3256, F1=0.9287 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0361, F1=0.9943 | Val: Loss=0.5207, F1=0.9261 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0735, F1=0.9893 | Val: Loss=0.4223, F1=0.9308 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0361, F1=0.9931 | Val: Loss=0.3833, F1=0.9365 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0075, F1=0.9968 | Val: Loss=0.5600, F1=0.9315 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0146, F1=0.9975 | Val: Loss=0.4304, F1=0.9372 (Per-Sample)
Best trial: 8. Best value: -0.957224:  18%|█▊        | 9/50 [17:44<1:34:51, 138.82s/it]
Early stopping triggered after 95 epochs.
Best model restored from epoch 45 with val_f1 0.9688

Cross-validation score: 0.9572 ± 0.0074
  > Trial 8 Result: Mean F1 = 0.9572
[I 2025-11-10 13:59:21,455] Trial 8 finished with value: -0.9572237819517072 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 8 with value: -0.9572237819517072.

--- Starting Trial 9 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.001
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7383, F1=0.6621 | Val: Loss=0.3992, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0822, F1=0.8936 | Val: Loss=0.0694, F1=0.9230 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0787, F1=0.9641 | Val: Loss=0.1754, F1=0.8992 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0471, F1=0.9795 | Val: Loss=0.3627, F1=0.9306 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0388, F1=0.9813 | Val: Loss=0.4226, F1=0.9380 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0055, F1=0.9943 | Val: Loss=0.2978, F1=0.9327 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0400, F1=0.9906 | Val: Loss=0.4059, F1=0.9332 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0370, F1=0.9912 | Val: Loss=0.5258, F1=0.9201 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0170, F1=0.9968 | Val: Loss=0.3746, F1=0.9286 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0084, F1=0.9968 | Val: Loss=0.3745, F1=0.9198 (Per-Sample)
Early stopping triggered after 99 epochs.
Best model restored from epoch 49 with val_f1 0.9630

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7536, F1=0.6621 | Val: Loss=0.3772, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0799, F1=0.9065 | Val: Loss=0.2150, F1=0.9003 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0375, F1=0.9768 | Val: Loss=0.4043, F1=0.9216 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0373, F1=0.9832 | Val: Loss=0.4642, F1=0.9374 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0433, F1=0.9795 | Val: Loss=0.4358, F1=0.9173 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0226, F1=0.9924 | Val: Loss=0.5055, F1=0.9237 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0250, F1=0.9943 | Val: Loss=0.5031, F1=0.9370 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0143, F1=0.9943 | Val: Loss=0.5468, F1=0.9298 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0037, F1=0.9981 | Val: Loss=0.4324, F1=0.9458 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0188, F1=0.9956 | Val: Loss=0.4552, F1=0.9539 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0103, F1=0.9937 | Val: Loss=0.6066, F1=0.9213 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0067, F1=0.9987 | Val: Loss=0.6011, F1=0.9141 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0095, F1=0.9975 | Val: Loss=0.7246, F1=0.9300 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0012, F1=0.9994 | Val: Loss=0.5616, F1=0.9133 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0005, F1=0.9994 | Val: Loss=0.7185, F1=0.9062 (Per-Sample)
Early stopping triggered after 140 epochs.
Best model restored from epoch 90 with val_f1 0.9539

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7467, F1=0.6757 | Val: Loss=0.4182, F1=0.6658 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0978, F1=0.8979 | Val: Loss=0.1588, F1=0.8582 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0417, F1=0.9753 | Val: Loss=0.2732, F1=0.9389 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0408, F1=0.9801 | Val: Loss=0.3594, F1=0.9295 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0803, F1=0.9855 | Val: Loss=0.3914, F1=0.9115 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0224, F1=0.9911 | Val: Loss=0.5172, F1=0.8891 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0353, F1=0.9847 | Val: Loss=0.4125, F1=0.9174 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0128, F1=0.9937 | Val: Loss=0.3295, F1=0.9297 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0018, F1=0.9987 | Val: Loss=0.5132, F1=0.9313 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0005, F1=0.9994 | Val: Loss=0.4509, F1=0.9317 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0143, F1=0.9968 | Val: Loss=0.4727, F1=0.9317 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0054, F1=0.9987 | Val: Loss=0.4338, F1=0.9384 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0053, F1=0.9981 | Val: Loss=0.3785, F1=0.9384 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0121, F1=0.9975 | Val: Loss=0.4148, F1=0.9317 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0112, F1=0.9987 | Val: Loss=0.4411, F1=0.9468 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0010, F1=0.9987 | Val: Loss=0.4874, F1=0.9468 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0086, F1=0.9975 | Val: Loss=0.4002, F1=0.9317 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0020, F1=0.9987 | Val: Loss=0.3967, F1=0.9360 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0004, F1=0.9994 | Val: Loss=0.4206, F1=0.9539 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0041, F1=0.9968 | Val: Loss=0.4321, F1=0.9468 (Per-Sample)
Epoch 200/300 | Train: Loss=0.0136, F1=0.9975 | Val: Loss=0.5388, F1=0.9394 (Per-Sample)
Early stopping triggered after 204 epochs.
Best model restored from epoch 154 with val_f1 0.9611

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7121, F1=0.6867 | Val: Loss=0.4144, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0884, F1=0.9088 | Val: Loss=0.1491, F1=0.8386 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0443, F1=0.9773 | Val: Loss=0.3240, F1=0.8954 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0595, F1=0.9809 | Val: Loss=0.3416, F1=0.9282 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0203, F1=0.9905 | Val: Loss=0.3942, F1=0.9279 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0296, F1=0.9918 | Val: Loss=0.3818, F1=0.9106 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0340, F1=0.9905 | Val: Loss=0.4890, F1=0.9124 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0084, F1=0.9981 | Val: Loss=0.4125, F1=0.9451 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0107, F1=0.9975 | Val: Loss=0.4813, F1=0.9293 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0136, F1=0.9968 | Val: Loss=0.5930, F1=0.9222 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0009, F1=0.9987 | Val: Loss=0.6173, F1=0.9214 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0030, F1=0.9994 | Val: Loss=0.4930, F1=0.9279 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0086, F1=0.9981 | Val: Loss=0.5865, F1=0.9208 (Per-Sample)
Early stopping triggered after 126 epochs.
Best model restored from epoch 76 with val_f1 0.9456

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7557, F1=0.6460 | Val: Loss=0.2624, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0707, F1=0.9164 | Val: Loss=0.1642, F1=0.9334 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0438, F1=0.9760 | Val: Loss=0.2266, F1=0.9312 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0536, F1=0.9815 | Val: Loss=0.3480, F1=0.9392 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0306, F1=0.9840 | Val: Loss=0.4006, F1=0.9125 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0253, F1=0.9924 | Val: Loss=0.4595, F1=0.9308 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0465, F1=0.9905 | Val: Loss=0.4667, F1=0.9378 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0138, F1=0.9962 | Val: Loss=0.4309, F1=0.9556 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0205, F1=0.9898 | Val: Loss=0.3864, F1=0.9459 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0126, F1=0.9949 | Val: Loss=0.4028, F1=0.9294 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0088, F1=0.9968 | Val: Loss=0.3878, F1=0.9549 (Per-Sample)
Best trial: 9. Best value: -0.958613:  20%|██        | 10/50 [20:18<1:35:32, 143.31s/it]
Early stopping triggered after 103 epochs.
Best model restored from epoch 53 with val_f1 0.9694

Cross-validation score: 0.9586 ± 0.0081
  > Trial 9 Result: Mean F1 = 0.9586
[I 2025-11-10 14:01:54,812] Trial 9 finished with value: -0.9586133797141736 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.001, 'noise_std_dev': 0.1}. Best is trial 9 with value: -0.9586133797141736.

--- Starting Trial 10 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.0
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7199, F1=0.6616 | Val: Loss=0.4363, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2107, F1=0.8993 | Val: Loss=0.2217, F1=0.9293 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1850, F1=0.9587 | Val: Loss=0.2191, F1=0.9290 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1676, F1=0.9819 | Val: Loss=0.2252, F1=0.9407 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1706, F1=0.9786 | Val: Loss=0.2643, F1=0.9383 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1725, F1=0.9793 | Val: Loss=0.2382, F1=0.9334 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1811, F1=0.9790 | Val: Loss=0.2093, F1=0.9487 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1660, F1=0.9912 | Val: Loss=0.2276, F1=0.9590 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1596, F1=0.9956 | Val: Loss=0.2498, F1=0.9407 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1577, F1=0.9949 | Val: Loss=0.2536, F1=0.9407 (Per-Sample)
Early stopping triggered after 94 epochs.
Best model restored from epoch 44 with val_f1 0.9668

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7418, F1=0.6592 | Val: Loss=0.4374, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1957, F1=0.9084 | Val: Loss=0.2608, F1=0.8788 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1744, F1=0.9640 | Val: Loss=0.2468, F1=0.9003 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1671, F1=0.9780 | Val: Loss=0.2585, F1=0.9035 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1616, F1=0.9826 | Val: Loss=0.2516, F1=0.9113 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1563, F1=0.9905 | Val: Loss=0.2584, F1=0.9215 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1565, F1=0.9943 | Val: Loss=0.2541, F1=0.9185 (Per-Sample)
Early stopping triggered after 64 epochs.
Best model restored from epoch 14 with val_f1 0.9351

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7284, F1=0.6708 | Val: Loss=0.4320, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1966, F1=0.9054 | Val: Loss=0.2417, F1=0.8246 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1596, F1=0.9688 | Val: Loss=0.2279, F1=0.8643 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1538, F1=0.9825 | Val: Loss=0.2270, F1=0.9097 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1502, F1=0.9865 | Val: Loss=0.2236, F1=0.9348 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1475, F1=0.9917 | Val: Loss=0.2226, F1=0.9099 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1533, F1=0.9879 | Val: Loss=0.2221, F1=0.9339 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1511, F1=0.9891 | Val: Loss=0.2430, F1=0.9205 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1475, F1=0.9924 | Val: Loss=0.2300, F1=0.9103 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1528, F1=0.9879 | Val: Loss=0.2279, F1=0.9079 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1444, F1=0.9968 | Val: Loss=0.2240, F1=0.9175 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1424, F1=0.9968 | Val: Loss=0.2322, F1=0.9201 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1456, F1=0.9956 | Val: Loss=0.2373, F1=0.9060 (Per-Sample)
Early stopping triggered after 124 epochs.
Best model restored from epoch 74 with val_f1 0.9455

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7280, F1=0.6777 | Val: Loss=0.4715, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1910, F1=0.9028 | Val: Loss=0.2467, F1=0.8448 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1579, F1=0.9622 | Val: Loss=0.2490, F1=0.8947 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1561, F1=0.9764 | Val: Loss=0.2290, F1=0.9279 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1525, F1=0.9792 | Val: Loss=0.2535, F1=0.9019 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1460, F1=0.9851 | Val: Loss=0.2390, F1=0.9212 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1450, F1=0.9943 | Val: Loss=0.2455, F1=0.9285 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1444, F1=0.9962 | Val: Loss=0.2453, F1=0.9125 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1441, F1=0.9943 | Val: Loss=0.2520, F1=0.9034 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1447, F1=0.9943 | Val: Loss=0.2435, F1=0.9197 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1433, F1=0.9918 | Val: Loss=0.2454, F1=0.9125 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1411, F1=0.9955 | Val: Loss=0.2500, F1=0.9052 (Per-Sample)
Early stopping triggered after 112 epochs.
Best model restored from epoch 62 with val_f1 0.9368

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6478, F1=0.6541 | Val: Loss=0.3154, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1994, F1=0.8995 | Val: Loss=0.2043, F1=0.9050 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1663, F1=0.9687 | Val: Loss=0.1974, F1=0.9260 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1600, F1=0.9779 | Val: Loss=0.2375, F1=0.9300 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1576, F1=0.9873 | Val: Loss=0.2397, F1=0.9287 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1589, F1=0.9885 | Val: Loss=0.2433, F1=0.9278 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1538, F1=0.9885 | Val: Loss=0.2437, F1=0.9266 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1522, F1=0.9930 | Val: Loss=0.2397, F1=0.9104 (Per-Sample)
Best trial: 9. Best value: -0.958613:  22%|██▏       | 11/50 [22:06<1:26:14, 132.68s/it]
Early stopping triggered after 77 epochs.
Best model restored from epoch 27 with val_f1 0.9359

Cross-validation score: 0.9440 ± 0.0120
  > Trial 10 Result: Mean F1 = 0.9440
[I 2025-11-10 14:03:43,376] Trial 10 finished with value: -0.9440042035460238 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.0, 'noise_std_dev': 0.1}. Best is trial 9 with value: -0.9586133797141736.

--- Starting Trial 11 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.01
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5825, F1=0.6662 | Val: Loss=0.3727, F1=0.7350 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1059, F1=0.8827 | Val: Loss=0.1450, F1=0.9094 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0760, F1=0.9668 | Val: Loss=0.3293, F1=0.9213 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0857, F1=0.9750 | Val: Loss=0.4738, F1=0.9283 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0301, F1=0.9937 | Val: Loss=0.5601, F1=0.9198 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0631, F1=0.9867 | Val: Loss=0.3470, F1=0.9414 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0626, F1=0.9912 | Val: Loss=0.3958, F1=0.9344 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0391, F1=0.9943 | Val: Loss=0.5489, F1=0.9213 (Per-Sample)
Early stopping triggered after 79 epochs.
Best model restored from epoch 29 with val_f1 0.9488

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5865, F1=0.6729 | Val: Loss=0.3569, F1=0.6749 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1131, F1=0.9095 | Val: Loss=0.2403, F1=0.8527 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1006, F1=0.9407 | Val: Loss=0.3574, F1=0.8716 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0642, F1=0.9828 | Val: Loss=0.4540, F1=0.9289 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0369, F1=0.9911 | Val: Loss=0.6224, F1=0.8961 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0336, F1=0.9937 | Val: Loss=0.6727, F1=0.9100 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0259, F1=0.9924 | Val: Loss=0.5782, F1=0.9220 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0063, F1=0.9987 | Val: Loss=0.6572, F1=0.9309 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0196, F1=0.9968 | Val: Loss=0.6877, F1=0.9114 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0357, F1=0.9880 | Val: Loss=0.4631, F1=0.9026 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0087, F1=0.9956 | Val: Loss=0.4355, F1=0.9394 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0067, F1=0.9981 | Val: Loss=0.4256, F1=0.9375 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0093, F1=0.9956 | Val: Loss=0.4076, F1=0.9376 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0006, F1=0.9994 | Val: Loss=0.5738, F1=0.9309 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0147, F1=0.9981 | Val: Loss=0.6611, F1=0.9368 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0077, F1=0.9987 | Val: Loss=0.5285, F1=0.9454 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0041, F1=0.9987 | Val: Loss=0.6006, F1=0.9300 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0059, F1=0.9994 | Val: Loss=0.6581, F1=0.9227 (Per-Sample)
Early stopping triggered after 172 epochs.
Best model restored from epoch 122 with val_f1 0.9545

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5910, F1=0.6801 | Val: Loss=0.3843, F1=0.6602 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0954, F1=0.9103 | Val: Loss=0.1724, F1=0.8576 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0711, F1=0.9770 | Val: Loss=0.4427, F1=0.9060 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0612, F1=0.9762 | Val: Loss=0.2743, F1=0.9378 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0434, F1=0.9899 | Val: Loss=0.2757, F1=0.9385 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0582, F1=0.9874 | Val: Loss=0.3714, F1=0.9304 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0344, F1=0.9943 | Val: Loss=0.5838, F1=0.9025 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0308, F1=0.9918 | Val: Loss=0.4525, F1=0.9094 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0162, F1=0.9956 | Val: Loss=0.4103, F1=0.9144 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0487, F1=0.9937 | Val: Loss=0.4883, F1=0.9224 (Per-Sample)
Early stopping triggered after 99 epochs.
Best model restored from epoch 49 with val_f1 0.9539

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6065, F1=0.6731 | Val: Loss=0.3889, F1=0.6512 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0815, F1=0.8775 | Val: Loss=0.2151, F1=0.8025 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0724, F1=0.9575 | Val: Loss=0.2239, F1=0.9117 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0925, F1=0.9798 | Val: Loss=0.3342, F1=0.9383 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0478, F1=0.9827 | Val: Loss=0.2758, F1=0.9383 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0585, F1=0.9880 | Val: Loss=0.4086, F1=0.9133 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0295, F1=0.9956 | Val: Loss=0.4133, F1=0.9284 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0427, F1=0.9937 | Val: Loss=0.3516, F1=0.9201 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0087, F1=0.9956 | Val: Loss=0.4445, F1=0.9214 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0253, F1=0.9956 | Val: Loss=0.3893, F1=0.9115 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0142, F1=0.9962 | Val: Loss=0.3477, F1=0.9383 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0079, F1=0.9981 | Val: Loss=0.4440, F1=0.9308 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0266, F1=0.9969 | Val: Loss=0.4024, F1=0.9376 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.4917, F1=0.9301 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0047, F1=0.9994 | Val: Loss=0.5501, F1=0.9222 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.4997, F1=0.9135 (Per-Sample)
Early stopping triggered after 155 epochs.
Best model restored from epoch 105 with val_f1 0.9453

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5887, F1=0.6657 | Val: Loss=0.2385, F1=0.7935 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1071, F1=0.8680 | Val: Loss=0.1880, F1=0.8692 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0587, F1=0.9610 | Val: Loss=0.1695, F1=0.9344 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0568, F1=0.9774 | Val: Loss=0.4480, F1=0.9103 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0200, F1=0.9898 | Val: Loss=0.3495, F1=0.9315 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0209, F1=0.9937 | Val: Loss=0.5116, F1=0.9301 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0242, F1=0.9968 | Val: Loss=0.3438, F1=0.9540 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0025, F1=0.9981 | Val: Loss=0.4902, F1=0.9232 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0111, F1=0.9968 | Val: Loss=0.3750, F1=0.9462 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0012, F1=0.9994 | Val: Loss=0.3031, F1=0.9614 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0229, F1=0.9962 | Val: Loss=0.4354, F1=0.9398 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0100, F1=0.9987 | Val: Loss=0.3814, F1=0.9470 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0089, F1=0.9968 | Val: Loss=0.5172, F1=0.9540 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0194, F1=0.9975 | Val: Loss=0.3469, F1=0.9620 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0128, F1=0.9975 | Val: Loss=0.4307, F1=0.9298 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0132, F1=0.9981 | Val: Loss=0.3257, F1=0.9456 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0222, F1=0.9968 | Val: Loss=0.3298, F1=0.9460 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3981, F1=0.9372 (Per-Sample)
Best trial: 9. Best value: -0.958613:  24%|██▍       | 12/50 [26:17<1:46:41, 168.45s/it]
Early stopping triggered after 177 epochs.
Best model restored from epoch 127 with val_f1 0.9623

Cross-validation score: 0.9530 ± 0.0058
  > Trial 11 Result: Mean F1 = 0.9530
[I 2025-11-10 14:07:53,663] Trial 11 finished with value: -0.9529716917058124 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.01, 'noise_std_dev': 0.0}. Best is trial 9 with value: -0.9586133797141736.

--- Starting Trial 12 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.5
  > l2_lambda: 0.001
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5851, F1=0.6652 | Val: Loss=0.3679, F1=0.7687 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0900, F1=0.9199 | Val: Loss=0.1904, F1=0.9173 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0841, F1=0.9711 | Val: Loss=0.5525, F1=0.9101 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0657, F1=0.9742 | Val: Loss=0.2119, F1=0.9415 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0656, F1=0.9797 | Val: Loss=0.4632, F1=0.9306 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0507, F1=0.9918 | Val: Loss=0.3642, F1=0.9449 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0443, F1=0.9912 | Val: Loss=0.4865, F1=0.9161 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0316, F1=0.9950 | Val: Loss=0.3247, F1=0.9630 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0250, F1=0.9962 | Val: Loss=0.3768, F1=0.9388 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0163, F1=0.9975 | Val: Loss=0.2668, F1=0.9506 (Per-Sample)
Early stopping triggered after 97 epochs.
Best model restored from epoch 47 with val_f1 0.9693

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5842, F1=0.6801 | Val: Loss=0.3572, F1=0.6720 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0974, F1=0.9198 | Val: Loss=0.2960, F1=0.8478 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1295, F1=0.9651 | Val: Loss=0.5770, F1=0.8996 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0562, F1=0.9821 | Val: Loss=0.6088, F1=0.9162 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0622, F1=0.9736 | Val: Loss=0.4351, F1=0.9132 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0452, F1=0.9873 | Val: Loss=0.5454, F1=0.9230 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0354, F1=0.9950 | Val: Loss=0.5279, F1=0.9205 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0183, F1=0.9943 | Val: Loss=0.5311, F1=0.9198 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0083, F1=0.9981 | Val: Loss=0.5499, F1=0.9442 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0542, F1=0.9918 | Val: Loss=0.6278, F1=0.9136 (Per-Sample)
Early stopping triggered after 95 epochs.
Best model restored from epoch 45 with val_f1 0.9530

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5625, F1=0.6810 | Val: Loss=0.3864, F1=0.6496 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0862, F1=0.9183 | Val: Loss=0.2022, F1=0.8495 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0738, F1=0.9551 | Val: Loss=0.3154, F1=0.9071 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0637, F1=0.9854 | Val: Loss=0.3286, F1=0.9217 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0269, F1=0.9911 | Val: Loss=0.5913, F1=0.8900 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0402, F1=0.9879 | Val: Loss=0.4959, F1=0.8900 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0352, F1=0.9918 | Val: Loss=0.4616, F1=0.9151 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0477, F1=0.9912 | Val: Loss=0.3981, F1=0.9227 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0090, F1=0.9956 | Val: Loss=0.4032, F1=0.9402 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0152, F1=0.9968 | Val: Loss=0.3708, F1=0.9285 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0043, F1=0.9987 | Val: Loss=0.4854, F1=0.9120 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.4976, F1=0.9137 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0059, F1=0.9987 | Val: Loss=0.4978, F1=0.9210 (Per-Sample)
Early stopping triggered after 124 epochs.
Best model restored from epoch 74 with val_f1 0.9473

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5814, F1=0.6882 | Val: Loss=0.3947, F1=0.6387 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0968, F1=0.8735 | Val: Loss=0.1651, F1=0.7984 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0632, F1=0.9733 | Val: Loss=0.3369, F1=0.9300 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0323, F1=0.9879 | Val: Loss=0.3528, F1=0.9294 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0721, F1=0.9854 | Val: Loss=0.4705, F1=0.9227 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0188, F1=0.9910 | Val: Loss=0.5297, F1=0.9141 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0096, F1=0.9981 | Val: Loss=0.3772, F1=0.9380 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0321, F1=0.9943 | Val: Loss=0.4188, F1=0.9211 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0221, F1=0.9956 | Val: Loss=0.3897, F1=0.9211 (Per-Sample)
Early stopping triggered after 83 epochs.
Best model restored from epoch 33 with val_f1 0.9464

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6060, F1=0.6446 | Val: Loss=0.2427, F1=0.7935 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0824, F1=0.8687 | Val: Loss=0.1775, F1=0.8592 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1177, F1=0.9665 | Val: Loss=0.1683, F1=0.9345 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0962, F1=0.9730 | Val: Loss=0.1888, F1=0.9446 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0305, F1=0.9885 | Val: Loss=0.3118, F1=0.9463 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0308, F1=0.9905 | Val: Loss=0.3779, F1=0.9472 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0068, F1=0.9968 | Val: Loss=0.4161, F1=0.9475 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0104, F1=0.9968 | Val: Loss=0.5002, F1=0.9398 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0164, F1=0.9975 | Val: Loss=0.3148, F1=0.9474 (Per-Sample)
Best trial: 9. Best value: -0.958613:  26%|██▌       | 13/50 [29:17<1:46:02, 171.97s/it]
Early stopping triggered after 82 epochs.
Best model restored from epoch 32 with val_f1 0.9694

Cross-validation score: 0.9571 ± 0.0103
  > Trial 12 Result: Mean F1 = 0.9571
[I 2025-11-10 14:10:53,709] Trial 12 finished with value: -0.9570878257278155 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.5, 'l2_lambda': 0.001, 'noise_std_dev': 0.1}. Best is trial 9 with value: -0.9586133797141736.

--- Starting Trial 13 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6471, F1=0.6624 | Val: Loss=0.4231, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2142, F1=0.9154 | Val: Loss=0.2452, F1=0.9154 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1791, F1=0.9774 | Val: Loss=0.2563, F1=0.9332 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1786, F1=0.9822 | Val: Loss=0.2713, F1=0.9146 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1691, F1=0.9860 | Val: Loss=0.2659, F1=0.9334 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1656, F1=0.9898 | Val: Loss=0.2523, F1=0.9333 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1666, F1=0.9911 | Val: Loss=0.2486, F1=0.9516 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1776, F1=0.9834 | Val: Loss=0.2748, F1=0.9278 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1619, F1=0.9923 | Val: Loss=0.2505, F1=0.9517 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1645, F1=0.9924 | Val: Loss=0.2539, F1=0.9286 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1606, F1=0.9943 | Val: Loss=0.2210, F1=0.9516 (Per-Sample)
Early stopping triggered after 109 epochs.
Best model restored from epoch 59 with val_f1 0.9613

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6189, F1=0.6771 | Val: Loss=0.4116, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1951, F1=0.8688 | Val: Loss=0.2880, F1=0.8321 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1798, F1=0.9653 | Val: Loss=0.2524, F1=0.9359 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1665, F1=0.9819 | Val: Loss=0.2811, F1=0.9114 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1627, F1=0.9872 | Val: Loss=0.2447, F1=0.9275 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1656, F1=0.9860 | Val: Loss=0.2917, F1=0.9007 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1650, F1=0.9860 | Val: Loss=0.2798, F1=0.9115 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1576, F1=0.9924 | Val: Loss=0.3048, F1=0.8956 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1599, F1=0.9918 | Val: Loss=0.2842, F1=0.9205 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1656, F1=0.9918 | Val: Loss=0.2645, F1=0.9292 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1534, F1=0.9968 | Val: Loss=0.2761, F1=0.9139 (Per-Sample)
Early stopping triggered after 102 epochs.
Best model restored from epoch 52 with val_f1 0.9461

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6592, F1=0.6685 | Val: Loss=0.4358, F1=0.6527 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1904, F1=0.8747 | Val: Loss=0.2359, F1=0.8246 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1808, F1=0.9590 | Val: Loss=0.2245, F1=0.9538 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1598, F1=0.9821 | Val: Loss=0.2323, F1=0.9200 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1569, F1=0.9873 | Val: Loss=0.2459, F1=0.9208 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1502, F1=0.9911 | Val: Loss=0.2523, F1=0.9299 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1543, F1=0.9892 | Val: Loss=0.2451, F1=0.9464 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1489, F1=0.9918 | Val: Loss=0.2199, F1=0.9344 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1478, F1=0.9943 | Val: Loss=0.2215, F1=0.9344 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1477, F1=0.9923 | Val: Loss=0.2240, F1=0.9366 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1551, F1=0.9886 | Val: Loss=0.2358, F1=0.9361 (Per-Sample)
Early stopping triggered after 105 epochs.
Best model restored from epoch 55 with val_f1 0.9772

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6503, F1=0.6670 | Val: Loss=0.4441, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1897, F1=0.8757 | Val: Loss=0.2809, F1=0.8046 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1983, F1=0.9386 | Val: Loss=0.2832, F1=0.8225 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1585, F1=0.9803 | Val: Loss=0.2727, F1=0.9028 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1487, F1=0.9924 | Val: Loss=0.2540, F1=0.9027 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1513, F1=0.9899 | Val: Loss=0.2488, F1=0.9106 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1480, F1=0.9924 | Val: Loss=0.2726, F1=0.9034 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1452, F1=0.9930 | Val: Loss=0.2615, F1=0.9197 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1443, F1=0.9943 | Val: Loss=0.2476, F1=0.9106 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1548, F1=0.9880 | Val: Loss=0.2338, F1=0.9344 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1511, F1=0.9892 | Val: Loss=0.2710, F1=0.9271 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1496, F1=0.9931 | Val: Loss=0.2257, F1=0.9266 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1569, F1=0.9880 | Val: Loss=0.2763, F1=0.8938 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1493, F1=0.9943 | Val: Loss=0.2584, F1=0.9010 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1395, F1=0.9987 | Val: Loss=0.2530, F1=0.9091 (Per-Sample)
Epoch 150/300 | Train: Loss=0.1434, F1=0.9968 | Val: Loss=0.2576, F1=0.9106 (Per-Sample)
Early stopping triggered after 152 epochs.
Best model restored from epoch 102 with val_f1 0.9436

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6063, F1=0.6515 | Val: Loss=0.3078, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2014, F1=0.9211 | Val: Loss=0.2062, F1=0.9227 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1650, F1=0.9813 | Val: Loss=0.2320, F1=0.9240 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1825, F1=0.9706 | Val: Loss=0.2304, F1=0.9301 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1726, F1=0.9778 | Val: Loss=0.2229, F1=0.9159 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1595, F1=0.9860 | Val: Loss=0.2046, F1=0.9363 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1532, F1=0.9930 | Val: Loss=0.2062, F1=0.9366 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1553, F1=0.9911 | Val: Loss=0.2268, F1=0.9390 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1549, F1=0.9904 | Val: Loss=0.2156, F1=0.9461 (Per-Sample)
Best trial: 13. Best value: -0.95925:  28%|██▊       | 14/50 [32:34<1:47:41, 179.47s/it]
Early stopping triggered after 88 epochs.
Best model restored from epoch 38 with val_f1 0.9680

Cross-validation score: 0.9592 ± 0.0128
  > Trial 13 Result: Mean F1 = 0.9592
[I 2025-11-10 14:14:10,529] Trial 13 finished with value: -0.9592498273290561 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 13 with value: -0.9592498273290561.

--- Starting Trial 14 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 20
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6797, F1=0.6692 | Val: Loss=0.4410, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2066, F1=0.9103 | Val: Loss=0.2301, F1=0.9124 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1791, F1=0.9692 | Val: Loss=0.2289, F1=0.9332 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1725, F1=0.9794 | Val: Loss=0.2290, F1=0.9264 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1748, F1=0.9820 | Val: Loss=0.2407, F1=0.9186 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1623, F1=0.9937 | Val: Loss=0.2364, F1=0.9275 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1654, F1=0.9924 | Val: Loss=0.2640, F1=0.9355 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1636, F1=0.9931 | Val: Loss=0.2415, F1=0.9333 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1643, F1=0.9937 | Val: Loss=0.2197, F1=0.9344 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1635, F1=0.9924 | Val: Loss=0.2332, F1=0.9450 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1603, F1=0.9969 | Val: Loss=0.2447, F1=0.9366 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1588, F1=0.9975 | Val: Loss=0.2614, F1=0.9252 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1610, F1=0.9969 | Val: Loss=0.2758, F1=0.9376 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1610, F1=0.9956 | Val: Loss=0.2312, F1=0.9603 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1713, F1=0.9906 | Val: Loss=0.2740, F1=0.9307 (Per-Sample)
Epoch 150/300 | Train: Loss=0.1576, F1=0.9968 | Val: Loss=0.2680, F1=0.9198 (Per-Sample)
Epoch 160/300 | Train: Loss=0.1579, F1=0.9987 | Val: Loss=0.2347, F1=0.9286 (Per-Sample)
Epoch 170/300 | Train: Loss=0.1575, F1=0.9987 | Val: Loss=0.2473, F1=0.9450 (Per-Sample)
Early stopping triggered after 174 epochs.
Best model restored from epoch 124 with val_f1 0.9686

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7066, F1=0.6729 | Val: Loss=0.4486, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1895, F1=0.9167 | Val: Loss=0.2765, F1=0.8792 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1691, F1=0.9657 | Val: Loss=0.2931, F1=0.9205 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1602, F1=0.9865 | Val: Loss=0.2883, F1=0.8955 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1569, F1=0.9864 | Val: Loss=0.2643, F1=0.9186 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1589, F1=0.9905 | Val: Loss=0.3009, F1=0.9062 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1548, F1=0.9956 | Val: Loss=0.2899, F1=0.9041 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1539, F1=0.9962 | Val: Loss=0.2573, F1=0.9359 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1538, F1=0.9956 | Val: Loss=0.2869, F1=0.9102 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1520, F1=0.9975 | Val: Loss=0.2868, F1=0.9271 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1674, F1=0.9867 | Val: Loss=0.2614, F1=0.9351 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1537, F1=0.9968 | Val: Loss=0.2710, F1=0.9185 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1561, F1=0.9969 | Val: Loss=0.2796, F1=0.9102 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1533, F1=0.9987 | Val: Loss=0.2658, F1=0.9186 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1574, F1=0.9962 | Val: Loss=0.2683, F1=0.9274 (Per-Sample)
Epoch 150/300 | Train: Loss=0.1530, F1=0.9987 | Val: Loss=0.2596, F1=0.9185 (Per-Sample)
Early stopping triggered after 158 epochs.
Best model restored from epoch 108 with val_f1 0.9442

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6548, F1=0.6856 | Val: Loss=0.4467, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1807, F1=0.9077 | Val: Loss=0.2312, F1=0.8231 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1569, F1=0.9680 | Val: Loss=0.2390, F1=0.8879 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1531, F1=0.9820 | Val: Loss=0.2430, F1=0.8931 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1512, F1=0.9885 | Val: Loss=0.2243, F1=0.9103 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1529, F1=0.9827 | Val: Loss=0.2249, F1=0.9105 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1464, F1=0.9897 | Val: Loss=0.2348, F1=0.9028 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1500, F1=0.9905 | Val: Loss=0.2224, F1=0.9272 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1505, F1=0.9891 | Val: Loss=0.2394, F1=0.9099 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1456, F1=0.9937 | Val: Loss=0.2245, F1=0.9293 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1425, F1=0.9968 | Val: Loss=0.2357, F1=0.9201 (Per-Sample)
Early stopping triggered after 104 epochs.
Best model restored from epoch 54 with val_f1 0.9384

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7332, F1=0.6717 | Val: Loss=0.4619, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1788, F1=0.8883 | Val: Loss=0.2525, F1=0.8296 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1559, F1=0.9693 | Val: Loss=0.2359, F1=0.8918 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1601, F1=0.9705 | Val: Loss=0.2413, F1=0.9135 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1477, F1=0.9851 | Val: Loss=0.2280, F1=0.9187 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1485, F1=0.9865 | Val: Loss=0.2287, F1=0.9187 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1489, F1=0.9885 | Val: Loss=0.2368, F1=0.9125 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1484, F1=0.9931 | Val: Loss=0.2330, F1=0.9279 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1447, F1=0.9936 | Val: Loss=0.2567, F1=0.9108 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1548, F1=0.9873 | Val: Loss=0.2487, F1=0.9116 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1448, F1=0.9937 | Val: Loss=0.2385, F1=0.9451 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1486, F1=0.9937 | Val: Loss=0.2433, F1=0.9018 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1431, F1=0.9956 | Val: Loss=0.2677, F1=0.9293 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1465, F1=0.9956 | Val: Loss=0.2467, F1=0.9373 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1428, F1=0.9956 | Val: Loss=0.2732, F1=0.9133 (Per-Sample)
Early stopping triggered after 147 epochs.
Best model restored from epoch 97 with val_f1 0.9451

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6850, F1=0.6553 | Val: Loss=0.3123, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1843, F1=0.9090 | Val: Loss=0.2141, F1=0.8991 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1703, F1=0.9596 | Val: Loss=0.2059, F1=0.9227 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1561, F1=0.9811 | Val: Loss=0.1990, F1=0.9511 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1591, F1=0.9826 | Val: Loss=0.2170, F1=0.9260 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1533, F1=0.9832 | Val: Loss=0.2137, F1=0.9343 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1601, F1=0.9846 | Val: Loss=0.2305, F1=0.9191 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1558, F1=0.9911 | Val: Loss=0.2544, F1=0.9241 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1504, F1=0.9897 | Val: Loss=0.2344, F1=0.9219 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1495, F1=0.9923 | Val: Loss=0.2403, F1=0.9240 (Per-Sample)
Best trial: 13. Best value: -0.95925:  30%|███       | 15/50 [35:10<1:40:41, 172.63s/it]
Early stopping triggered after 93 epochs.
Best model restored from epoch 43 with val_f1 0.9531

Cross-validation score: 0.9499 ± 0.0105
  > Trial 14 Result: Mean F1 = 0.9499
[I 2025-11-10 14:16:47,288] Trial 14 finished with value: -0.9498552570744512 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 20, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.2}. Best is trial 13 with value: -0.9592498273290561.

--- Starting Trial 15 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6707, F1=0.6530 | Val: Loss=0.4174, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2088, F1=0.8780 | Val: Loss=0.2083, F1=0.9221 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1784, F1=0.9787 | Val: Loss=0.2540, F1=0.9344 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1923, F1=0.9742 | Val: Loss=0.2498, F1=0.9449 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1679, F1=0.9872 | Val: Loss=0.2354, F1=0.9360 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1875, F1=0.9713 | Val: Loss=0.2192, F1=0.9334 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1701, F1=0.9885 | Val: Loss=0.2239, F1=0.9344 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1673, F1=0.9885 | Val: Loss=0.2252, F1=0.9426 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1646, F1=0.9943 | Val: Loss=0.2479, F1=0.9348 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1619, F1=0.9949 | Val: Loss=0.2434, F1=0.9442 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1636, F1=0.9943 | Val: Loss=0.2630, F1=0.9286 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1624, F1=0.9930 | Val: Loss=0.2521, F1=0.9272 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1629, F1=0.9949 | Val: Loss=0.2792, F1=0.9201 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1570, F1=0.9987 | Val: Loss=0.2336, F1=0.9455 (Per-Sample)
Early stopping triggered after 131 epochs.
Best model restored from epoch 81 with val_f1 0.9456

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6536, F1=0.6770 | Val: Loss=0.4226, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1996, F1=0.9224 | Val: Loss=0.2885, F1=0.9045 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1669, F1=0.9771 | Val: Loss=0.2899, F1=0.9215 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1742, F1=0.9815 | Val: Loss=0.3059, F1=0.8956 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1663, F1=0.9846 | Val: Loss=0.2959, F1=0.9027 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1653, F1=0.9860 | Val: Loss=0.2370, F1=0.9468 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1714, F1=0.9855 | Val: Loss=0.3117, F1=0.9201 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1585, F1=0.9924 | Val: Loss=0.2862, F1=0.9287 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1523, F1=0.9962 | Val: Loss=0.2782, F1=0.9201 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1534, F1=0.9956 | Val: Loss=0.2910, F1=0.9025 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1677, F1=0.9880 | Val: Loss=0.2836, F1=0.9285 (Per-Sample)
Early stopping triggered after 100 epochs.
Best model restored from epoch 50 with val_f1 0.9468

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6373, F1=0.6904 | Val: Loss=0.4277, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1846, F1=0.9215 | Val: Loss=0.2394, F1=0.8830 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1746, F1=0.9697 | Val: Loss=0.2500, F1=0.8937 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1537, F1=0.9873 | Val: Loss=0.2350, F1=0.9306 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1647, F1=0.9754 | Val: Loss=0.2386, F1=0.9035 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1555, F1=0.9873 | Val: Loss=0.2522, F1=0.9122 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1546, F1=0.9860 | Val: Loss=0.2520, F1=0.9016 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1507, F1=0.9924 | Val: Loss=0.2523, F1=0.9214 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1464, F1=0.9943 | Val: Loss=0.2369, F1=0.9450 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1482, F1=0.9931 | Val: Loss=0.2566, F1=0.9030 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1519, F1=0.9899 | Val: Loss=0.2543, F1=0.9113 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1525, F1=0.9925 | Val: Loss=0.2382, F1=0.9024 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1431, F1=0.9975 | Val: Loss=0.2534, F1=0.9017 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1428, F1=0.9975 | Val: Loss=0.2310, F1=0.9200 (Per-Sample)
Early stopping triggered after 130 epochs.
Best model restored from epoch 80 with val_f1 0.9450

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6551, F1=0.6801 | Val: Loss=0.4617, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1842, F1=0.8777 | Val: Loss=0.2474, F1=0.8138 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1758, F1=0.9613 | Val: Loss=0.2392, F1=0.9118 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1583, F1=0.9727 | Val: Loss=0.2934, F1=0.8855 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1458, F1=0.9924 | Val: Loss=0.2477, F1=0.9215 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1423, F1=0.9949 | Val: Loss=0.2352, F1=0.9368 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1452, F1=0.9924 | Val: Loss=0.2385, F1=0.9189 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1409, F1=0.9981 | Val: Loss=0.2381, F1=0.9368 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1518, F1=0.9892 | Val: Loss=0.2351, F1=0.9194 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1444, F1=0.9943 | Val: Loss=0.2299, F1=0.9360 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1446, F1=0.9956 | Val: Loss=0.2550, F1=0.8992 (Per-Sample)
Early stopping triggered after 103 epochs.
Best model restored from epoch 53 with val_f1 0.9525

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6546, F1=0.6503 | Val: Loss=0.2988, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2053, F1=0.8734 | Val: Loss=0.2291, F1=0.8633 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1670, F1=0.9710 | Val: Loss=0.2260, F1=0.9373 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1632, F1=0.9814 | Val: Loss=0.1816, F1=0.9589 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1723, F1=0.9789 | Val: Loss=0.1943, F1=0.9605 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1629, F1=0.9860 | Val: Loss=0.2315, F1=0.9312 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1540, F1=0.9911 | Val: Loss=0.2000, F1=0.9431 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1518, F1=0.9918 | Val: Loss=0.1963, F1=0.9373 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1611, F1=0.9866 | Val: Loss=0.2217, F1=0.9383 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1589, F1=0.9898 | Val: Loss=0.2151, F1=0.9386 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1633, F1=0.9847 | Val: Loss=0.2409, F1=0.9309 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1528, F1=0.9950 | Val: Loss=0.2444, F1=0.9166 (Per-Sample)
Best trial: 13. Best value: -0.95925:  32%|███▏      | 16/50 [38:37<1:43:42, 183.02s/it]
Early stopping triggered after 118 epochs.
Best model restored from epoch 68 with val_f1 0.9619

Cross-validation score: 0.9504 ± 0.0063
  > Trial 15 Result: Mean F1 = 0.9504
[I 2025-11-10 14:20:14,444] Trial 15 finished with value: -0.9503625983221541 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.001, 'noise_std_dev': 0.0}. Best is trial 13 with value: -0.9592498273290561.

--- Starting Trial 16 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.0
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6587, F1=0.6570 | Val: Loss=0.4168, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2216, F1=0.8628 | Val: Loss=0.2329, F1=0.9163 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1863, F1=0.9744 | Val: Loss=0.2684, F1=0.9248 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1705, F1=0.9814 | Val: Loss=0.2194, F1=0.9524 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1717, F1=0.9840 | Val: Loss=0.2102, F1=0.9426 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1695, F1=0.9873 | Val: Loss=0.2209, F1=0.9414 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1585, F1=0.9956 | Val: Loss=0.2189, F1=0.9515 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1660, F1=0.9905 | Val: Loss=0.2256, F1=0.9515 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1591, F1=0.9949 | Val: Loss=0.2210, F1=0.9515 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1619, F1=0.9937 | Val: Loss=0.2435, F1=0.9340 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1605, F1=0.9949 | Val: Loss=0.2212, F1=0.9333 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1622, F1=0.9956 | Val: Loss=0.2207, F1=0.9426 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1677, F1=0.9879 | Val: Loss=0.2736, F1=0.9296 (Per-Sample)
Early stopping triggered after 124 epochs.
Best model restored from epoch 74 with val_f1 0.9689

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6593, F1=0.6829 | Val: Loss=0.4266, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1915, F1=0.9275 | Val: Loss=0.3072, F1=0.8543 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1770, F1=0.9630 | Val: Loss=0.3133, F1=0.8907 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1687, F1=0.9799 | Val: Loss=0.3177, F1=0.8961 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1735, F1=0.9810 | Val: Loss=0.2995, F1=0.9279 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1599, F1=0.9898 | Val: Loss=0.3020, F1=0.9127 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1585, F1=0.9911 | Val: Loss=0.3093, F1=0.9142 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1596, F1=0.9911 | Val: Loss=0.2991, F1=0.9070 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1629, F1=0.9924 | Val: Loss=0.3174, F1=0.9055 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1580, F1=0.9936 | Val: Loss=0.2872, F1=0.9300 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1558, F1=0.9956 | Val: Loss=0.2795, F1=0.9207 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1541, F1=0.9956 | Val: Loss=0.2695, F1=0.9289 (Per-Sample)
Early stopping triggered after 113 epochs.
Best model restored from epoch 63 with val_f1 0.9375

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6451, F1=0.6810 | Val: Loss=0.4334, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1900, F1=0.8719 | Val: Loss=0.2513, F1=0.8084 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1603, F1=0.9781 | Val: Loss=0.2433, F1=0.8961 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1720, F1=0.9704 | Val: Loss=0.2297, F1=0.8987 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1540, F1=0.9846 | Val: Loss=0.2126, F1=0.9539 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1596, F1=0.9855 | Val: Loss=0.2381, F1=0.9212 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1494, F1=0.9878 | Val: Loss=0.2177, F1=0.9271 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1478, F1=0.9937 | Val: Loss=0.2350, F1=0.9267 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1467, F1=0.9943 | Val: Loss=0.2274, F1=0.9194 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1441, F1=0.9943 | Val: Loss=0.2071, F1=0.9444 (Per-Sample)
Early stopping triggered after 90 epochs.
Best model restored from epoch 40 with val_f1 0.9539

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6576, F1=0.6835 | Val: Loss=0.4521, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1911, F1=0.8747 | Val: Loss=0.2777, F1=0.7912 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1671, F1=0.9706 | Val: Loss=0.2730, F1=0.9047 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1701, F1=0.9743 | Val: Loss=0.2793, F1=0.8843 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1526, F1=0.9873 | Val: Loss=0.2502, F1=0.8921 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1523, F1=0.9911 | Val: Loss=0.2624, F1=0.9213 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1506, F1=0.9912 | Val: Loss=0.2480, F1=0.9299 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1444, F1=0.9962 | Val: Loss=0.2306, F1=0.9285 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1613, F1=0.9855 | Val: Loss=0.2603, F1=0.9042 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1548, F1=0.9898 | Val: Loss=0.2631, F1=0.9206 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1464, F1=0.9937 | Val: Loss=0.2550, F1=0.9293 (Per-Sample)
Early stopping triggered after 102 epochs.
Best model restored from epoch 52 with val_f1 0.9447

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6592, F1=0.6363 | Val: Loss=0.3108, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2014, F1=0.8711 | Val: Loss=0.2129, F1=0.8991 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1658, F1=0.9786 | Val: Loss=0.2450, F1=0.9201 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1599, F1=0.9834 | Val: Loss=0.2585, F1=0.9081 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1684, F1=0.9796 | Val: Loss=0.2160, F1=0.9390 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1707, F1=0.9789 | Val: Loss=0.2025, F1=0.9468 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1660, F1=0.9835 | Val: Loss=0.2158, F1=0.9446 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1542, F1=0.9924 | Val: Loss=0.2021, F1=0.9440 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1524, F1=0.9937 | Val: Loss=0.2112, F1=0.9372 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1564, F1=0.9918 | Val: Loss=0.2279, F1=0.9330 (Per-Sample)
Best trial: 13. Best value: -0.95925:  34%|███▍      | 17/50 [41:42<1:40:58, 183.58s/it]
Early stopping triggered after 92 epochs.
Best model restored from epoch 42 with val_f1 0.9544

Cross-validation score: 0.9519 ± 0.0106
  > Trial 16 Result: Mean F1 = 0.9519
[I 2025-11-10 14:23:19,328] Trial 16 finished with value: -0.9518825337310897 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.0, 'noise_std_dev': 0.1}. Best is trial 13 with value: -0.9592498273290561.

--- Starting Trial 17 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 20
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.05
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.01
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6545, F1=0.6641 | Val: Loss=0.4139, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1417, F1=0.9225 | Val: Loss=0.1863, F1=0.9208 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1209, F1=0.9685 | Val: Loss=0.1723, F1=0.9407 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1077, F1=0.9820 | Val: Loss=0.2042, F1=0.9433 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1059, F1=0.9854 | Val: Loss=0.2122, F1=0.9260 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0975, F1=0.9930 | Val: Loss=0.2218, F1=0.9334 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0950, F1=0.9956 | Val: Loss=0.2302, F1=0.9359 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0992, F1=0.9949 | Val: Loss=0.2260, F1=0.9252 (Per-Sample)
Early stopping triggered after 77 epochs.
Best model restored from epoch 27 with val_f1 0.9606

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7161, F1=0.6813 | Val: Loss=0.4201, F1=0.6807 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1436, F1=0.9085 | Val: Loss=0.2428, F1=0.8672 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1155, F1=0.9692 | Val: Loss=0.2466, F1=0.8961 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1082, F1=0.9753 | Val: Loss=0.2382, F1=0.8940 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1025, F1=0.9866 | Val: Loss=0.2551, F1=0.9053 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1035, F1=0.9825 | Val: Loss=0.2334, F1=0.9202 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1031, F1=0.9867 | Val: Loss=0.2502, F1=0.8974 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0966, F1=0.9911 | Val: Loss=0.2807, F1=0.9129 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0945, F1=0.9930 | Val: Loss=0.2554, F1=0.9227 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0977, F1=0.9943 | Val: Loss=0.2396, F1=0.9317 (Per-Sample)
Early stopping triggered after 94 epochs.
Best model restored from epoch 44 with val_f1 0.9443

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7452, F1=0.6755 | Val: Loss=0.4133, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1348, F1=0.9089 | Val: Loss=0.1918, F1=0.8166 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1084, F1=0.9668 | Val: Loss=0.1928, F1=0.8896 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0901, F1=0.9837 | Val: Loss=0.2067, F1=0.9161 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0958, F1=0.9879 | Val: Loss=0.2128, F1=0.8989 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0932, F1=0.9898 | Val: Loss=0.1989, F1=0.9234 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0959, F1=0.9886 | Val: Loss=0.1941, F1=0.9248 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0911, F1=0.9898 | Val: Loss=0.1985, F1=0.9314 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0898, F1=0.9930 | Val: Loss=0.2019, F1=0.9137 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0880, F1=0.9937 | Val: Loss=0.1991, F1=0.9296 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0861, F1=0.9956 | Val: Loss=0.2001, F1=0.9230 (Per-Sample)
Early stopping triggered after 104 epochs.
Best model restored from epoch 54 with val_f1 0.9470

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7407, F1=0.6850 | Val: Loss=0.4201, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1215, F1=0.9131 | Val: Loss=0.2125, F1=0.8328 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1099, F1=0.9679 | Val: Loss=0.1890, F1=0.9010 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1037, F1=0.9794 | Val: Loss=0.2216, F1=0.9212 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0973, F1=0.9846 | Val: Loss=0.2152, F1=0.9206 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0909, F1=0.9892 | Val: Loss=0.2152, F1=0.9266 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0986, F1=0.9873 | Val: Loss=0.1995, F1=0.9188 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0911, F1=0.9930 | Val: Loss=0.2389, F1=0.9114 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0945, F1=0.9898 | Val: Loss=0.2080, F1=0.9446 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0896, F1=0.9937 | Val: Loss=0.2177, F1=0.9284 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0898, F1=0.9879 | Val: Loss=0.2289, F1=0.9219 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0907, F1=0.9950 | Val: Loss=0.2091, F1=0.9446 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0930, F1=0.9918 | Val: Loss=0.2264, F1=0.9293 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0925, F1=0.9918 | Val: Loss=0.2318, F1=0.9138 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1011, F1=0.9880 | Val: Loss=0.2588, F1=0.9053 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0842, F1=0.9981 | Val: Loss=0.1960, F1=0.9366 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0841, F1=0.9975 | Val: Loss=0.2067, F1=0.9446 (Per-Sample)
Early stopping triggered after 166 epochs.
Best model restored from epoch 116 with val_f1 0.9446

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7463, F1=0.6473 | Val: Loss=0.2960, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1387, F1=0.9031 | Val: Loss=0.1429, F1=0.9227 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1137, F1=0.9694 | Val: Loss=0.1548, F1=0.9308 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1057, F1=0.9854 | Val: Loss=0.1683, F1=0.9248 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0991, F1=0.9885 | Val: Loss=0.1942, F1=0.9195 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1012, F1=0.9861 | Val: Loss=0.1670, F1=0.9461 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0941, F1=0.9891 | Val: Loss=0.1731, F1=0.9521 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0913, F1=0.9943 | Val: Loss=0.1914, F1=0.9378 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0877, F1=0.9955 | Val: Loss=0.1925, F1=0.9372 (Per-Sample)
Best trial: 13. Best value: -0.95925:  36%|███▌      | 18/50 [43:50<1:28:58, 166.83s/it]
Early stopping triggered after 89 epochs.
Best model restored from epoch 39 with val_f1 0.9602

Cross-validation score: 0.9513 ± 0.0075
  > Trial 17 Result: Mean F1 = 0.9513
[I 2025-11-10 14:25:27,155] Trial 17 finished with value: -0.9513203693663842 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 20, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.05, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.01, 'noise_std_dev': 0.0}. Best is trial 13 with value: -0.9592498273290561.

--- Starting Trial 18 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 40
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 32
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.0
  > l2_lambda: 0.0001
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5636, F1=0.6815 | Val: Loss=0.3957, F1=0.7637 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2162, F1=0.8785 | Val: Loss=0.2072, F1=0.8995 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1855, F1=0.9537 | Val: Loss=0.1984, F1=0.9486 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1788, F1=0.9678 | Val: Loss=0.1919, F1=0.9380 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1770, F1=0.9788 | Val: Loss=0.1840, F1=0.9564 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1759, F1=0.9815 | Val: Loss=0.2043, F1=0.9494 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1692, F1=0.9834 | Val: Loss=0.2020, F1=0.9506 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1703, F1=0.9866 | Val: Loss=0.2002, F1=0.9571 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1652, F1=0.9905 | Val: Loss=0.2234, F1=0.9356 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1632, F1=0.9917 | Val: Loss=0.2103, F1=0.9564 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1616, F1=0.9937 | Val: Loss=0.2148, F1=0.9486 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1614, F1=0.9962 | Val: Loss=0.2292, F1=0.9521 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1684, F1=0.9918 | Val: Loss=0.1954, F1=0.9487 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1616, F1=0.9956 | Val: Loss=0.2289, F1=0.9420 (Per-Sample)
Early stopping triggered after 134 epochs.
Best model restored from epoch 84 with val_f1 0.9746

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5700, F1=0.6883 | Val: Loss=0.4003, F1=0.6985 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2061, F1=0.8701 | Val: Loss=0.2575, F1=0.8700 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1843, F1=0.9442 | Val: Loss=0.2436, F1=0.9444 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1762, F1=0.9714 | Val: Loss=0.2514, F1=0.9201 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1613, F1=0.9844 | Val: Loss=0.2337, F1=0.9360 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1574, F1=0.9878 | Val: Loss=0.2420, F1=0.9217 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1652, F1=0.9852 | Val: Loss=0.2642, F1=0.9237 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1661, F1=0.9867 | Val: Loss=0.2446, F1=0.9269 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1625, F1=0.9893 | Val: Loss=0.2450, F1=0.9287 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1567, F1=0.9930 | Val: Loss=0.2444, F1=0.9456 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1591, F1=0.9937 | Val: Loss=0.2308, F1=0.9359 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1511, F1=0.9981 | Val: Loss=0.2504, F1=0.9363 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1577, F1=0.9962 | Val: Loss=0.2507, F1=0.9289 (Per-Sample)
Early stopping triggered after 126 epochs.
Best model restored from epoch 76 with val_f1 0.9608

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5603, F1=0.6996 | Val: Loss=0.4148, F1=0.6870 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1926, F1=0.9114 | Val: Loss=0.2598, F1=0.8269 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1786, F1=0.9484 | Val: Loss=0.2614, F1=0.8823 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1638, F1=0.9747 | Val: Loss=0.2368, F1=0.9020 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1611, F1=0.9803 | Val: Loss=0.2278, F1=0.8990 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1624, F1=0.9776 | Val: Loss=0.2374, F1=0.9212 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1525, F1=0.9892 | Val: Loss=0.2570, F1=0.8684 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1475, F1=0.9904 | Val: Loss=0.2383, F1=0.9137 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1500, F1=0.9911 | Val: Loss=0.2524, F1=0.9119 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1447, F1=0.9936 | Val: Loss=0.2270, F1=0.9093 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1415, F1=0.9968 | Val: Loss=0.2301, F1=0.9187 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1495, F1=0.9917 | Val: Loss=0.2620, F1=0.8908 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1524, F1=0.9892 | Val: Loss=0.2215, F1=0.9266 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1544, F1=0.9892 | Val: Loss=0.2610, F1=0.9366 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1427, F1=0.9943 | Val: Loss=0.2471, F1=0.9212 (Per-Sample)
Epoch 150/300 | Train: Loss=0.1464, F1=0.9943 | Val: Loss=0.2426, F1=0.9210 (Per-Sample)
Epoch 160/300 | Train: Loss=0.1459, F1=0.9956 | Val: Loss=0.2559, F1=0.9007 (Per-Sample)
Epoch 170/300 | Train: Loss=0.1435, F1=0.9968 | Val: Loss=0.2475, F1=0.9013 (Per-Sample)
Epoch 180/300 | Train: Loss=0.1464, F1=0.9943 | Val: Loss=0.2413, F1=0.9195 (Per-Sample)
Early stopping triggered after 180 epochs.
Best model restored from epoch 130 with val_f1 0.9366

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.4923, F1=0.6994 | Val: Loss=0.4285, F1=0.6517 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1841, F1=0.9004 | Val: Loss=0.2442, F1=0.8409 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1708, F1=0.9467 | Val: Loss=0.2439, F1=0.8718 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1596, F1=0.9715 | Val: Loss=0.2409, F1=0.9286 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1532, F1=0.9820 | Val: Loss=0.2453, F1=0.9002 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1474, F1=0.9898 | Val: Loss=0.2364, F1=0.9102 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1528, F1=0.9854 | Val: Loss=0.2459, F1=0.9010 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1485, F1=0.9924 | Val: Loss=0.2433, F1=0.9067 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1470, F1=0.9949 | Val: Loss=0.2530, F1=0.9193 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1460, F1=0.9924 | Val: Loss=0.2411, F1=0.9091 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1438, F1=0.9936 | Val: Loss=0.2476, F1=0.9187 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1433, F1=0.9956 | Val: Loss=0.2388, F1=0.9187 (Per-Sample)
Early stopping triggered after 116 epochs.
Best model restored from epoch 66 with val_f1 0.9368

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6401, F1=0.6263 | Val: Loss=0.3093, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2038, F1=0.8799 | Val: Loss=0.2156, F1=0.8598 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1752, F1=0.9445 | Val: Loss=0.2182, F1=0.9114 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1634, F1=0.9693 | Val: Loss=0.2061, F1=0.9227 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1591, F1=0.9743 | Val: Loss=0.2110, F1=0.9227 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1626, F1=0.9835 | Val: Loss=0.1914, F1=0.9511 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1576, F1=0.9898 | Val: Loss=0.2348, F1=0.9378 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1556, F1=0.9885 | Val: Loss=0.2153, F1=0.9530 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1594, F1=0.9867 | Val: Loss=0.2220, F1=0.9461 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1580, F1=0.9872 | Val: Loss=0.2334, F1=0.9225 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1580, F1=0.9899 | Val: Loss=0.2231, F1=0.9461 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1531, F1=0.9923 | Val: Loss=0.2258, F1=0.9330 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1554, F1=0.9912 | Val: Loss=0.2227, F1=0.9375 (Per-Sample)
Best trial: 13. Best value: -0.95925:  38%|███▊      | 19/50 [47:22<1:33:14, 180.46s/it]
Early stopping triggered after 123 epochs.
Best model restored from epoch 73 with val_f1 0.9605

Cross-validation score: 0.9538 ± 0.0149
  > Trial 18 Result: Mean F1 = 0.9538
[I 2025-11-10 14:28:59,383] Trial 18 finished with value: -0.9538469501599135 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 40, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 32, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.0, 'l2_lambda': 0.0001, 'noise_std_dev': 0.1}. Best is trial 13 with value: -0.9592498273290561.

--- Starting Trial 19 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 64
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.3
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5909, F1=0.6639 | Val: Loss=0.3988, F1=0.7327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2059, F1=0.9243 | Val: Loss=0.2223, F1=0.9433 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1786, F1=0.9733 | Val: Loss=0.2354, F1=0.9259 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1709, F1=0.9846 | Val: Loss=0.2131, F1=0.9344 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1703, F1=0.9860 | Val: Loss=0.1876, F1=0.9594 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1731, F1=0.9854 | Val: Loss=0.2190, F1=0.9442 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1647, F1=0.9924 | Val: Loss=0.2331, F1=0.9333 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1620, F1=0.9949 | Val: Loss=0.2007, F1=0.9414 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1596, F1=0.9968 | Val: Loss=0.2210, F1=0.9408 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1635, F1=0.9931 | Val: Loss=0.2506, F1=0.9436 (Per-Sample)
Early stopping triggered after 91 epochs.
Best model restored from epoch 41 with val_f1 0.9595

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6668, F1=0.6670 | Val: Loss=0.4144, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1890, F1=0.9128 | Val: Loss=0.2678, F1=0.8690 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1673, F1=0.9716 | Val: Loss=0.2664, F1=0.9186 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1646, F1=0.9806 | Val: Loss=0.2693, F1=0.9113 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1711, F1=0.9762 | Val: Loss=0.2766, F1=0.8933 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1633, F1=0.9846 | Val: Loss=0.2639, F1=0.9206 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1599, F1=0.9885 | Val: Loss=0.2576, F1=0.9445 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1584, F1=0.9924 | Val: Loss=0.2853, F1=0.9183 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1600, F1=0.9899 | Val: Loss=0.2552, F1=0.9439 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1553, F1=0.9962 | Val: Loss=0.2853, F1=0.9187 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1607, F1=0.9918 | Val: Loss=0.2792, F1=0.9271 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1556, F1=0.9930 | Val: Loss=0.2582, F1=0.9353 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1581, F1=0.9937 | Val: Loss=0.2864, F1=0.9271 (Per-Sample)
Early stopping triggered after 125 epochs.
Best model restored from epoch 75 with val_f1 0.9524

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6368, F1=0.6922 | Val: Loss=0.4151, F1=0.6499 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1816, F1=0.9205 | Val: Loss=0.2428, F1=0.8151 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1599, F1=0.9672 | Val: Loss=0.2360, F1=0.8767 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1563, F1=0.9770 | Val: Loss=0.2250, F1=0.9017 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1562, F1=0.9834 | Val: Loss=0.2273, F1=0.9201 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1459, F1=0.9891 | Val: Loss=0.2119, F1=0.9245 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1513, F1=0.9918 | Val: Loss=0.2267, F1=0.9293 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1525, F1=0.9805 | Val: Loss=0.2321, F1=0.9079 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1537, F1=0.9880 | Val: Loss=0.2218, F1=0.9175 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1481, F1=0.9918 | Val: Loss=0.2169, F1=0.9272 (Per-Sample)
Early stopping triggered after 99 epochs.
Best model restored from epoch 49 with val_f1 0.9437

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6283, F1=0.6823 | Val: Loss=0.4194, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1815, F1=0.8766 | Val: Loss=0.2556, F1=0.7928 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1526, F1=0.9667 | Val: Loss=0.2367, F1=0.8909 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1501, F1=0.9831 | Val: Loss=0.2385, F1=0.9106 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1501, F1=0.9824 | Val: Loss=0.2412, F1=0.8827 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1452, F1=0.9898 | Val: Loss=0.2537, F1=0.8927 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1471, F1=0.9924 | Val: Loss=0.2587, F1=0.9106 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1514, F1=0.9892 | Val: Loss=0.2717, F1=0.9034 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1471, F1=0.9898 | Val: Loss=0.2400, F1=0.9285 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1428, F1=0.9916 | Val: Loss=0.2663, F1=0.9134 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1487, F1=0.9917 | Val: Loss=0.2402, F1=0.9115 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1446, F1=0.9924 | Val: Loss=0.2552, F1=0.9018 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1447, F1=0.9956 | Val: Loss=0.2679, F1=0.9106 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1503, F1=0.9931 | Val: Loss=0.2559, F1=0.9056 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1404, F1=0.9981 | Val: Loss=0.2405, F1=0.9281 (Per-Sample)
Epoch 150/300 | Train: Loss=0.1441, F1=0.9956 | Val: Loss=0.2570, F1=0.9125 (Per-Sample)
Early stopping triggered after 153 epochs.
Best model restored from epoch 103 with val_f1 0.9373

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6323, F1=0.6566 | Val: Loss=0.3038, F1=0.7785 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1854, F1=0.9166 | Val: Loss=0.2171, F1=0.9138 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1612, F1=0.9734 | Val: Loss=0.2175, F1=0.9195 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1574, F1=0.9824 | Val: Loss=0.2306, F1=0.9357 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1522, F1=0.9878 | Val: Loss=0.2260, F1=0.9312 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1631, F1=0.9808 | Val: Loss=0.2329, F1=0.9136 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1507, F1=0.9930 | Val: Loss=0.2389, F1=0.9378 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1500, F1=0.9937 | Val: Loss=0.2462, F1=0.9219 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1561, F1=0.9892 | Val: Loss=0.2598, F1=0.9173 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1624, F1=0.9807 | Val: Loss=0.2138, F1=0.9428 (Per-Sample)
Best trial: 13. Best value: -0.95925:  40%|████      | 20/50 [49:50<1:25:16, 170.53s/it]
Early stopping triggered after 92 epochs.
Best model restored from epoch 42 with val_f1 0.9602

Cross-validation score: 0.9506 ± 0.0089
  > Trial 19 Result: Mean F1 = 0.9506
[I 2025-11-10 14:31:26,776] Trial 19 finished with value: -0.9506206193674342 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.3, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 13 with value: -0.9592498273290561.

--- Starting Trial 20 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.05
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7085, F1=0.6329 | Val: Loss=0.3982, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1662, F1=0.8746 | Val: Loss=0.2186, F1=0.9085 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1311, F1=0.9684 | Val: Loss=0.1680, F1=0.9524 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1201, F1=0.9815 | Val: Loss=0.1814, F1=0.9463 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1077, F1=0.9860 | Val: Loss=0.1752, F1=0.9517 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1100, F1=0.9899 | Val: Loss=0.2043, F1=0.9538 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0971, F1=0.9962 | Val: Loss=0.2071, F1=0.9545 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1043, F1=0.9949 | Val: Loss=0.2597, F1=0.9420 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1026, F1=0.9944 | Val: Loss=0.2369, F1=0.9308 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0947, F1=0.9949 | Val: Loss=0.2000, F1=0.9442 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0932, F1=0.9981 | Val: Loss=0.2142, F1=0.9431 (Per-Sample)
Early stopping triggered after 108 epochs.
Best model restored from epoch 58 with val_f1 0.9626

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6650, F1=0.6651 | Val: Loss=0.4218, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1483, F1=0.8772 | Val: Loss=0.3001, F1=0.8501 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1302, F1=0.9655 | Val: Loss=0.2397, F1=0.9291 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1159, F1=0.9782 | Val: Loss=0.2336, F1=0.9242 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1121, F1=0.9817 | Val: Loss=0.2420, F1=0.9217 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1066, F1=0.9866 | Val: Loss=0.2457, F1=0.9216 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0953, F1=0.9917 | Val: Loss=0.2587, F1=0.9454 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0921, F1=0.9962 | Val: Loss=0.2495, F1=0.9454 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0925, F1=0.9962 | Val: Loss=0.2287, F1=0.9382 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0903, F1=0.9981 | Val: Loss=0.2381, F1=0.9368 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0964, F1=0.9956 | Val: Loss=0.2684, F1=0.9285 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0959, F1=0.9956 | Val: Loss=0.2473, F1=0.9375 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0893, F1=0.9987 | Val: Loss=0.2504, F1=0.9454 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0887, F1=0.9987 | Val: Loss=0.2299, F1=0.9374 (Per-Sample)
Early stopping triggered after 136 epochs.
Best model restored from epoch 86 with val_f1 0.9528

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6678, F1=0.6488 | Val: Loss=0.4061, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1392, F1=0.9130 | Val: Loss=0.2185, F1=0.8953 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1132, F1=0.9743 | Val: Loss=0.1882, F1=0.9293 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1034, F1=0.9860 | Val: Loss=0.1968, F1=0.9137 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1028, F1=0.9868 | Val: Loss=0.1737, F1=0.9231 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0978, F1=0.9911 | Val: Loss=0.1861, F1=0.9296 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1006, F1=0.9899 | Val: Loss=0.2017, F1=0.9166 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1002, F1=0.9867 | Val: Loss=0.1705, F1=0.9346 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1033, F1=0.9918 | Val: Loss=0.1875, F1=0.9365 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0936, F1=0.9911 | Val: Loss=0.1806, F1=0.9297 (Per-Sample)
Early stopping triggered after 92 epochs.
Best model restored from epoch 42 with val_f1 0.9549

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6350, F1=0.6913 | Val: Loss=0.4358, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1515, F1=0.8716 | Val: Loss=0.2315, F1=0.8038 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1019, F1=0.9815 | Val: Loss=0.1789, F1=0.9439 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1147, F1=0.9800 | Val: Loss=0.1846, F1=0.9358 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1145, F1=0.9805 | Val: Loss=0.1995, F1=0.9372 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1104, F1=0.9835 | Val: Loss=0.2071, F1=0.9206 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0889, F1=0.9949 | Val: Loss=0.2257, F1=0.9212 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0886, F1=0.9956 | Val: Loss=0.1844, F1=0.9358 (Per-Sample)
Early stopping triggered after 75 epochs.
Best model restored from epoch 25 with val_f1 0.9526

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6325, F1=0.6510 | Val: Loss=0.2750, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1424, F1=0.8866 | Val: Loss=0.1736, F1=0.9066 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1098, F1=0.9753 | Val: Loss=0.1759, F1=0.9273 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1132, F1=0.9782 | Val: Loss=0.1671, F1=0.9440 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1073, F1=0.9840 | Val: Loss=0.2109, F1=0.9167 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1025, F1=0.9879 | Val: Loss=0.1722, F1=0.9437 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0973, F1=0.9918 | Val: Loss=0.1955, F1=0.9398 (Per-Sample)
Best trial: 13. Best value: -0.95925:  42%|████▏     | 21/50 [52:41<1:22:29, 170.66s/it]
Early stopping triggered after 69 epochs.
Best model restored from epoch 19 with val_f1 0.9528

Cross-validation score: 0.9551 ± 0.0038
  > Trial 20 Result: Mean F1 = 0.9551
[I 2025-11-10 14:34:17,723] Trial 20 finished with value: -0.9551378453471034 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.05, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 13 with value: -0.9592498273290561.

--- Starting Trial 21 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6091, F1=0.6627 | Val: Loss=0.3714, F1=0.7350 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0986, F1=0.8677 | Val: Loss=0.1463, F1=0.9075 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1113, F1=0.9706 | Val: Loss=0.4941, F1=0.9220 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0800, F1=0.9783 | Val: Loss=0.4165, F1=0.9344 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0832, F1=0.9786 | Val: Loss=0.5243, F1=0.9057 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0672, F1=0.9906 | Val: Loss=0.3907, F1=0.9360 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0299, F1=0.9937 | Val: Loss=0.5294, F1=0.9223 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0358, F1=0.9905 | Val: Loss=0.3520, F1=0.9515 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0189, F1=0.9962 | Val: Loss=0.5834, F1=0.9308 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0126, F1=0.9975 | Val: Loss=0.4016, F1=0.9388 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0070, F1=0.9975 | Val: Loss=0.2992, F1=0.9392 (Per-Sample)
Early stopping triggered after 105 epochs.
Best model restored from epoch 55 with val_f1 0.9611

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5890, F1=0.6752 | Val: Loss=0.3633, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1147, F1=0.8948 | Val: Loss=0.2940, F1=0.8227 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0701, F1=0.9712 | Val: Loss=0.3186, F1=0.9271 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0683, F1=0.9756 | Val: Loss=0.7359, F1=0.8643 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0535, F1=0.9854 | Val: Loss=0.4165, F1=0.9374 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0379, F1=0.9905 | Val: Loss=0.5912, F1=0.9136 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0362, F1=0.9912 | Val: Loss=0.7023, F1=0.8893 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0088, F1=0.9962 | Val: Loss=0.6158, F1=0.9218 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0305, F1=0.9937 | Val: Loss=0.5722, F1=0.9393 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0167, F1=0.9949 | Val: Loss=0.6620, F1=0.9159 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0089, F1=0.9987 | Val: Loss=0.6169, F1=0.9394 (Per-Sample)
Early stopping triggered after 101 epochs.
Best model restored from epoch 51 with val_f1 0.9624

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5783, F1=0.6800 | Val: Loss=0.3946, F1=0.7079 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1018, F1=0.8842 | Val: Loss=0.2026, F1=0.8244 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0837, F1=0.9756 | Val: Loss=0.3619, F1=0.9073 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0944, F1=0.9658 | Val: Loss=0.2789, F1=0.9101 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0346, F1=0.9905 | Val: Loss=0.2932, F1=0.9385 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0271, F1=0.9924 | Val: Loss=0.3602, F1=0.9216 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0180, F1=0.9937 | Val: Loss=0.3092, F1=0.9541 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0089, F1=0.9962 | Val: Loss=0.3987, F1=0.9363 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0052, F1=0.9994 | Val: Loss=0.3495, F1=0.9455 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0114, F1=0.9987 | Val: Loss=0.3199, F1=0.9544 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0153, F1=0.9975 | Val: Loss=0.3123, F1=0.9693 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0180, F1=0.9962 | Val: Loss=0.2825, F1=0.9526 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0010, F1=0.9994 | Val: Loss=0.5110, F1=0.9392 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.4710, F1=0.9126 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0082, F1=0.9987 | Val: Loss=0.4519, F1=0.9378 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0417, F1=0.9899 | Val: Loss=0.4177, F1=0.9226 (Per-Sample)
Early stopping triggered after 150 epochs.
Best model restored from epoch 100 with val_f1 0.9693

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5964, F1=0.6795 | Val: Loss=0.3835, F1=0.6415 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0984, F1=0.8822 | Val: Loss=0.1846, F1=0.8006 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1360, F1=0.9549 | Val: Loss=0.1680, F1=0.8849 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0748, F1=0.9823 | Val: Loss=0.3314, F1=0.9210 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0596, F1=0.9893 | Val: Loss=0.4111, F1=0.9208 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0307, F1=0.9937 | Val: Loss=0.4245, F1=0.9457 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0131, F1=0.9956 | Val: Loss=0.4260, F1=0.9460 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0492, F1=0.9937 | Val: Loss=0.5173, F1=0.9059 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0032, F1=0.9981 | Val: Loss=0.4587, F1=0.9379 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0150, F1=0.9975 | Val: Loss=0.5039, F1=0.9211 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0556, F1=0.9893 | Val: Loss=0.3760, F1=0.9366 (Per-Sample)
Early stopping triggered after 109 epochs.
Best model restored from epoch 59 with val_f1 0.9527

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5685, F1=0.6584 | Val: Loss=0.2486, F1=0.8129 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1089, F1=0.9202 | Val: Loss=0.1339, F1=0.9432 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0640, F1=0.9675 | Val: Loss=0.3640, F1=0.9197 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0673, F1=0.9789 | Val: Loss=0.2922, F1=0.9552 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0615, F1=0.9867 | Val: Loss=0.3101, F1=0.9475 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0336, F1=0.9937 | Val: Loss=0.4301, F1=0.9331 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0553, F1=0.9918 | Val: Loss=0.2812, F1=0.9440 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0293, F1=0.9930 | Val: Loss=0.2612, F1=0.9606 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0286, F1=0.9931 | Val: Loss=0.4080, F1=0.9235 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0220, F1=0.9962 | Val: Loss=0.3048, F1=0.9428 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0351, F1=0.9956 | Val: Loss=0.3608, F1=0.9366 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0053, F1=0.9981 | Val: Loss=0.3457, F1=0.9543 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.4040, F1=0.9543 (Per-Sample)
Best trial: 21. Best value: -0.963:  44%|████▍     | 22/50 [56:15<1:25:48, 183.87s/it]  
Early stopping triggered after 124 epochs.
Best model restored from epoch 74 with val_f1 0.9695

Cross-validation score: 0.9630 ± 0.0062
  > Trial 21 Result: Mean F1 = 0.9630
[I 2025-11-10 14:37:52,411] Trial 21 finished with value: -0.9629996290867942 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 22 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5966, F1=0.6598 | Val: Loss=0.3649, F1=0.7214 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1212, F1=0.8776 | Val: Loss=0.1479, F1=0.8975 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1287, F1=0.9720 | Val: Loss=0.3265, F1=0.9689 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0879, F1=0.9704 | Val: Loss=0.3611, F1=0.9259 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0426, F1=0.9880 | Val: Loss=0.3353, F1=0.9451 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0606, F1=0.9899 | Val: Loss=0.4084, F1=0.9451 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0487, F1=0.9912 | Val: Loss=0.4275, F1=0.9295 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0209, F1=0.9956 | Val: Loss=0.3575, F1=0.9341 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0542, F1=0.9899 | Val: Loss=0.4068, F1=0.9451 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0129, F1=0.9975 | Val: Loss=0.3499, F1=0.9341 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0047, F1=0.9994 | Val: Loss=0.4704, F1=0.9367 (Per-Sample)
Early stopping triggered after 107 epochs.
Best model restored from epoch 57 with val_f1 0.9698

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6239, F1=0.6638 | Val: Loss=0.3537, F1=0.6880 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0633, F1=0.9245 | Val: Loss=0.2252, F1=0.8975 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0587, F1=0.9706 | Val: Loss=0.6752, F1=0.8939 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1229, F1=0.9672 | Val: Loss=0.3302, F1=0.9299 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0603, F1=0.9822 | Val: Loss=0.5986, F1=0.9019 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0880, F1=0.9836 | Val: Loss=0.5865, F1=0.9061 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0258, F1=0.9937 | Val: Loss=0.6574, F1=0.9172 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0166, F1=0.9962 | Val: Loss=0.5625, F1=0.9151 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0316, F1=0.9943 | Val: Loss=0.6284, F1=0.9223 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0076, F1=0.9994 | Val: Loss=0.6185, F1=0.9136 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0100, F1=0.9987 | Val: Loss=0.6403, F1=0.9251 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0137, F1=0.9987 | Val: Loss=0.5540, F1=0.9384 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0082, F1=0.9987 | Val: Loss=0.5094, F1=0.9449 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0219, F1=0.9949 | Val: Loss=0.5971, F1=0.9195 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0289, F1=0.9937 | Val: Loss=0.4715, F1=0.9545 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0100, F1=0.9968 | Val: Loss=0.3240, F1=0.9464 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0214, F1=0.9956 | Val: Loss=0.5077, F1=0.9223 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0099, F1=0.9968 | Val: Loss=0.5775, F1=0.9254 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0102, F1=0.9975 | Val: Loss=0.4765, F1=0.9398 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.6268, F1=0.9407 (Per-Sample)
Early stopping triggered after 195 epochs.
Best model restored from epoch 145 with val_f1 0.9630

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5980, F1=0.6705 | Val: Loss=0.3887, F1=0.6560 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0851, F1=0.9349 | Val: Loss=0.2000, F1=0.8783 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0812, F1=0.9717 | Val: Loss=0.3224, F1=0.9326 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0442, F1=0.9872 | Val: Loss=0.5138, F1=0.9159 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0783, F1=0.9736 | Val: Loss=0.2366, F1=0.9462 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0291, F1=0.9892 | Val: Loss=0.4091, F1=0.9218 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0388, F1=0.9924 | Val: Loss=0.4535, F1=0.9398 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0158, F1=0.9968 | Val: Loss=0.6046, F1=0.9251 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0223, F1=0.9943 | Val: Loss=0.3052, F1=0.9367 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0209, F1=0.9956 | Val: Loss=0.4004, F1=0.9198 (Per-Sample)
Early stopping triggered after 93 epochs.
Best model restored from epoch 43 with val_f1 0.9537

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5516, F1=0.6893 | Val: Loss=0.3869, F1=0.6371 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0661, F1=0.9311 | Val: Loss=0.1557, F1=0.8270 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0989, F1=0.9589 | Val: Loss=0.2119, F1=0.9048 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0643, F1=0.9795 | Val: Loss=0.3256, F1=0.9295 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0375, F1=0.9898 | Val: Loss=0.4646, F1=0.9224 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0488, F1=0.9874 | Val: Loss=0.5356, F1=0.9151 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0355, F1=0.9950 | Val: Loss=0.4742, F1=0.9227 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0272, F1=0.9950 | Val: Loss=0.3910, F1=0.9140 (Per-Sample)
Early stopping triggered after 79 epochs.
Best model restored from epoch 29 with val_f1 0.9381

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6307, F1=0.6484 | Val: Loss=0.2619, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1122, F1=0.8812 | Val: Loss=0.1343, F1=0.8805 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0683, F1=0.9569 | Val: Loss=0.2252, F1=0.9179 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0416, F1=0.9853 | Val: Loss=0.3836, F1=0.9136 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0916, F1=0.9822 | Val: Loss=0.3951, F1=0.9532 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0409, F1=0.9886 | Val: Loss=0.3584, F1=0.9301 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0395, F1=0.9911 | Val: Loss=0.4659, F1=0.9321 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0108, F1=0.9943 | Val: Loss=0.3348, F1=0.9366 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0115, F1=0.9968 | Val: Loss=0.4356, F1=0.9390 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0023, F1=0.9975 | Val: Loss=0.5022, F1=0.9236 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0109, F1=0.9981 | Val: Loss=0.4632, F1=0.9236 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0020, F1=0.9987 | Val: Loss=0.4818, F1=0.9390 (Per-Sample)
Best trial: 21. Best value: -0.963:  46%|████▌     | 23/50 [59:49<1:26:43, 192.73s/it]
Early stopping triggered after 115 epochs.
Best model restored from epoch 65 with val_f1 0.9615

Cross-validation score: 0.9572 ± 0.0109
  > Trial 22 Result: Mean F1 = 0.9572
[I 2025-11-10 14:41:25,789] Trial 22 finished with value: -0.9572041897538421 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 23 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 32
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6564, F1=0.6604 | Val: Loss=0.4013, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0922, F1=0.8565 | Val: Loss=0.1615, F1=0.8997 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1189, F1=0.9652 | Val: Loss=0.4430, F1=0.9133 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0313, F1=0.9860 | Val: Loss=0.5898, F1=0.9191 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0819, F1=0.9836 | Val: Loss=0.2829, F1=0.9414 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0354, F1=0.9924 | Val: Loss=0.5562, F1=0.9183 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0141, F1=0.9962 | Val: Loss=0.3231, F1=0.9524 (Per-Sample)
Early stopping triggered after 65 epochs.
Best model restored from epoch 15 with val_f1 0.9604

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6885, F1=0.6636 | Val: Loss=0.4748, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0959, F1=0.8602 | Val: Loss=0.2958, F1=0.8544 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0716, F1=0.9687 | Val: Loss=0.4199, F1=0.9292 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0608, F1=0.9847 | Val: Loss=0.5492, F1=0.9133 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0412, F1=0.9904 | Val: Loss=0.5121, F1=0.9205 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0125, F1=0.9956 | Val: Loss=0.6715, F1=0.9285 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0183, F1=0.9962 | Val: Loss=0.5790, F1=0.9274 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0247, F1=0.9956 | Val: Loss=0.5796, F1=0.9359 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0157, F1=0.9968 | Val: Loss=0.5931, F1=0.9187 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0076, F1=0.9968 | Val: Loss=0.6042, F1=0.9281 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0327, F1=0.9956 | Val: Loss=0.6180, F1=0.9289 (Per-Sample)
Early stopping triggered after 108 epochs.
Best model restored from epoch 58 with val_f1 0.9534

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5638, F1=0.6825 | Val: Loss=0.4205, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0885, F1=0.8741 | Val: Loss=0.1662, F1=0.8175 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0793, F1=0.9715 | Val: Loss=0.4422, F1=0.8843 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0717, F1=0.9763 | Val: Loss=0.6102, F1=0.8864 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0338, F1=0.9912 | Val: Loss=0.5239, F1=0.9204 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0373, F1=0.9937 | Val: Loss=0.6734, F1=0.8935 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0624, F1=0.9829 | Val: Loss=0.6224, F1=0.8928 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0153, F1=0.9968 | Val: Loss=0.5502, F1=0.9065 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0070, F1=0.9987 | Val: Loss=0.5942, F1=0.8923 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0161, F1=0.9956 | Val: Loss=0.4171, F1=0.9141 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0251, F1=0.9962 | Val: Loss=0.6587, F1=0.8913 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0029, F1=0.9994 | Val: Loss=0.6027, F1=0.9056 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0218, F1=0.9975 | Val: Loss=0.6489, F1=0.9072 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0206, F1=0.9956 | Val: Loss=0.5169, F1=0.9091 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0427, F1=0.9918 | Val: Loss=0.5330, F1=0.9167 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0135, F1=0.9975 | Val: Loss=0.4347, F1=0.9403 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0047, F1=0.9987 | Val: Loss=0.4520, F1=0.9321 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0068, F1=0.9981 | Val: Loss=0.4502, F1=0.9225 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0019, F1=0.9987 | Val: Loss=0.5612, F1=0.9403 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0065, F1=0.9994 | Val: Loss=0.4616, F1=0.9309 (Per-Sample)
Epoch 200/300 | Train: Loss=0.0093, F1=0.9994 | Val: Loss=0.5134, F1=0.9300 (Per-Sample)
Early stopping triggered after 205 epochs.
Best model restored from epoch 155 with val_f1 0.9469

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6421, F1=0.6749 | Val: Loss=0.4670, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1041, F1=0.8714 | Val: Loss=0.1892, F1=0.8010 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0981, F1=0.9661 | Val: Loss=0.3502, F1=0.9114 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0705, F1=0.9810 | Val: Loss=0.4428, F1=0.9201 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0235, F1=0.9918 | Val: Loss=0.4725, F1=0.9284 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0318, F1=0.9937 | Val: Loss=0.4705, F1=0.9296 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0109, F1=0.9975 | Val: Loss=0.5247, F1=0.9213 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0091, F1=0.9981 | Val: Loss=0.4155, F1=0.9379 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0169, F1=0.9968 | Val: Loss=0.3959, F1=0.9279 (Per-Sample)
Early stopping triggered after 88 epochs.
Best model restored from epoch 38 with val_f1 0.9379

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6559, F1=0.6507 | Val: Loss=0.2759, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1176, F1=0.8534 | Val: Loss=0.1412, F1=0.8692 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0875, F1=0.9680 | Val: Loss=0.2132, F1=0.9300 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0572, F1=0.9841 | Val: Loss=0.3116, F1=0.9540 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0622, F1=0.9841 | Val: Loss=0.4049, F1=0.9213 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0444, F1=0.9911 | Val: Loss=0.4152, F1=0.9219 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0038, F1=0.9942 | Val: Loss=0.6606, F1=0.9013 (Per-Sample)
Best trial: 21. Best value: -0.963:  48%|████▊     | 24/50 [1:02:53<1:22:26, 190.24s/it]
Early stopping triggered after 69 epochs.
Best model restored from epoch 19 with val_f1 0.9602

Cross-validation score: 0.9518 ± 0.0085
  > Trial 23 Result: Mean F1 = 0.9518
[I 2025-11-10 14:44:30,227] Trial 23 finished with value: -0.9517624688449613 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 32, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 24 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6284, F1=0.6604 | Val: Loss=0.3709, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1028, F1=0.9143 | Val: Loss=0.1283, F1=0.9346 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0652, F1=0.9711 | Val: Loss=0.2238, F1=0.9495 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0749, F1=0.9775 | Val: Loss=0.3415, F1=0.9515 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0397, F1=0.9943 | Val: Loss=0.4615, F1=0.9442 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0246, F1=0.9950 | Val: Loss=0.4666, F1=0.9363 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0230, F1=0.9943 | Val: Loss=0.5207, F1=0.9295 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0290, F1=0.9943 | Val: Loss=0.4348, F1=0.9426 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0113, F1=0.9987 | Val: Loss=0.5891, F1=0.9367 (Per-Sample)
Early stopping triggered after 87 epochs.
Best model restored from epoch 37 with val_f1 0.9604

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6699, F1=0.6538 | Val: Loss=0.3940, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0824, F1=0.9074 | Val: Loss=0.3219, F1=0.8836 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0991, F1=0.9500 | Val: Loss=0.3042, F1=0.9201 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0470, F1=0.9841 | Val: Loss=0.5062, F1=0.8972 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0308, F1=0.9873 | Val: Loss=0.6182, F1=0.9037 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0305, F1=0.9956 | Val: Loss=0.5476, F1=0.9386 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0228, F1=0.9956 | Val: Loss=0.4803, F1=0.9285 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0268, F1=0.9956 | Val: Loss=0.6334, F1=0.9197 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0083, F1=0.9981 | Val: Loss=0.5155, F1=0.9445 (Per-Sample)
Early stopping triggered after 82 epochs.
Best model restored from epoch 32 with val_f1 0.9461

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5855, F1=0.6938 | Val: Loss=0.3897, F1=0.6602 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0857, F1=0.8888 | Val: Loss=0.1632, F1=0.8175 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0889, F1=0.9735 | Val: Loss=0.4768, F1=0.8852 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0431, F1=0.9872 | Val: Loss=0.2209, F1=0.9247 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0423, F1=0.9905 | Val: Loss=0.2974, F1=0.9364 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0297, F1=0.9937 | Val: Loss=0.3321, F1=0.9361 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0216, F1=0.9911 | Val: Loss=0.3872, F1=0.9379 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0336, F1=0.9943 | Val: Loss=0.5164, F1=0.9474 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0183, F1=0.9956 | Val: Loss=0.4359, F1=0.9215 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0366, F1=0.9893 | Val: Loss=0.2898, F1=0.9193 (Per-Sample)
Early stopping triggered after 96 epochs.
Best model restored from epoch 46 with val_f1 0.9537

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6714, F1=0.6782 | Val: Loss=0.4492, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0798, F1=0.8794 | Val: Loss=0.2494, F1=0.8222 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0725, F1=0.9701 | Val: Loss=0.3076, F1=0.9034 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0526, F1=0.9880 | Val: Loss=0.3236, F1=0.9272 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0352, F1=0.9899 | Val: Loss=0.4058, F1=0.8940 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0203, F1=0.9956 | Val: Loss=0.4245, F1=0.9214 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0300, F1=0.9924 | Val: Loss=0.3936, F1=0.9129 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0287, F1=0.9924 | Val: Loss=0.3998, F1=0.9194 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0296, F1=0.9937 | Val: Loss=0.3954, F1=0.9366 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0311, F1=0.9950 | Val: Loss=0.3703, F1=0.9457 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0014, F1=0.9994 | Val: Loss=0.4056, F1=0.9206 (Per-Sample)
Early stopping triggered after 102 epochs.
Best model restored from epoch 52 with val_f1 0.9524

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6318, F1=0.6490 | Val: Loss=0.2496, F1=0.7935 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0798, F1=0.8742 | Val: Loss=0.2735, F1=0.8690 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0990, F1=0.9601 | Val: Loss=0.2155, F1=0.9366 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0461, F1=0.9809 | Val: Loss=0.3763, F1=0.9316 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0245, F1=0.9930 | Val: Loss=0.4265, F1=0.9330 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0373, F1=0.9918 | Val: Loss=0.5250, F1=0.9166 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0272, F1=0.9924 | Val: Loss=0.3922, F1=0.9470 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0072, F1=0.9975 | Val: Loss=0.5567, F1=0.9315 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0087, F1=0.9975 | Val: Loss=0.4446, F1=0.9390 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0083, F1=0.9981 | Val: Loss=0.5759, F1=0.9334 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0120, F1=0.9975 | Val: Loss=0.2720, F1=0.9372 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0026, F1=0.9981 | Val: Loss=0.4212, F1=0.9544 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0079, F1=0.9975 | Val: Loss=0.3815, F1=0.9544 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0041, F1=0.9987 | Val: Loss=0.3997, F1=0.9623 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0330, F1=0.9968 | Val: Loss=0.4495, F1=0.9623 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0135, F1=0.9987 | Val: Loss=0.5122, F1=0.9556 (Per-Sample)
Best trial: 21. Best value: -0.963:  50%|█████     | 25/50 [1:05:49<1:17:26, 185.85s/it]
Early stopping triggered after 152 epochs.
Best model restored from epoch 102 with val_f1 0.9623

Cross-validation score: 0.9550 ± 0.0059
  > Trial 24 Result: Mean F1 = 0.9550
[I 2025-11-10 14:47:25,838] Trial 24 finished with value: -0.9549938941777529 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 25 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.5
  > l2_lambda: 0.001
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6061, F1=0.6642 | Val: Loss=0.4055, F1=0.7327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2142, F1=0.9008 | Val: Loss=0.2548, F1=0.8901 (Per-Sample)
Epoch  20/300 | Train: Loss=0.2246, F1=0.9312 | Val: Loss=0.2186, F1=0.9344 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1790, F1=0.9755 | Val: Loss=0.2202, F1=0.9455 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1734, F1=0.9848 | Val: Loss=0.2412, F1=0.9354 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1848, F1=0.9763 | Val: Loss=0.2419, F1=0.9360 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1720, F1=0.9848 | Val: Loss=0.2558, F1=0.9272 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1635, F1=0.9911 | Val: Loss=0.2385, F1=0.9367 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1703, F1=0.9912 | Val: Loss=0.2565, F1=0.9260 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1792, F1=0.9855 | Val: Loss=0.2357, F1=0.9367 (Per-Sample)
Early stopping triggered after 94 epochs.
Best model restored from epoch 44 with val_f1 0.9604

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5958, F1=0.6735 | Val: Loss=0.4059, F1=0.6662 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1989, F1=0.9227 | Val: Loss=0.2803, F1=0.9104 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1862, F1=0.9608 | Val: Loss=0.2360, F1=0.9445 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1991, F1=0.9480 | Val: Loss=0.2831, F1=0.8915 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1707, F1=0.9808 | Val: Loss=0.2957, F1=0.9030 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1715, F1=0.9821 | Val: Loss=0.2736, F1=0.9118 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1580, F1=0.9924 | Val: Loss=0.2797, F1=0.9285 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1591, F1=0.9918 | Val: Loss=0.3137, F1=0.9151 (Per-Sample)
Early stopping triggered after 73 epochs.
Best model restored from epoch 23 with val_f1 0.9458

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5610, F1=0.6981 | Val: Loss=0.4244, F1=0.6459 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1847, F1=0.9073 | Val: Loss=0.2541, F1=0.8451 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1728, F1=0.9650 | Val: Loss=0.2352, F1=0.9067 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1737, F1=0.9631 | Val: Loss=0.2622, F1=0.9023 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1622, F1=0.9841 | Val: Loss=0.2763, F1=0.8876 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1667, F1=0.9776 | Val: Loss=0.2400, F1=0.9001 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1451, F1=0.9930 | Val: Loss=0.2463, F1=0.9538 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1505, F1=0.9892 | Val: Loss=0.2236, F1=0.9341 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1473, F1=0.9936 | Val: Loss=0.2261, F1=0.9453 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1447, F1=0.9949 | Val: Loss=0.2423, F1=0.9293 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1432, F1=0.9968 | Val: Loss=0.2283, F1=0.9534 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1493, F1=0.9911 | Val: Loss=0.2358, F1=0.9539 (Per-Sample)
Early stopping triggered after 112 epochs.
Best model restored from epoch 62 with val_f1 0.9611

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5916, F1=0.6884 | Val: Loss=0.4354, F1=0.6545 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1942, F1=0.8883 | Val: Loss=0.2481, F1=0.8580 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1628, F1=0.9721 | Val: Loss=0.2465, F1=0.8911 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1512, F1=0.9845 | Val: Loss=0.2630, F1=0.8955 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1554, F1=0.9880 | Val: Loss=0.2301, F1=0.9190 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1498, F1=0.9892 | Val: Loss=0.2451, F1=0.9117 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1525, F1=0.9879 | Val: Loss=0.2617, F1=0.9187 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1615, F1=0.9750 | Val: Loss=0.2703, F1=0.8930 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1483, F1=0.9930 | Val: Loss=0.2784, F1=0.8827 (Per-Sample)
Early stopping triggered after 82 epochs.
Best model restored from epoch 32 with val_f1 0.9360

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6188, F1=0.6424 | Val: Loss=0.3021, F1=0.7804 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1940, F1=0.9123 | Val: Loss=0.2012, F1=0.9159 (Per-Sample)
Epoch  20/300 | Train: Loss=0.2063, F1=0.9392 | Val: Loss=0.2294, F1=0.9227 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1634, F1=0.9773 | Val: Loss=0.2120, F1=0.9451 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1624, F1=0.9813 | Val: Loss=0.2394, F1=0.9065 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1622, F1=0.9840 | Val: Loss=0.2280, F1=0.9295 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1584, F1=0.9885 | Val: Loss=0.2520, F1=0.9068 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1546, F1=0.9911 | Val: Loss=0.2245, F1=0.9394 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1528, F1=0.9911 | Val: Loss=0.2262, F1=0.9277 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1502, F1=0.9949 | Val: Loss=0.2107, F1=0.9386 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1470, F1=0.9981 | Val: Loss=0.2325, F1=0.9137 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1660, F1=0.9861 | Val: Loss=0.2142, F1=0.9446 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1493, F1=0.9956 | Val: Loss=0.1932, F1=0.9510 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1501, F1=0.9968 | Val: Loss=0.2223, F1=0.9300 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1501, F1=0.9962 | Val: Loss=0.2192, F1=0.9298 (Per-Sample)
Best trial: 21. Best value: -0.963:  52%|█████▏    | 26/50 [1:09:00<1:15:00, 187.51s/it]
Early stopping triggered after 148 epochs.
Best model restored from epoch 98 with val_f1 0.9614

Cross-validation score: 0.9529 ± 0.0103
  > Trial 25 Result: Mean F1 = 0.9529
[I 2025-11-10 14:50:37,227] Trial 25 finished with value: -0.9529462681090684 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.5, 'l2_lambda': 0.001, 'noise_std_dev': 0.1}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 26 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.01
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6939, F1=0.6881 | Val: Loss=0.4127, F1=0.7488 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1084, F1=0.8714 | Val: Loss=0.0945, F1=0.9077 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0544, F1=0.9518 | Val: Loss=0.0979, F1=0.9318 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0297, F1=0.9786 | Val: Loss=0.1353, F1=0.9301 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0145, F1=0.9879 | Val: Loss=0.2698, F1=0.9344 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0166, F1=0.9956 | Val: Loss=0.3391, F1=0.9352 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0344, F1=0.9937 | Val: Loss=0.3271, F1=0.9341 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0078, F1=0.9987 | Val: Loss=0.3364, F1=0.9259 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0259, F1=0.9962 | Val: Loss=0.3010, F1=0.9515 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0071, F1=0.9981 | Val: Loss=0.2913, F1=0.9426 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0058, F1=0.9994 | Val: Loss=0.3478, F1=0.9414 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0030, F1=0.9987 | Val: Loss=0.2745, F1=0.9494 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0002, F1=0.9994 | Val: Loss=0.3137, F1=0.9354 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0072, F1=0.9981 | Val: Loss=0.3464, F1=0.9426 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0001, F1=0.9994 | Val: Loss=0.3017, F1=0.9344 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0108, F1=0.9994 | Val: Loss=0.2941, F1=0.9426 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3428, F1=0.9426 (Per-Sample)
Early stopping triggered after 164 epochs.
Best model restored from epoch 114 with val_f1 0.9604

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6968, F1=0.7203 | Val: Loss=0.3686, F1=0.7131 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1208, F1=0.8795 | Val: Loss=0.1819, F1=0.8758 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0578, F1=0.9426 | Val: Loss=0.1540, F1=0.8992 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0326, F1=0.9719 | Val: Loss=0.2805, F1=0.9111 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0404, F1=0.9800 | Val: Loss=0.4910, F1=0.9107 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0311, F1=0.9853 | Val: Loss=0.3523, F1=0.9444 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0141, F1=0.9930 | Val: Loss=0.4334, F1=0.9117 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0321, F1=0.9911 | Val: Loss=0.5797, F1=0.9403 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0042, F1=0.9968 | Val: Loss=0.6310, F1=0.9220 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0088, F1=0.9975 | Val: Loss=0.6124, F1=0.9374 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0091, F1=0.9962 | Val: Loss=0.4837, F1=0.9451 (Per-Sample)
Early stopping triggered after 108 epochs.
Best model restored from epoch 58 with val_f1 0.9536

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5982, F1=0.7200 | Val: Loss=0.4365, F1=0.6953 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0951, F1=0.8800 | Val: Loss=0.1734, F1=0.8266 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0352, F1=0.9588 | Val: Loss=0.1914, F1=0.8658 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0184, F1=0.9824 | Val: Loss=0.2497, F1=0.8961 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0246, F1=0.9826 | Val: Loss=0.3314, F1=0.9227 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0314, F1=0.9873 | Val: Loss=0.4098, F1=0.9326 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0056, F1=0.9955 | Val: Loss=0.4407, F1=0.9242 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0070, F1=0.9975 | Val: Loss=0.4338, F1=0.9381 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0109, F1=0.9937 | Val: Loss=0.4682, F1=0.9096 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0010, F1=0.9994 | Val: Loss=0.5810, F1=0.9245 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0244, F1=0.9962 | Val: Loss=0.5389, F1=0.9245 (Per-Sample)
Early stopping triggered after 106 epochs.
Best model restored from epoch 56 with val_f1 0.9398

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6706, F1=0.7042 | Val: Loss=0.4514, F1=0.6371 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0855, F1=0.8934 | Val: Loss=0.1944, F1=0.7833 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0670, F1=0.9498 | Val: Loss=0.1823, F1=0.8749 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0348, F1=0.9800 | Val: Loss=0.2180, F1=0.9026 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0327, F1=0.9860 | Val: Loss=0.4266, F1=0.9153 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0242, F1=0.9851 | Val: Loss=0.3537, F1=0.9133 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0252, F1=0.9950 | Val: Loss=0.4554, F1=0.9213 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0300, F1=0.9898 | Val: Loss=0.4273, F1=0.9069 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0276, F1=0.9917 | Val: Loss=0.3961, F1=0.9213 (Per-Sample)
Early stopping triggered after 87 epochs.
Best model restored from epoch 37 with val_f1 0.9370

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6461, F1=0.6806 | Val: Loss=0.2498, F1=0.7785 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1119, F1=0.8621 | Val: Loss=0.1610, F1=0.8791 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0845, F1=0.9315 | Val: Loss=0.1482, F1=0.9260 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0434, F1=0.9716 | Val: Loss=0.2544, F1=0.9159 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0303, F1=0.9822 | Val: Loss=0.2574, F1=0.9440 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0217, F1=0.9911 | Val: Loss=0.4646, F1=0.9175 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0095, F1=0.9975 | Val: Loss=0.4082, F1=0.9530 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0155, F1=0.9943 | Val: Loss=0.4468, F1=0.9460 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0071, F1=0.9956 | Val: Loss=0.4382, F1=0.9357 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0441, F1=0.9943 | Val: Loss=0.5105, F1=0.9428 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0089, F1=0.9956 | Val: Loss=0.6088, F1=0.9175 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0243, F1=0.9981 | Val: Loss=0.4659, F1=0.9511 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0032, F1=0.9981 | Val: Loss=0.4935, F1=0.9520 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0235, F1=0.9969 | Val: Loss=0.4934, F1=0.9614 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0132, F1=0.9975 | Val: Loss=0.4967, F1=0.9601 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0004, F1=1.0000 | Val: Loss=0.5198, F1=0.9511 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.5152, F1=0.9443 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0033, F1=0.9981 | Val: Loss=0.5854, F1=0.9280 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0050, F1=0.9981 | Val: Loss=0.4968, F1=0.9241 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.4638, F1=0.9415 (Per-Sample)
Best trial: 21. Best value: -0.963:  54%|█████▍    | 27/50 [1:11:25<1:06:57, 174.65s/it]
Early stopping triggered after 192 epochs.
Best model restored from epoch 142 with val_f1 0.9687

Cross-validation score: 0.9519 ± 0.0120
  > Trial 26 Result: Mean F1 = 0.9519
[I 2025-11-10 14:53:01,879] Trial 26 finished with value: -0.9519095907004177 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.01, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 27 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 0.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.3
  > l2_lambda: 0.0
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Best trial: 21. Best value: -0.963:  56%|█████▌    | 28/50 [1:11:25<44:53, 122.42s/it]  
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.0): [0. 0. 0.]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
NaN loss at batch 0
NaN loss at batch 1
NaN loss at batch 2
NaN loss at batch 3
NaN loss at batch 4
NaN loss at batch 5
NaN loss at batch 6
NaN loss at batch 7
NaN loss at batch 8
NaN loss at batch 9
NaN loss at batch 10
NaN loss at batch 11
NaN loss at batch 12
NaN loss at batch 13
NaN loss at batch 14
NaN loss at batch 15
NaN loss at batch 16
NaN loss at batch 17
NaN loss at batch 18
NaN loss at batch 19
NaN loss at batch 20
NaN loss at batch 21
NaN loss at batch 22
NaN loss at batch 23
NaN loss at batch 24
NaN loss at batch 25
NaN loss at batch 26
NaN loss at batch 27
NaN loss at batch 28
NaN loss at batch 29
NaN loss at batch 30
NaN loss at batch 31
NaN loss at batch 32
NaN loss at batch 33
NaN loss at batch 34
NaN loss at batch 35
NaN loss at batch 36
NaN loss at batch 37
NaN loss at batch 38
NaN loss at batch 39
NaN loss at batch 40
NaN loss at batch 41
NaN loss at batch 42
NaN loss at batch 43
NaN loss at batch 44
NaN loss at batch 45
NaN loss at batch 46
NaN loss at batch 47
NaN loss at batch 48
NaN loss at batch 49
--- ERROR in Trial 27 ---
Configuration: {'batch_size': 32, 'cross_entropy_weighting': True, 'l1_lambda': 0.0, 'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.3, 'l2_lambda': 0.0, 'noise_std_dev': 0.0}
Error: need at least one array to concatenate
  > Trial 27 Result: F1 = 0.0 (due to error)
[I 2025-11-10 14:53:02,425] Trial 27 finished with value: 0.0 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.3, 'l2_lambda': 0.0, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 28 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 20
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.05
  > hidden_layers: 1
  > hidden_size: 32
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 64
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.0
  > l2_lambda: 0.0001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7368, F1=0.6695 | Val: Loss=0.4443, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1495, F1=0.8716 | Val: Loss=0.1754, F1=0.8987 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1209, F1=0.9658 | Val: Loss=0.1549, F1=0.9369 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1028, F1=0.9798 | Val: Loss=0.1435, F1=0.9486 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1086, F1=0.9827 | Val: Loss=0.1648, F1=0.9407 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1050, F1=0.9853 | Val: Loss=0.1711, F1=0.9413 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1045, F1=0.9853 | Val: Loss=0.1559, F1=0.9407 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0990, F1=0.9937 | Val: Loss=0.1667, F1=0.9407 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0982, F1=0.9943 | Val: Loss=0.1854, F1=0.9326 (Per-Sample)
Early stopping triggered after 83 epochs.
Best model restored from epoch 33 with val_f1 0.9563

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7964, F1=0.6504 | Val: Loss=0.4504, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1413, F1=0.8929 | Val: Loss=0.2041, F1=0.8878 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1064, F1=0.9693 | Val: Loss=0.2023, F1=0.9268 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1094, F1=0.9772 | Val: Loss=0.2174, F1=0.9359 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0980, F1=0.9810 | Val: Loss=0.2095, F1=0.9270 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1009, F1=0.9846 | Val: Loss=0.2136, F1=0.9086 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0945, F1=0.9891 | Val: Loss=0.2200, F1=0.9275 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0946, F1=0.9877 | Val: Loss=0.1965, F1=0.9445 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1079, F1=0.9861 | Val: Loss=0.2127, F1=0.9202 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0932, F1=0.9930 | Val: Loss=0.2205, F1=0.9350 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0939, F1=0.9949 | Val: Loss=0.2240, F1=0.9360 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0962, F1=0.9930 | Val: Loss=0.2184, F1=0.9274 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0943, F1=0.9924 | Val: Loss=0.1899, F1=0.9442 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0911, F1=0.9968 | Val: Loss=0.2134, F1=0.9360 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0976, F1=0.9937 | Val: Loss=0.2098, F1=0.9360 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0886, F1=0.9981 | Val: Loss=0.2018, F1=0.9353 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0947, F1=0.9943 | Val: Loss=0.2376, F1=0.9202 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0878, F1=0.9987 | Val: Loss=0.2172, F1=0.9359 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0895, F1=0.9975 | Val: Loss=0.2115, F1=0.9445 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0912, F1=0.9987 | Val: Loss=0.2218, F1=0.9444 (Per-Sample)
Epoch 200/300 | Train: Loss=0.0940, F1=0.9968 | Val: Loss=0.2169, F1=0.9363 (Per-Sample)
Epoch 210/300 | Train: Loss=0.0895, F1=0.9981 | Val: Loss=0.2090, F1=0.9445 (Per-Sample)
Epoch 220/300 | Train: Loss=0.0915, F1=0.9981 | Val: Loss=0.2347, F1=0.9374 (Per-Sample)
Early stopping triggered after 229 epochs.
Best model restored from epoch 179 with val_f1 0.9528

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7678, F1=0.6616 | Val: Loss=0.4426, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1275, F1=0.8932 | Val: Loss=0.1876, F1=0.8231 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1039, F1=0.9619 | Val: Loss=0.1692, F1=0.8647 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0961, F1=0.9783 | Val: Loss=0.1557, F1=0.9344 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0992, F1=0.9847 | Val: Loss=0.1730, F1=0.9453 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0953, F1=0.9886 | Val: Loss=0.1557, F1=0.9341 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0906, F1=0.9904 | Val: Loss=0.1713, F1=0.9382 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0977, F1=0.9860 | Val: Loss=0.1769, F1=0.9270 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0863, F1=0.9936 | Val: Loss=0.1766, F1=0.9435 (Per-Sample)
Early stopping triggered after 81 epochs.
Best model restored from epoch 31 with val_f1 0.9600

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6945, F1=0.6920 | Val: Loss=0.4812, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1288, F1=0.8809 | Val: Loss=0.2081, F1=0.8238 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1013, F1=0.9566 | Val: Loss=0.1883, F1=0.8909 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0889, F1=0.9779 | Val: Loss=0.1918, F1=0.9197 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1054, F1=0.9702 | Val: Loss=0.2065, F1=0.9124 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1016, F1=0.9787 | Val: Loss=0.2113, F1=0.9106 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0919, F1=0.9872 | Val: Loss=0.2303, F1=0.9051 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0925, F1=0.9879 | Val: Loss=0.2230, F1=0.9034 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0882, F1=0.9924 | Val: Loss=0.2171, F1=0.9124 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0868, F1=0.9930 | Val: Loss=0.2084, F1=0.9206 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0876, F1=0.9930 | Val: Loss=0.2321, F1=0.9051 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0812, F1=0.9968 | Val: Loss=0.2142, F1=0.9197 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0880, F1=0.9950 | Val: Loss=0.2252, F1=0.8961 (Per-Sample)
Early stopping triggered after 129 epochs.
Best model restored from epoch 79 with val_f1 0.9212

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7313, F1=0.6454 | Val: Loss=0.2857, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1373, F1=0.8686 | Val: Loss=0.1796, F1=0.8711 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1198, F1=0.9603 | Val: Loss=0.1683, F1=0.9233 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1021, F1=0.9808 | Val: Loss=0.1659, F1=0.9294 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0999, F1=0.9813 | Val: Loss=0.1944, F1=0.9136 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0927, F1=0.9884 | Val: Loss=0.1949, F1=0.9301 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0977, F1=0.9853 | Val: Loss=0.1777, F1=0.9342 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0986, F1=0.9879 | Val: Loss=0.1823, F1=0.9251 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0961, F1=0.9911 | Val: Loss=0.2077, F1=0.9323 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0881, F1=0.9943 | Val: Loss=0.1742, F1=0.9470 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0928, F1=0.9949 | Val: Loss=0.1744, F1=0.9544 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0874, F1=0.9975 | Val: Loss=0.1678, F1=0.9372 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0891, F1=0.9962 | Val: Loss=0.1656, F1=0.9544 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0884, F1=0.9968 | Val: Loss=0.1707, F1=0.9544 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0863, F1=0.9987 | Val: Loss=0.1860, F1=0.9470 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0885, F1=0.9962 | Val: Loss=0.1647, F1=0.9539 (Per-Sample)
Best trial: 21. Best value: -0.963:  58%|█████▊    | 29/50 [1:14:02<46:26, 132.67s/it]
Early stopping triggered after 155 epochs.
Best model restored from epoch 105 with val_f1 0.9693

Cross-validation score: 0.9519 ± 0.0163
  > Trial 28 Result: Mean F1 = 0.9519
[I 2025-11-10 14:55:39,023] Trial 28 finished with value: -0.9519309988533106 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.05, 'hidden_layers': 1, 'hidden_size': 32, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.0, 'l2_lambda': 0.0001, 'noise_std_dev': 0.2}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 29 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 20
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.5
  > l2_lambda: 0.001
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6708, F1=0.6786 | Val: Loss=0.4161, F1=0.7488 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2132, F1=0.9197 | Val: Loss=0.2494, F1=0.8949 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1785, F1=0.9662 | Val: Loss=0.2097, F1=0.9495 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1759, F1=0.9794 | Val: Loss=0.1980, F1=0.9565 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1697, F1=0.9833 | Val: Loss=0.2036, F1=0.9604 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1690, F1=0.9846 | Val: Loss=0.2291, F1=0.9516 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1795, F1=0.9743 | Val: Loss=0.2345, F1=0.9369 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1692, F1=0.9873 | Val: Loss=0.2291, F1=0.9516 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1717, F1=0.9880 | Val: Loss=0.2240, F1=0.9271 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1626, F1=0.9950 | Val: Loss=0.2384, F1=0.9468 (Per-Sample)
Early stopping triggered after 98 epochs.
Best model restored from epoch 48 with val_f1 0.9668

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6689, F1=0.6776 | Val: Loss=0.4291, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1885, F1=0.9241 | Val: Loss=0.2739, F1=0.8934 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1782, F1=0.9673 | Val: Loss=0.3028, F1=0.9026 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1661, F1=0.9806 | Val: Loss=0.2779, F1=0.9051 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1630, F1=0.9840 | Val: Loss=0.2595, F1=0.9359 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1560, F1=0.9917 | Val: Loss=0.2872, F1=0.9197 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1588, F1=0.9880 | Val: Loss=0.2613, F1=0.9359 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1516, F1=0.9962 | Val: Loss=0.2866, F1=0.9454 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1607, F1=0.9924 | Val: Loss=0.2990, F1=0.9045 (Per-Sample)
Early stopping triggered after 89 epochs.
Best model restored from epoch 39 with val_f1 0.9524

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6414, F1=0.6892 | Val: Loss=0.4272, F1=0.6944 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1859, F1=0.9244 | Val: Loss=0.2286, F1=0.8300 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1610, F1=0.9674 | Val: Loss=0.2163, F1=0.8784 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1536, F1=0.9818 | Val: Loss=0.2389, F1=0.9072 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1645, F1=0.9757 | Val: Loss=0.2576, F1=0.9058 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1607, F1=0.9775 | Val: Loss=0.2441, F1=0.9137 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1495, F1=0.9879 | Val: Loss=0.2297, F1=0.9173 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1466, F1=0.9917 | Val: Loss=0.2409, F1=0.9381 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1443, F1=0.9956 | Val: Loss=0.2340, F1=0.9171 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1446, F1=0.9949 | Val: Loss=0.2461, F1=0.9381 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1428, F1=0.9962 | Val: Loss=0.2430, F1=0.9373 (Per-Sample)
Early stopping triggered after 106 epochs.
Best model restored from epoch 56 with val_f1 0.9538

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6514, F1=0.6826 | Val: Loss=0.4765, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1842, F1=0.9133 | Val: Loss=0.2490, F1=0.8459 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1571, F1=0.9694 | Val: Loss=0.2241, F1=0.8894 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1551, F1=0.9794 | Val: Loss=0.2340, F1=0.9106 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1571, F1=0.9773 | Val: Loss=0.2362, F1=0.8812 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1452, F1=0.9917 | Val: Loss=0.2384, F1=0.9010 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1505, F1=0.9912 | Val: Loss=0.2479, F1=0.9206 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1475, F1=0.9912 | Val: Loss=0.2367, F1=0.9280 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1408, F1=0.9968 | Val: Loss=0.2345, F1=0.9187 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1395, F1=0.9981 | Val: Loss=0.2344, F1=0.9280 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1404, F1=0.9955 | Val: Loss=0.2371, F1=0.9281 (Per-Sample)
Early stopping triggered after 101 epochs.
Best model restored from epoch 51 with val_f1 0.9368

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6591, F1=0.6541 | Val: Loss=0.3094, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1923, F1=0.9198 | Val: Loss=0.2054, F1=0.8931 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1796, F1=0.9472 | Val: Loss=0.1980, F1=0.9412 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1653, F1=0.9781 | Val: Loss=0.2187, F1=0.9206 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1555, F1=0.9878 | Val: Loss=0.2229, F1=0.9301 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1538, F1=0.9878 | Val: Loss=0.2237, F1=0.9260 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1594, F1=0.9879 | Val: Loss=0.2461, F1=0.9154 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1540, F1=0.9898 | Val: Loss=0.2442, F1=0.9122 (Per-Sample)
Best trial: 21. Best value: -0.963:  60%|██████    | 30/50 [1:16:05<43:13, 129.67s/it]
Early stopping triggered after 75 epochs.
Best model restored from epoch 25 with val_f1 0.9440

Cross-validation score: 0.9507 ± 0.0101
  > Trial 29 Result: Mean F1 = 0.9507
[I 2025-11-10 14:57:41,673] Trial 29 finished with value: -0.9507467128405123 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 20, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.5, 'l2_lambda': 0.001, 'noise_std_dev': 0.1}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 30 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.0
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6826, F1=0.6555 | Val: Loss=0.4136, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2040, F1=0.9295 | Val: Loss=0.2333, F1=0.9073 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1853, F1=0.9749 | Val: Loss=0.2489, F1=0.9332 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1758, F1=0.9800 | Val: Loss=0.2400, F1=0.9408 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1753, F1=0.9827 | Val: Loss=0.2174, F1=0.9414 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1670, F1=0.9866 | Val: Loss=0.2574, F1=0.9333 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1601, F1=0.9962 | Val: Loss=0.2358, F1=0.9440 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1636, F1=0.9937 | Val: Loss=0.2407, F1=0.9606 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1566, F1=0.9994 | Val: Loss=0.2414, F1=0.9530 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1631, F1=0.9930 | Val: Loss=0.2500, F1=0.9510 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1638, F1=0.9949 | Val: Loss=0.2632, F1=0.9260 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1592, F1=0.9968 | Val: Loss=0.2373, F1=0.9433 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1736, F1=0.9874 | Val: Loss=0.2432, F1=0.9436 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1693, F1=0.9886 | Val: Loss=0.2556, F1=0.9338 (Per-Sample)
Early stopping triggered after 139 epochs.
Best model restored from epoch 89 with val_f1 0.9610

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7013, F1=0.6494 | Val: Loss=0.4267, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2104, F1=0.8950 | Val: Loss=0.3161, F1=0.8341 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1760, F1=0.9600 | Val: Loss=0.2793, F1=0.9029 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1668, F1=0.9814 | Val: Loss=0.2773, F1=0.9152 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1627, F1=0.9885 | Val: Loss=0.2491, F1=0.9372 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1576, F1=0.9911 | Val: Loss=0.2431, F1=0.9369 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1570, F1=0.9930 | Val: Loss=0.2863, F1=0.9132 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1705, F1=0.9835 | Val: Loss=0.2697, F1=0.9370 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1636, F1=0.9872 | Val: Loss=0.2849, F1=0.9137 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1577, F1=0.9937 | Val: Loss=0.2727, F1=0.9372 (Per-Sample)
Early stopping triggered after 94 epochs.
Best model restored from epoch 44 with val_f1 0.9613

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5675, F1=0.6846 | Val: Loss=0.4244, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1795, F1=0.9283 | Val: Loss=0.2329, F1=0.8527 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1665, F1=0.9735 | Val: Loss=0.2168, F1=0.9065 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1666, F1=0.9635 | Val: Loss=0.2459, F1=0.9101 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1546, F1=0.9860 | Val: Loss=0.2175, F1=0.9075 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1465, F1=0.9917 | Val: Loss=0.2305, F1=0.9177 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1560, F1=0.9860 | Val: Loss=0.2639, F1=0.9130 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1524, F1=0.9886 | Val: Loss=0.2246, F1=0.9290 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1506, F1=0.9905 | Val: Loss=0.2159, F1=0.9384 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1494, F1=0.9943 | Val: Loss=0.2559, F1=0.9131 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1467, F1=0.9943 | Val: Loss=0.2368, F1=0.9296 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1425, F1=0.9981 | Val: Loss=0.2444, F1=0.9212 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1422, F1=0.9987 | Val: Loss=0.2363, F1=0.9285 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1462, F1=0.9937 | Val: Loss=0.2411, F1=0.9387 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1430, F1=0.9962 | Val: Loss=0.2478, F1=0.9269 (Per-Sample)
Early stopping triggered after 149 epochs.
Best model restored from epoch 99 with val_f1 0.9611

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5829, F1=0.6994 | Val: Loss=0.4340, F1=0.6530 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1826, F1=0.8786 | Val: Loss=0.2364, F1=0.8205 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1630, F1=0.9698 | Val: Loss=0.2408, F1=0.9124 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1530, F1=0.9880 | Val: Loss=0.2424, F1=0.9189 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1736, F1=0.9732 | Val: Loss=0.2664, F1=0.8837 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1700, F1=0.9742 | Val: Loss=0.2680, F1=0.8927 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1499, F1=0.9892 | Val: Loss=0.2537, F1=0.9108 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1433, F1=0.9937 | Val: Loss=0.2366, F1=0.9281 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1427, F1=0.9968 | Val: Loss=0.2420, F1=0.9368 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1413, F1=0.9975 | Val: Loss=0.2633, F1=0.9286 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1396, F1=0.9987 | Val: Loss=0.2596, F1=0.9368 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1427, F1=0.9975 | Val: Loss=0.2496, F1=0.9447 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1467, F1=0.9943 | Val: Loss=0.2716, F1=0.9220 (Per-Sample)
Early stopping triggered after 125 epochs.
Best model restored from epoch 75 with val_f1 0.9525

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6323, F1=0.6514 | Val: Loss=0.3198, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1947, F1=0.9125 | Val: Loss=0.2222, F1=0.9201 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1659, F1=0.9786 | Val: Loss=0.2315, F1=0.9215 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1772, F1=0.9672 | Val: Loss=0.2662, F1=0.8827 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1619, F1=0.9848 | Val: Loss=0.2260, F1=0.9198 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1500, F1=0.9936 | Val: Loss=0.2531, F1=0.9234 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1561, F1=0.9918 | Val: Loss=0.2002, F1=0.9440 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1549, F1=0.9918 | Val: Loss=0.2215, F1=0.9399 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1542, F1=0.9924 | Val: Loss=0.2297, F1=0.9206 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1526, F1=0.9917 | Val: Loss=0.2438, F1=0.9165 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1524, F1=0.9923 | Val: Loss=0.2534, F1=0.9226 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1527, F1=0.9911 | Val: Loss=0.2204, F1=0.9269 (Per-Sample)
Best trial: 21. Best value: -0.963:  62%|██████▏   | 31/50 [1:19:42<49:24, 156.02s/it]
Early stopping triggered after 113 epochs.
Best model restored from epoch 63 with val_f1 0.9534

Cross-validation score: 0.9579 ± 0.0040
  > Trial 30 Result: Mean F1 = 0.9579
[I 2025-11-10 15:01:19,168] Trial 30 finished with value: -0.9578711223912002 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.0, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 31 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.0
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6754, F1=0.6547 | Val: Loss=0.4099, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2209, F1=0.8512 | Val: Loss=0.2408, F1=0.9085 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1861, F1=0.9682 | Val: Loss=0.2367, F1=0.9248 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1710, F1=0.9833 | Val: Loss=0.2286, F1=0.9442 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1765, F1=0.9802 | Val: Loss=0.2295, F1=0.9524 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1669, F1=0.9892 | Val: Loss=0.2251, F1=0.9442 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1649, F1=0.9898 | Val: Loss=0.2261, F1=0.9456 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1740, F1=0.9848 | Val: Loss=0.2546, F1=0.9532 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1784, F1=0.9854 | Val: Loss=0.2250, F1=0.9414 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1688, F1=0.9880 | Val: Loss=0.2318, F1=0.9523 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1617, F1=0.9930 | Val: Loss=0.2604, F1=0.9267 (Per-Sample)
Early stopping triggered after 109 epochs.
Best model restored from epoch 59 with val_f1 0.9614

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6294, F1=0.6638 | Val: Loss=0.4126, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1983, F1=0.9249 | Val: Loss=0.2950, F1=0.8794 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1762, F1=0.9776 | Val: Loss=0.2826, F1=0.9186 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1708, F1=0.9821 | Val: Loss=0.2783, F1=0.9202 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1568, F1=0.9898 | Val: Loss=0.3089, F1=0.9072 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1716, F1=0.9842 | Val: Loss=0.2699, F1=0.9370 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1553, F1=0.9910 | Val: Loss=0.2896, F1=0.9285 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1580, F1=0.9917 | Val: Loss=0.2591, F1=0.9389 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1682, F1=0.9893 | Val: Loss=0.2987, F1=0.9226 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1599, F1=0.9931 | Val: Loss=0.2843, F1=0.9218 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1759, F1=0.9822 | Val: Loss=0.3026, F1=0.9207 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1634, F1=0.9845 | Val: Loss=0.2536, F1=0.9536 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1552, F1=0.9949 | Val: Loss=0.2599, F1=0.9389 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1530, F1=0.9975 | Val: Loss=0.2961, F1=0.9230 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1637, F1=0.9937 | Val: Loss=0.2758, F1=0.9285 (Per-Sample)
Early stopping triggered after 141 epochs.
Best model restored from epoch 91 with val_f1 0.9542

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6698, F1=0.6764 | Val: Loss=0.4358, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1775, F1=0.9061 | Val: Loss=0.2366, F1=0.8163 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1712, F1=0.9708 | Val: Loss=0.2359, F1=0.9038 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1603, F1=0.9767 | Val: Loss=0.2345, F1=0.9285 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1558, F1=0.9853 | Val: Loss=0.2327, F1=0.9410 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1526, F1=0.9866 | Val: Loss=0.2204, F1=0.9103 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1488, F1=0.9924 | Val: Loss=0.2417, F1=0.9281 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1453, F1=0.9936 | Val: Loss=0.2462, F1=0.9382 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1452, F1=0.9936 | Val: Loss=0.2363, F1=0.9204 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1458, F1=0.9949 | Val: Loss=0.2351, F1=0.9286 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1426, F1=0.9975 | Val: Loss=0.2342, F1=0.9194 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1461, F1=0.9943 | Val: Loss=0.2387, F1=0.9449 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1439, F1=0.9968 | Val: Loss=0.2396, F1=0.9469 (Per-Sample)
Early stopping triggered after 122 epochs.
Best model restored from epoch 72 with val_f1 0.9616

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6136, F1=0.6902 | Val: Loss=0.4311, F1=0.6051 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1759, F1=0.9177 | Val: Loss=0.2610, F1=0.8534 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1713, F1=0.9647 | Val: Loss=0.2570, F1=0.9010 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1520, F1=0.9826 | Val: Loss=0.2361, F1=0.9281 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1510, F1=0.9892 | Val: Loss=0.2327, F1=0.9280 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1543, F1=0.9859 | Val: Loss=0.2303, F1=0.9091 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1618, F1=0.9840 | Val: Loss=0.2524, F1=0.9368 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1497, F1=0.9918 | Val: Loss=0.2299, F1=0.9279 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1486, F1=0.9911 | Val: Loss=0.2447, F1=0.9286 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1512, F1=0.9911 | Val: Loss=0.2599, F1=0.9010 (Per-Sample)
Early stopping triggered after 94 epochs.
Best model restored from epoch 44 with val_f1 0.9447

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6607, F1=0.6398 | Val: Loss=0.3015, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1906, F1=0.9143 | Val: Loss=0.2421, F1=0.8791 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1654, F1=0.9781 | Val: Loss=0.2217, F1=0.9428 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1615, F1=0.9849 | Val: Loss=0.1972, F1=0.9511 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1569, F1=0.9865 | Val: Loss=0.2193, F1=0.9463 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1638, F1=0.9860 | Val: Loss=0.2166, F1=0.9404 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1585, F1=0.9853 | Val: Loss=0.2216, F1=0.9212 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1654, F1=0.9835 | Val: Loss=0.2275, F1=0.9261 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1534, F1=0.9917 | Val: Loss=0.2255, F1=0.9378 (Per-Sample)
Best trial: 21. Best value: -0.963:  64%|██████▍   | 32/50 [1:22:57<50:17, 167.64s/it]
Early stopping triggered after 89 epochs.
Best model restored from epoch 39 with val_f1 0.9605

Cross-validation score: 0.9565 ± 0.0065
  > Trial 31 Result: Mean F1 = 0.9565
[I 2025-11-10 15:04:33,948] Trial 31 finished with value: -0.9564713722191703 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.0, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 32 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.0
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6117, F1=0.6640 | Val: Loss=0.4363, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2157, F1=0.8938 | Val: Loss=0.2520, F1=0.9066 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1910, F1=0.9590 | Val: Loss=0.2439, F1=0.9230 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1775, F1=0.9808 | Val: Loss=0.2735, F1=0.9260 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1690, F1=0.9879 | Val: Loss=0.2286, F1=0.9370 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1738, F1=0.9860 | Val: Loss=0.2738, F1=0.9157 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1749, F1=0.9880 | Val: Loss=0.2674, F1=0.9114 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1612, F1=0.9956 | Val: Loss=0.2696, F1=0.9180 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1585, F1=0.9955 | Val: Loss=0.2687, F1=0.9259 (Per-Sample)
Early stopping triggered after 84 epochs.
Best model restored from epoch 34 with val_f1 0.9590

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6489, F1=0.6692 | Val: Loss=0.4378, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2054, F1=0.9223 | Val: Loss=0.2778, F1=0.9042 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1739, F1=0.9775 | Val: Loss=0.2543, F1=0.9454 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1665, F1=0.9846 | Val: Loss=0.2666, F1=0.9383 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1657, F1=0.9847 | Val: Loss=0.2620, F1=0.9279 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1745, F1=0.9802 | Val: Loss=0.2826, F1=0.9213 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1677, F1=0.9835 | Val: Loss=0.3000, F1=0.9145 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1574, F1=0.9918 | Val: Loss=0.2699, F1=0.9455 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1559, F1=0.9956 | Val: Loss=0.2415, F1=0.9454 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1542, F1=0.9975 | Val: Loss=0.2632, F1=0.9458 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1525, F1=0.9975 | Val: Loss=0.2382, F1=0.9522 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1548, F1=0.9956 | Val: Loss=0.2414, F1=0.9619 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1576, F1=0.9949 | Val: Loss=0.2756, F1=0.9307 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1534, F1=0.9949 | Val: Loss=0.2786, F1=0.9302 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1554, F1=0.9956 | Val: Loss=0.2614, F1=0.9466 (Per-Sample)
Epoch 150/300 | Train: Loss=0.1540, F1=0.9949 | Val: Loss=0.2867, F1=0.9217 (Per-Sample)
Epoch 160/300 | Train: Loss=0.1563, F1=0.9962 | Val: Loss=0.2828, F1=0.9287 (Per-Sample)
Early stopping triggered after 160 epochs.
Best model restored from epoch 110 with val_f1 0.9619

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6487, F1=0.6709 | Val: Loss=0.4284, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1901, F1=0.8942 | Val: Loss=0.2221, F1=0.8320 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1703, F1=0.9666 | Val: Loss=0.2643, F1=0.8890 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1613, F1=0.9828 | Val: Loss=0.2531, F1=0.8961 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1523, F1=0.9878 | Val: Loss=0.2518, F1=0.8951 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1473, F1=0.9904 | Val: Loss=0.2454, F1=0.9293 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1488, F1=0.9924 | Val: Loss=0.2427, F1=0.9286 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1453, F1=0.9956 | Val: Loss=0.2490, F1=0.9452 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1450, F1=0.9956 | Val: Loss=0.2515, F1=0.9450 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1442, F1=0.9949 | Val: Loss=0.2147, F1=0.9349 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1586, F1=0.9899 | Val: Loss=0.2653, F1=0.9120 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1479, F1=0.9891 | Val: Loss=0.2214, F1=0.9414 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1463, F1=0.9930 | Val: Loss=0.2293, F1=0.9244 (Per-Sample)
Early stopping triggered after 120 epochs.
Best model restored from epoch 70 with val_f1 0.9452

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5991, F1=0.6939 | Val: Loss=0.4329, F1=0.6530 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1833, F1=0.8896 | Val: Loss=0.2834, F1=0.8269 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1551, F1=0.9814 | Val: Loss=0.2412, F1=0.9033 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1538, F1=0.9872 | Val: Loss=0.2285, F1=0.9017 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1620, F1=0.9769 | Val: Loss=0.2618, F1=0.9115 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1494, F1=0.9905 | Val: Loss=0.2786, F1=0.8827 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1541, F1=0.9899 | Val: Loss=0.2506, F1=0.9197 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1501, F1=0.9930 | Val: Loss=0.2610, F1=0.9279 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1771, F1=0.9805 | Val: Loss=0.2617, F1=0.9187 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1400, F1=0.9975 | Val: Loss=0.2543, F1=0.9279 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1488, F1=0.9917 | Val: Loss=0.2449, F1=0.9266 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1510, F1=0.9937 | Val: Loss=0.2554, F1=0.9373 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1484, F1=0.9918 | Val: Loss=0.2630, F1=0.8989 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1471, F1=0.9956 | Val: Loss=0.2467, F1=0.9279 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1416, F1=0.9949 | Val: Loss=0.2737, F1=0.8855 (Per-Sample)
Epoch 150/300 | Train: Loss=0.1528, F1=0.9912 | Val: Loss=0.2487, F1=0.9197 (Per-Sample)
Early stopping triggered after 156 epochs.
Best model restored from epoch 106 with val_f1 0.9525

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6244, F1=0.6531 | Val: Loss=0.3082, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1978, F1=0.8672 | Val: Loss=0.2249, F1=0.8791 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1795, F1=0.9616 | Val: Loss=0.2195, F1=0.9215 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1739, F1=0.9624 | Val: Loss=0.2283, F1=0.9355 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1559, F1=0.9891 | Val: Loss=0.2294, F1=0.9456 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1565, F1=0.9905 | Val: Loss=0.2136, F1=0.9520 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1544, F1=0.9892 | Val: Loss=0.2290, F1=0.9287 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1539, F1=0.9905 | Val: Loss=0.2320, F1=0.9394 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1556, F1=0.9924 | Val: Loss=0.2418, F1=0.9290 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1605, F1=0.9860 | Val: Loss=0.2277, F1=0.9312 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1552, F1=0.9904 | Val: Loss=0.2078, F1=0.9528 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1514, F1=0.9917 | Val: Loss=0.2226, F1=0.9295 (Per-Sample)
Best trial: 21. Best value: -0.963:  66%|██████▌   | 33/50 [1:26:38<52:00, 183.55s/it]
Early stopping triggered after 115 epochs.
Best model restored from epoch 65 with val_f1 0.9694

Cross-validation score: 0.9576 ± 0.0082
  > Trial 32 Result: Mean F1 = 0.9576
[I 2025-11-10 15:08:14,613] Trial 32 finished with value: -0.9576182388210187 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.0, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 33 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 20
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.0
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6117, F1=0.6640 | Val: Loss=0.4102, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2053, F1=0.9376 | Val: Loss=0.2227, F1=0.9380 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1849, F1=0.9749 | Val: Loss=0.2430, F1=0.9333 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1726, F1=0.9841 | Val: Loss=0.2265, F1=0.9370 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1701, F1=0.9866 | Val: Loss=0.2598, F1=0.9247 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1697, F1=0.9880 | Val: Loss=0.2624, F1=0.9433 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1746, F1=0.9880 | Val: Loss=0.2512, F1=0.9288 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1668, F1=0.9918 | Val: Loss=0.2847, F1=0.9242 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1652, F1=0.9899 | Val: Loss=0.2379, F1=0.9433 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1715, F1=0.9880 | Val: Loss=0.2919, F1=0.9167 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1676, F1=0.9905 | Val: Loss=0.2445, F1=0.9458 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1568, F1=0.9975 | Val: Loss=0.2557, F1=0.9359 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1588, F1=0.9956 | Val: Loss=0.2564, F1=0.9383 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1593, F1=0.9975 | Val: Loss=0.2619, F1=0.9277 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1658, F1=0.9937 | Val: Loss=0.2607, F1=0.9462 (Per-Sample)
Epoch 150/300 | Train: Loss=0.1595, F1=0.9975 | Val: Loss=0.2469, F1=0.9359 (Per-Sample)
Epoch 160/300 | Train: Loss=0.1587, F1=0.9981 | Val: Loss=0.2700, F1=0.9374 (Per-Sample)
Epoch 170/300 | Train: Loss=0.1609, F1=0.9968 | Val: Loss=0.2255, F1=0.9521 (Per-Sample)
Epoch 180/300 | Train: Loss=0.1655, F1=0.9950 | Val: Loss=0.2830, F1=0.9282 (Per-Sample)
Epoch 190/300 | Train: Loss=0.1805, F1=0.9854 | Val: Loss=0.2696, F1=0.9449 (Per-Sample)
Early stopping triggered after 191 epochs.
Best model restored from epoch 141 with val_f1 0.9683

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7038, F1=0.6410 | Val: Loss=0.4179, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1939, F1=0.9269 | Val: Loss=0.2905, F1=0.8852 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1831, F1=0.9663 | Val: Loss=0.2950, F1=0.8983 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1683, F1=0.9827 | Val: Loss=0.2524, F1=0.9292 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1636, F1=0.9839 | Val: Loss=0.2824, F1=0.9132 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1662, F1=0.9846 | Val: Loss=0.2738, F1=0.9207 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1629, F1=0.9879 | Val: Loss=0.2857, F1=0.8977 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1695, F1=0.9848 | Val: Loss=0.3047, F1=0.8828 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1632, F1=0.9905 | Val: Loss=0.2778, F1=0.9151 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1575, F1=0.9931 | Val: Loss=0.2887, F1=0.9116 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1569, F1=0.9956 | Val: Loss=0.2658, F1=0.9230 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1554, F1=0.9956 | Val: Loss=0.2923, F1=0.9133 (Per-Sample)
Early stopping triggered after 112 epochs.
Best model restored from epoch 62 with val_f1 0.9401

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5979, F1=0.6711 | Val: Loss=0.4209, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1892, F1=0.9170 | Val: Loss=0.2637, F1=0.8346 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1613, F1=0.9734 | Val: Loss=0.2174, F1=0.8955 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1524, F1=0.9846 | Val: Loss=0.2533, F1=0.8987 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1518, F1=0.9873 | Val: Loss=0.2523, F1=0.9101 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1520, F1=0.9858 | Val: Loss=0.2571, F1=0.8934 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1498, F1=0.9898 | Val: Loss=0.2549, F1=0.9060 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1546, F1=0.9846 | Val: Loss=0.2577, F1=0.9109 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1506, F1=0.9899 | Val: Loss=0.2308, F1=0.9367 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1467, F1=0.9943 | Val: Loss=0.2587, F1=0.9228 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1446, F1=0.9937 | Val: Loss=0.2540, F1=0.9230 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1451, F1=0.9962 | Val: Loss=0.2492, F1=0.8944 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1414, F1=0.9981 | Val: Loss=0.2325, F1=0.9189 (Per-Sample)
Early stopping triggered after 129 epochs.
Best model restored from epoch 79 with val_f1 0.9437

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6228, F1=0.6818 | Val: Loss=0.4386, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1836, F1=0.9262 | Val: Loss=0.2619, F1=0.8526 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1630, F1=0.9712 | Val: Loss=0.2633, F1=0.8886 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1534, F1=0.9873 | Val: Loss=0.2447, F1=0.9279 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1535, F1=0.9905 | Val: Loss=0.2339, F1=0.9368 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1492, F1=0.9899 | Val: Loss=0.2394, F1=0.9279 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1459, F1=0.9943 | Val: Loss=0.2501, F1=0.9279 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1452, F1=0.9956 | Val: Loss=0.2431, F1=0.9091 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1481, F1=0.9943 | Val: Loss=0.2488, F1=0.9187 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1449, F1=0.9949 | Val: Loss=0.2476, F1=0.9206 (Per-Sample)
Early stopping triggered after 90 epochs.
Best model restored from epoch 40 with val_f1 0.9368

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6370, F1=0.6517 | Val: Loss=0.3242, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2047, F1=0.8647 | Val: Loss=0.2130, F1=0.9233 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1757, F1=0.9682 | Val: Loss=0.2299, F1=0.9361 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1647, F1=0.9789 | Val: Loss=0.2197, F1=0.9229 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1629, F1=0.9765 | Val: Loss=0.2117, F1=0.9450 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1618, F1=0.9873 | Val: Loss=0.2096, F1=0.9239 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1538, F1=0.9878 | Val: Loss=0.2156, F1=0.9439 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1520, F1=0.9937 | Val: Loss=0.2184, F1=0.9330 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1596, F1=0.9905 | Val: Loss=0.2259, F1=0.9157 (Per-Sample)
Best trial: 21. Best value: -0.963:  68%|██████▊   | 34/50 [1:30:07<51:00, 191.27s/it]
Early stopping triggered after 81 epochs.
Best model restored from epoch 31 with val_f1 0.9614

Cross-validation score: 0.9501 ± 0.0125
  > Trial 33 Result: Mean F1 = 0.9501
[I 2025-11-10 15:11:43,878] Trial 33 finished with value: -0.9500573404193844 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.0, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 34 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 40
  > weight_ce_intensity: 1.0
  > label_smoothing_epsilon: 0.1
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.0
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.56359588 0.36821598 0.06818814]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5255, F1=0.7039 | Val: Loss=0.3923, F1=0.7729 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2203, F1=0.8842 | Val: Loss=0.2472, F1=0.8868 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1925, F1=0.9465 | Val: Loss=0.2336, F1=0.9344 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1847, F1=0.9660 | Val: Loss=0.2345, F1=0.9152 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1900, F1=0.9638 | Val: Loss=0.2375, F1=0.9290 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1680, F1=0.9806 | Val: Loss=0.1944, F1=0.9594 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1618, F1=0.9884 | Val: Loss=0.2163, F1=0.9431 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1657, F1=0.9905 | Val: Loss=0.2420, F1=0.9371 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1606, F1=0.9949 | Val: Loss=0.2377, F1=0.9517 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1633, F1=0.9924 | Val: Loss=0.2248, F1=0.9494 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1630, F1=0.9937 | Val: Loss=0.2276, F1=0.9333 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1632, F1=0.9943 | Val: Loss=0.2142, F1=0.9520 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1668, F1=0.9918 | Val: Loss=0.2453, F1=0.9260 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1622, F1=0.9924 | Val: Loss=0.2060, F1=0.9487 (Per-Sample)
Early stopping triggered after 134 epochs.
Best model restored from epoch 84 with val_f1 0.9672

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.57394339 0.36166296 0.06439365]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5516, F1=0.6943 | Val: Loss=0.3906, F1=0.7131 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2093, F1=0.8876 | Val: Loss=0.2897, F1=0.8913 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1877, F1=0.9325 | Val: Loss=0.2612, F1=0.8968 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1657, F1=0.9690 | Val: Loss=0.2551, F1=0.9275 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1602, F1=0.9831 | Val: Loss=0.2728, F1=0.9141 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1645, F1=0.9866 | Val: Loss=0.2546, F1=0.9302 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1595, F1=0.9899 | Val: Loss=0.2692, F1=0.9289 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1697, F1=0.9815 | Val: Loss=0.2463, F1=0.9113 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1587, F1=0.9898 | Val: Loss=0.2851, F1=0.9115 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1684, F1=0.9880 | Val: Loss=0.2741, F1=0.9226 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1535, F1=0.9956 | Val: Loss=0.2801, F1=0.9136 (Per-Sample)
Early stopping triggered after 101 epochs.
Best model restored from epoch 51 with val_f1 0.9458

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59906696 0.34001098 0.06092206]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5139, F1=0.7064 | Val: Loss=0.4052, F1=0.7079 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1853, F1=0.9160 | Val: Loss=0.2354, F1=0.8569 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1691, F1=0.9535 | Val: Loss=0.2417, F1=0.8820 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1578, F1=0.9757 | Val: Loss=0.2258, F1=0.9004 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1553, F1=0.9819 | Val: Loss=0.2376, F1=0.9097 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1574, F1=0.9787 | Val: Loss=0.2372, F1=0.9218 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1655, F1=0.9776 | Val: Loss=0.2389, F1=0.9230 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1441, F1=0.9936 | Val: Loss=0.2174, F1=0.9381 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1485, F1=0.9923 | Val: Loss=0.2037, F1=0.9265 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1582, F1=0.9794 | Val: Loss=0.2595, F1=0.9114 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1513, F1=0.9873 | Val: Loss=0.2130, F1=0.9270 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1474, F1=0.9931 | Val: Loss=0.2341, F1=0.9387 (Per-Sample)
Epoch 120/300 | Train: Loss=0.1457, F1=0.9943 | Val: Loss=0.2208, F1=0.9464 (Per-Sample)
Epoch 130/300 | Train: Loss=0.1408, F1=0.9981 | Val: Loss=0.2156, F1=0.9619 (Per-Sample)
Epoch 140/300 | Train: Loss=0.1428, F1=0.9981 | Val: Loss=0.2233, F1=0.9539 (Per-Sample)
Epoch 150/300 | Train: Loss=0.1442, F1=0.9950 | Val: Loss=0.2202, F1=0.9464 (Per-Sample)
Epoch 160/300 | Train: Loss=0.1453, F1=0.9956 | Val: Loss=0.2229, F1=0.9309 (Per-Sample)
Epoch 170/300 | Train: Loss=0.1423, F1=0.9975 | Val: Loss=0.2201, F1=0.9539 (Per-Sample)
Early stopping triggered after 174 epochs.
Best model restored from epoch 124 with val_f1 0.9686

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59951962 0.34139311 0.05908727]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.4912, F1=0.7123 | Val: Loss=0.4297, F1=0.6517 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1838, F1=0.8932 | Val: Loss=0.2629, F1=0.8003 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1668, F1=0.9524 | Val: Loss=0.2311, F1=0.8774 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1525, F1=0.9784 | Val: Loss=0.2438, F1=0.8839 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1565, F1=0.9701 | Val: Loss=0.2381, F1=0.9027 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1741, F1=0.9503 | Val: Loss=0.2549, F1=0.8530 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1424, F1=0.9930 | Val: Loss=0.2688, F1=0.8774 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1432, F1=0.9949 | Val: Loss=0.2341, F1=0.9097 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1504, F1=0.9886 | Val: Loss=0.2422, F1=0.8928 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1418, F1=0.9968 | Val: Loss=0.2567, F1=0.8939 (Per-Sample)
Early stopping triggered after 91 epochs.
Best model restored from epoch 41 with val_f1 0.9280

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 1.0): [0.59666739 0.33471586 0.06861675]
  Dynamic Loss: Label Smoothing Epsilon: 0.1
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5464, F1=0.6778 | Val: Loss=0.3122, F1=0.7945 (Per-Sample)
Epoch  10/300 | Train: Loss=0.2104, F1=0.8828 | Val: Loss=0.2266, F1=0.8852 (Per-Sample)
Epoch  20/300 | Train: Loss=0.1815, F1=0.9450 | Val: Loss=0.2165, F1=0.9159 (Per-Sample)
Epoch  30/300 | Train: Loss=0.1692, F1=0.9697 | Val: Loss=0.2155, F1=0.9248 (Per-Sample)
Epoch  40/300 | Train: Loss=0.1547, F1=0.9865 | Val: Loss=0.2214, F1=0.9277 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1673, F1=0.9761 | Val: Loss=0.2178, F1=0.9428 (Per-Sample)
Epoch  60/300 | Train: Loss=0.1649, F1=0.9828 | Val: Loss=0.2333, F1=0.9206 (Per-Sample)
Epoch  70/300 | Train: Loss=0.1632, F1=0.9800 | Val: Loss=0.2254, F1=0.9432 (Per-Sample)
Epoch  80/300 | Train: Loss=0.1587, F1=0.9880 | Val: Loss=0.2081, F1=0.9440 (Per-Sample)
Epoch  90/300 | Train: Loss=0.1528, F1=0.9937 | Val: Loss=0.1987, F1=0.9474 (Per-Sample)
Epoch 100/300 | Train: Loss=0.1564, F1=0.9898 | Val: Loss=0.2366, F1=0.9110 (Per-Sample)
Epoch 110/300 | Train: Loss=0.1639, F1=0.9821 | Val: Loss=0.2143, F1=0.9532 (Per-Sample)
Best trial: 21. Best value: -0.963:  70%|███████   | 35/50 [1:33:31<48:45, 195.03s/it]
Early stopping triggered after 119 epochs.
Best model restored from epoch 69 with val_f1 0.9767

Cross-validation score: 0.9573 ± 0.0178
  > Trial 34 Result: Mean F1 = 0.9573
[I 2025-11-10 15:15:07,693] Trial 34 finished with value: -0.9572790978457147 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 40, 'weight_ce_intensity': 1.0, 'label_smoothing_epsilon': 0.1, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.0, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 35 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 20
  > weight_ce_intensity: 0.0
  > label_smoothing_epsilon: 0.05
  > hidden_layers: 3
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 2
  > pain_hidden_size: 64
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.0001
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Best trial: 21. Best value: -0.963:  72%|███████▏  | 36/50 [1:33:31<31:53, 136.69s/it]
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.0): [0. 0. 0.]
  Dynamic Loss: Label Smoothing Epsilon: 0.05
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
NaN loss at batch 0
NaN loss at batch 1
NaN loss at batch 2
NaN loss at batch 3
NaN loss at batch 4
NaN loss at batch 5
NaN loss at batch 6
NaN loss at batch 7
NaN loss at batch 8
NaN loss at batch 9
NaN loss at batch 10
NaN loss at batch 11
NaN loss at batch 12
NaN loss at batch 13
NaN loss at batch 14
NaN loss at batch 15
NaN loss at batch 16
NaN loss at batch 17
NaN loss at batch 18
NaN loss at batch 19
NaN loss at batch 20
NaN loss at batch 21
NaN loss at batch 22
NaN loss at batch 23
NaN loss at batch 24
NaN loss at batch 25
NaN loss at batch 26
NaN loss at batch 27
NaN loss at batch 28
NaN loss at batch 29
NaN loss at batch 30
NaN loss at batch 31
NaN loss at batch 32
NaN loss at batch 33
NaN loss at batch 34
NaN loss at batch 35
NaN loss at batch 36
NaN loss at batch 37
NaN loss at batch 38
NaN loss at batch 39
NaN loss at batch 40
NaN loss at batch 41
NaN loss at batch 42
NaN loss at batch 43
NaN loss at batch 44
NaN loss at batch 45
NaN loss at batch 46
NaN loss at batch 47
NaN loss at batch 48
NaN loss at batch 49
--- ERROR in Trial 35 ---
Configuration: {'batch_size': 32, 'cross_entropy_weighting': True, 'l1_lambda': 0.0, 'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.05, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.1}
Error: need at least one array to concatenate
  > Trial 35 Result: F1 = 0.0 (due to error)
[I 2025-11-10 15:15:08,248] Trial 35 finished with value: 0.0 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 20, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.05, 'hidden_layers': 3, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.1}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 36 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.0
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6536, F1=0.6640 | Val: Loss=0.3823, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0835, F1=0.9027 | Val: Loss=0.1191, F1=0.9154 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0413, F1=0.9754 | Val: Loss=0.1485, F1=0.9449 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0435, F1=0.9818 | Val: Loss=0.2386, F1=0.9543 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0356, F1=0.9827 | Val: Loss=0.4134, F1=0.9494 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0353, F1=0.9873 | Val: Loss=0.2079, F1=0.9696 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0291, F1=0.9937 | Val: Loss=0.2843, F1=0.9490 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0069, F1=0.9968 | Val: Loss=0.3001, F1=0.9433 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0083, F1=0.9956 | Val: Loss=0.4272, F1=0.9381 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0009, F1=0.9994 | Val: Loss=0.3390, F1=0.9433 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0156, F1=0.9981 | Val: Loss=0.3579, F1=0.9449 (Per-Sample)
Early stopping triggered after 100 epochs.
Best model restored from epoch 50 with val_f1 0.9696

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7124, F1=0.6667 | Val: Loss=0.3932, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1012, F1=0.9102 | Val: Loss=0.3746, F1=0.8475 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0400, F1=0.9709 | Val: Loss=0.2772, F1=0.9115 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0300, F1=0.9786 | Val: Loss=0.3259, F1=0.9190 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0184, F1=0.9918 | Val: Loss=0.4808, F1=0.9296 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0361, F1=0.9906 | Val: Loss=0.5444, F1=0.9365 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0187, F1=0.9956 | Val: Loss=0.6111, F1=0.9438 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0029, F1=0.9981 | Val: Loss=0.5882, F1=0.9298 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0022, F1=0.9975 | Val: Loss=0.5499, F1=0.9450 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0117, F1=0.9962 | Val: Loss=0.5717, F1=0.9524 (Per-Sample)
Early stopping triggered after 99 epochs.
Best model restored from epoch 49 with val_f1 0.9526

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6713, F1=0.6631 | Val: Loss=0.3994, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0707, F1=0.9247 | Val: Loss=0.1763, F1=0.8518 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0294, F1=0.9759 | Val: Loss=0.2066, F1=0.9228 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0488, F1=0.9795 | Val: Loss=0.3486, F1=0.9318 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0376, F1=0.9886 | Val: Loss=0.4014, F1=0.9389 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0196, F1=0.9931 | Val: Loss=0.3228, F1=0.9339 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0258, F1=0.9917 | Val: Loss=0.3995, F1=0.9373 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0412, F1=0.9899 | Val: Loss=0.4427, F1=0.9222 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0221, F1=0.9930 | Val: Loss=0.4818, F1=0.9239 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0057, F1=0.9968 | Val: Loss=0.2822, F1=0.9285 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0095, F1=0.9968 | Val: Loss=0.3184, F1=0.9535 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0091, F1=0.9981 | Val: Loss=0.3429, F1=0.9362 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0015, F1=0.9987 | Val: Loss=0.4135, F1=0.9234 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0046, F1=0.9981 | Val: Loss=0.3666, F1=0.9452 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0012, F1=0.9994 | Val: Loss=0.3023, F1=0.9447 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0085, F1=0.9981 | Val: Loss=0.3574, F1=0.9456 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0024, F1=0.9987 | Val: Loss=0.4149, F1=0.9297 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0125, F1=0.9987 | Val: Loss=0.4407, F1=0.9284 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0208, F1=0.9968 | Val: Loss=0.5628, F1=0.9390 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0072, F1=0.9975 | Val: Loss=0.5607, F1=0.9392 (Per-Sample)
Early stopping triggered after 191 epochs.
Best model restored from epoch 141 with val_f1 0.9549

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6649, F1=0.6908 | Val: Loss=0.4088, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0731, F1=0.9039 | Val: Loss=0.1781, F1=0.8238 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0154, F1=0.9838 | Val: Loss=0.2747, F1=0.9129 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0411, F1=0.9814 | Val: Loss=0.3165, F1=0.9369 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0244, F1=0.9924 | Val: Loss=0.3135, F1=0.9279 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0110, F1=0.9924 | Val: Loss=0.4149, F1=0.9301 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0131, F1=0.9911 | Val: Loss=0.3002, F1=0.9454 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0069, F1=0.9962 | Val: Loss=0.3454, F1=0.9296 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0089, F1=0.9968 | Val: Loss=0.3753, F1=0.9375 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0210, F1=0.9943 | Val: Loss=0.3361, F1=0.9375 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0009, F1=0.9975 | Val: Loss=0.4705, F1=0.9460 (Per-Sample)
Early stopping triggered after 106 epochs.
Best model restored from epoch 56 with val_f1 0.9531

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7278, F1=0.6312 | Val: Loss=0.2548, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1117, F1=0.9038 | Val: Loss=0.1372, F1=0.8991 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0398, F1=0.9706 | Val: Loss=0.1651, F1=0.9342 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0471, F1=0.9802 | Val: Loss=0.1674, F1=0.9412 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0155, F1=0.9917 | Val: Loss=0.3466, F1=0.9312 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0125, F1=0.9910 | Val: Loss=0.3114, F1=0.9460 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0609, F1=0.9906 | Val: Loss=0.3929, F1=0.9229 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0107, F1=0.9956 | Val: Loss=0.6353, F1=0.9076 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0052, F1=0.9956 | Val: Loss=0.4358, F1=0.9461 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0359, F1=0.9924 | Val: Loss=0.4937, F1=0.9390 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0091, F1=0.9956 | Val: Loss=0.4688, F1=0.9308 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0032, F1=0.9975 | Val: Loss=0.4859, F1=0.9304 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0176, F1=0.9981 | Val: Loss=0.4648, F1=0.9474 (Per-Sample)
Best trial: 21. Best value: -0.963:  74%|███████▍  | 37/50 [1:35:54<30:00, 138.52s/it]
Early stopping triggered after 129 epochs.
Best model restored from epoch 79 with val_f1 0.9610

Cross-validation score: 0.9582 ± 0.0064
  > Trial 36 Result: Mean F1 = 0.9582
[I 2025-11-10 15:17:31,033] Trial 36 finished with value: -0.9582441468585554 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.0, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 37 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6068, F1=0.7152 | Val: Loss=0.3977, F1=0.7327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0975, F1=0.9039 | Val: Loss=0.1171, F1=0.9080 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0595, F1=0.9498 | Val: Loss=0.1158, F1=0.9431 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0324, F1=0.9750 | Val: Loss=0.1590, F1=0.9606 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0230, F1=0.9840 | Val: Loss=0.2389, F1=0.9494 (Per-Sample)
Epoch  50/300 | Train: Loss=0.1019, F1=0.9738 | Val: Loss=0.2412, F1=0.9529 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0299, F1=0.9879 | Val: Loss=0.4509, F1=0.9382 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0068, F1=0.9962 | Val: Loss=0.2676, F1=0.9597 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0157, F1=0.9975 | Val: Loss=0.4903, F1=0.9515 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0133, F1=0.9968 | Val: Loss=0.4457, F1=0.9515 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0393, F1=0.9956 | Val: Loss=0.3490, F1=0.9428 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0034, F1=0.9987 | Val: Loss=0.4205, F1=0.9414 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0031, F1=0.9987 | Val: Loss=0.3365, F1=0.9494 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0075, F1=0.9968 | Val: Loss=0.3646, F1=0.9419 (Per-Sample)
Early stopping triggered after 132 epochs.
Best model restored from epoch 82 with val_f1 0.9689

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6204, F1=0.7098 | Val: Loss=0.3540, F1=0.6985 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1062, F1=0.8972 | Val: Loss=0.1949, F1=0.8725 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0528, F1=0.9546 | Val: Loss=0.2128, F1=0.9123 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0489, F1=0.9701 | Val: Loss=0.2860, F1=0.9363 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0437, F1=0.9827 | Val: Loss=0.3894, F1=0.9155 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0205, F1=0.9918 | Val: Loss=0.4852, F1=0.9379 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0220, F1=0.9924 | Val: Loss=0.4443, F1=0.9374 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0124, F1=0.9937 | Val: Loss=0.5430, F1=0.9080 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0024, F1=0.9987 | Val: Loss=0.6355, F1=0.9535 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0214, F1=0.9975 | Val: Loss=0.4219, F1=0.9694 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0084, F1=0.9994 | Val: Loss=0.6460, F1=0.9309 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.7232, F1=0.9376 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0224, F1=0.9949 | Val: Loss=0.8962, F1=0.9221 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0101, F1=0.9987 | Val: Loss=0.6668, F1=0.9449 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0091, F1=0.9987 | Val: Loss=1.0207, F1=0.9070 (Per-Sample)
Early stopping triggered after 140 epochs.
Best model restored from epoch 90 with val_f1 0.9694

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5848, F1=0.7379 | Val: Loss=0.3718, F1=0.6944 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0964, F1=0.9010 | Val: Loss=0.3201, F1=0.8298 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0375, F1=0.9681 | Val: Loss=0.2766, F1=0.8690 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0224, F1=0.9805 | Val: Loss=0.2546, F1=0.8778 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0622, F1=0.9777 | Val: Loss=0.3571, F1=0.8964 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0096, F1=0.9917 | Val: Loss=0.4521, F1=0.9049 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0281, F1=0.9956 | Val: Loss=0.5456, F1=0.9231 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0401, F1=0.9899 | Val: Loss=0.3841, F1=0.9300 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0153, F1=0.9981 | Val: Loss=0.5873, F1=0.9306 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0026, F1=0.9994 | Val: Loss=0.5880, F1=0.9394 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.4958, F1=0.9262 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0053, F1=0.9975 | Val: Loss=0.5970, F1=0.9293 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5418, F1=0.9201 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0030, F1=0.9994 | Val: Loss=0.5006, F1=0.9300 (Per-Sample)
Early stopping triggered after 133 epochs.
Best model restored from epoch 83 with val_f1 0.9394

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6069, F1=0.7301 | Val: Loss=0.4127, F1=0.6517 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0748, F1=0.9174 | Val: Loss=0.1523, F1=0.8672 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0397, F1=0.9605 | Val: Loss=0.1963, F1=0.8825 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0332, F1=0.9813 | Val: Loss=0.2813, F1=0.9206 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0284, F1=0.9866 | Val: Loss=0.4347, F1=0.9287 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0228, F1=0.9854 | Val: Loss=0.5206, F1=0.9128 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0243, F1=0.9943 | Val: Loss=0.5987, F1=0.9059 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0188, F1=0.9975 | Val: Loss=0.5987, F1=0.9139 (Per-Sample)
Early stopping triggered after 75 epochs.
Best model restored from epoch 25 with val_f1 0.9531

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6092, F1=0.6993 | Val: Loss=0.2541, F1=0.7625 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0959, F1=0.8901 | Val: Loss=0.1397, F1=0.8633 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0514, F1=0.9487 | Val: Loss=0.1898, F1=0.9159 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0323, F1=0.9745 | Val: Loss=0.2306, F1=0.9334 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0325, F1=0.9796 | Val: Loss=0.3679, F1=0.9195 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0225, F1=0.9879 | Val: Loss=0.3865, F1=0.9219 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0247, F1=0.9930 | Val: Loss=0.3568, F1=0.9521 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0122, F1=0.9975 | Val: Loss=0.5065, F1=0.9462 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0148, F1=0.9968 | Val: Loss=0.5720, F1=0.9378 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0264, F1=0.9949 | Val: Loss=0.4537, F1=0.9265 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0032, F1=0.9987 | Val: Loss=0.5445, F1=0.9265 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.6286, F1=0.9294 (Per-Sample)
Best trial: 21. Best value: -0.963:  76%|███████▌  | 38/50 [1:38:08<27:27, 137.25s/it]
Early stopping triggered after 117 epochs.
Best model restored from epoch 67 with val_f1 0.9615

Cross-validation score: 0.9585 ± 0.0112
  > Trial 37 Result: Mean F1 = 0.9585
[I 2025-11-10 15:19:45,330] Trial 37 finished with value: -0.9584663756575795 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 38 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6098, F1=0.7191 | Val: Loss=0.3604, F1=0.7488 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0986, F1=0.8949 | Val: Loss=0.1576, F1=0.8931 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0805, F1=0.9515 | Val: Loss=0.2144, F1=0.9120 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0448, F1=0.9722 | Val: Loss=0.1944, F1=0.9220 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0534, F1=0.9775 | Val: Loss=0.2342, F1=0.9515 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0109, F1=0.9918 | Val: Loss=0.3094, F1=0.9332 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0216, F1=0.9962 | Val: Loss=0.2245, F1=0.9414 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0088, F1=0.9968 | Val: Loss=0.2481, F1=0.9414 (Per-Sample)
Early stopping triggered after 73 epochs.
Best model restored from epoch 23 with val_f1 0.9606

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6736, F1=0.6648 | Val: Loss=0.3598, F1=0.7118 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0816, F1=0.9094 | Val: Loss=0.2004, F1=0.8585 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0500, F1=0.9583 | Val: Loss=0.2223, F1=0.9190 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0263, F1=0.9785 | Val: Loss=0.2974, F1=0.8938 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0331, F1=0.9819 | Val: Loss=0.4012, F1=0.8973 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0345, F1=0.9873 | Val: Loss=0.5529, F1=0.9116 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0155, F1=0.9931 | Val: Loss=0.6731, F1=0.9136 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0233, F1=0.9950 | Val: Loss=0.5408, F1=0.9312 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0188, F1=0.9956 | Val: Loss=0.6393, F1=0.9292 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.6099, F1=0.9304 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0079, F1=0.9981 | Val: Loss=0.6574, F1=0.9309 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0079, F1=0.9975 | Val: Loss=0.5980, F1=0.9376 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0120, F1=0.9956 | Val: Loss=0.6901, F1=0.9237 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0090, F1=0.9975 | Val: Loss=0.7802, F1=0.9386 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0021, F1=0.9981 | Val: Loss=0.8281, F1=0.9376 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0078, F1=0.9981 | Val: Loss=0.6954, F1=0.9221 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0072, F1=0.9987 | Val: Loss=0.6899, F1=0.9230 (Per-Sample)
Early stopping triggered after 166 epochs.
Best model restored from epoch 116 with val_f1 0.9476

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6173, F1=0.7294 | Val: Loss=0.3707, F1=0.6944 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0837, F1=0.9003 | Val: Loss=0.1943, F1=0.8294 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0444, F1=0.9502 | Val: Loss=0.2128, F1=0.8809 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0442, F1=0.9664 | Val: Loss=0.2887, F1=0.9136 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0403, F1=0.9825 | Val: Loss=0.4334, F1=0.9315 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0187, F1=0.9898 | Val: Loss=0.6131, F1=0.9096 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0413, F1=0.9886 | Val: Loss=0.6396, F1=0.9086 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0113, F1=0.9956 | Val: Loss=0.6238, F1=0.9145 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0163, F1=0.9975 | Val: Loss=0.6157, F1=0.9228 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0322, F1=0.9956 | Val: Loss=0.3878, F1=0.9285 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0101, F1=0.9968 | Val: Loss=0.6497, F1=0.9153 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0010, F1=0.9994 | Val: Loss=0.5969, F1=0.9385 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.5676, F1=0.9470 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0064, F1=0.9975 | Val: Loss=0.4971, F1=0.9394 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0011, F1=0.9994 | Val: Loss=0.5541, F1=0.9293 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0001, F1=0.9994 | Val: Loss=0.7516, F1=0.9321 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0010, F1=0.9994 | Val: Loss=0.5540, F1=0.9381 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5234, F1=0.9297 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5661, F1=0.9388 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.6105, F1=0.9396 (Per-Sample)
Early stopping triggered after 197 epochs.
Best model restored from epoch 147 with val_f1 0.9538

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6335, F1=0.7087 | Val: Loss=0.3826, F1=0.6517 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0890, F1=0.9030 | Val: Loss=0.2027, F1=0.8025 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0621, F1=0.9424 | Val: Loss=0.1999, F1=0.8919 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0476, F1=0.9740 | Val: Loss=0.3396, F1=0.8707 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0446, F1=0.9814 | Val: Loss=0.3175, F1=0.9047 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0279, F1=0.9918 | Val: Loss=0.5081, F1=0.9368 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0046, F1=0.9962 | Val: Loss=0.6486, F1=0.9054 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0292, F1=0.9962 | Val: Loss=0.5713, F1=0.9297 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0171, F1=0.9962 | Val: Loss=0.6308, F1=0.9140 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0064, F1=0.9981 | Val: Loss=0.6349, F1=0.9296 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0022, F1=0.9975 | Val: Loss=0.6636, F1=0.9296 (Per-Sample)
Early stopping triggered after 102 epochs.
Best model restored from epoch 52 with val_f1 0.9370

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6313, F1=0.6933 | Val: Loss=0.2515, F1=0.7785 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0958, F1=0.8820 | Val: Loss=0.1336, F1=0.8852 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0515, F1=0.9539 | Val: Loss=0.2417, F1=0.9260 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0247, F1=0.9739 | Val: Loss=0.1942, F1=0.9450 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0296, F1=0.9859 | Val: Loss=0.3933, F1=0.9290 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0559, F1=0.9905 | Val: Loss=0.4615, F1=0.9290 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0325, F1=0.9956 | Val: Loss=0.4233, F1=0.9531 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0420, F1=0.9931 | Val: Loss=0.4956, F1=0.9377 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0099, F1=0.9987 | Val: Loss=0.4537, F1=0.9377 (Per-Sample)
Best trial: 21. Best value: -0.963:  78%|███████▊  | 39/50 [1:40:26<25:11, 137.40s/it]
Early stopping triggered after 85 epochs.
Best model restored from epoch 35 with val_f1 0.9544

Cross-validation score: 0.9507 ± 0.0080
  > Trial 38 Result: Mean F1 = 0.9507
[I 2025-11-10 15:22:03,068] Trial 38 finished with value: -0.950675438849807 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 39 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5961, F1=0.7055 | Val: Loss=0.3557, F1=0.7327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1049, F1=0.8845 | Val: Loss=0.1389, F1=0.8823 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0708, F1=0.9483 | Val: Loss=0.2383, F1=0.9356 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0682, F1=0.9541 | Val: Loss=0.1487, F1=0.9416 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0339, F1=0.9795 | Val: Loss=0.3182, F1=0.9147 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0439, F1=0.9847 | Val: Loss=0.4682, F1=0.9207 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0240, F1=0.9937 | Val: Loss=0.4227, F1=0.9530 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0500, F1=0.9924 | Val: Loss=0.4514, F1=0.9442 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0143, F1=0.9981 | Val: Loss=0.5812, F1=0.9433 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0149, F1=0.9981 | Val: Loss=0.4262, F1=0.9515 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0230, F1=0.9969 | Val: Loss=0.6130, F1=0.9332 (Per-Sample)
Early stopping triggered after 101 epochs.
Best model restored from epoch 51 with val_f1 0.9621

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6191, F1=0.7248 | Val: Loss=0.3630, F1=0.6985 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1225, F1=0.8977 | Val: Loss=0.2298, F1=0.8664 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0448, F1=0.9533 | Val: Loss=0.2453, F1=0.9019 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0238, F1=0.9792 | Val: Loss=0.3186, F1=0.9174 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0238, F1=0.9871 | Val: Loss=0.3331, F1=0.9456 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0518, F1=0.9853 | Val: Loss=0.4208, F1=0.9137 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0188, F1=0.9949 | Val: Loss=0.4742, F1=0.9368 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0363, F1=0.9968 | Val: Loss=0.5428, F1=0.9191 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0031, F1=0.9987 | Val: Loss=0.5962, F1=0.9363 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0151, F1=0.9975 | Val: Loss=0.6485, F1=0.9270 (Per-Sample)
Early stopping triggered after 92 epochs.
Best model restored from epoch 42 with val_f1 0.9466

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5728, F1=0.7352 | Val: Loss=0.3891, F1=0.6962 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0840, F1=0.8991 | Val: Loss=0.1449, F1=0.8527 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0499, F1=0.9691 | Val: Loss=0.3167, F1=0.8683 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0231, F1=0.9831 | Val: Loss=0.3235, F1=0.9242 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0598, F1=0.9854 | Val: Loss=0.2455, F1=0.9300 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0110, F1=0.9968 | Val: Loss=0.3595, F1=0.9215 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0233, F1=0.9937 | Val: Loss=0.3840, F1=0.9306 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0157, F1=0.9956 | Val: Loss=0.3431, F1=0.9289 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0115, F1=0.9981 | Val: Loss=0.4174, F1=0.9309 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0007, F1=0.9987 | Val: Loss=0.4102, F1=0.9378 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0535, F1=0.9918 | Val: Loss=0.5037, F1=0.9130 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0109, F1=0.9968 | Val: Loss=0.5057, F1=0.9298 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0113, F1=0.9975 | Val: Loss=0.5707, F1=0.9138 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0054, F1=0.9994 | Val: Loss=0.3635, F1=0.9177 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0164, F1=0.9981 | Val: Loss=0.5653, F1=0.9275 (Per-Sample)
Early stopping triggered after 144 epochs.
Best model restored from epoch 94 with val_f1 0.9618

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5640, F1=0.7388 | Val: Loss=0.3912, F1=0.6517 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0835, F1=0.8987 | Val: Loss=0.2012, F1=0.8102 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0650, F1=0.9586 | Val: Loss=0.1486, F1=0.9178 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0289, F1=0.9703 | Val: Loss=0.2191, F1=0.9106 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0584, F1=0.9834 | Val: Loss=0.3454, F1=0.9119 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0185, F1=0.9892 | Val: Loss=0.4149, F1=0.9198 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0094, F1=0.9969 | Val: Loss=0.5289, F1=0.9066 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0036, F1=0.9981 | Val: Loss=0.4595, F1=0.9208 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0157, F1=0.9956 | Val: Loss=0.4455, F1=0.9140 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0105, F1=0.9994 | Val: Loss=0.5166, F1=0.9214 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5261, F1=0.9140 (Per-Sample)
Early stopping triggered after 104 epochs.
Best model restored from epoch 54 with val_f1 0.9437

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6095, F1=0.6904 | Val: Loss=0.2492, F1=0.7785 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0876, F1=0.9010 | Val: Loss=0.1419, F1=0.8870 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0633, F1=0.9420 | Val: Loss=0.2004, F1=0.9186 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0389, F1=0.9761 | Val: Loss=0.2693, F1=0.9377 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0551, F1=0.9810 | Val: Loss=0.4187, F1=0.9531 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0058, F1=0.9968 | Val: Loss=0.4877, F1=0.9273 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0309, F1=0.9949 | Val: Loss=0.5299, F1=0.9447 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0166, F1=0.9975 | Val: Loss=0.5798, F1=0.9369 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0017, F1=0.9987 | Val: Loss=0.6339, F1=0.9202 (Per-Sample)
Best trial: 21. Best value: -0.963:  80%|████████  | 40/50 [1:42:24<21:54, 131.44s/it]
Early stopping triggered after 89 epochs.
Best model restored from epoch 39 with val_f1 0.9531

Cross-validation score: 0.9535 ± 0.0076
  > Trial 39 Result: Mean F1 = 0.9535
[I 2025-11-10 15:24:00,594] Trial 39 finished with value: -0.9534569870508575 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 40 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5934, F1=0.7177 | Val: Loss=0.3535, F1=0.7506 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1096, F1=0.8971 | Val: Loss=0.1320, F1=0.8917 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0704, F1=0.9382 | Val: Loss=0.1279, F1=0.9407 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0823, F1=0.9653 | Val: Loss=0.1120, F1=0.9510 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0352, F1=0.9808 | Val: Loss=0.2147, F1=0.9301 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0279, F1=0.9930 | Val: Loss=0.2788, F1=0.9354 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0103, F1=0.9962 | Val: Loss=0.3333, F1=0.9332 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0100, F1=0.9994 | Val: Loss=0.3177, F1=0.9442 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0068, F1=0.9987 | Val: Loss=0.4129, F1=0.9408 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0003, F1=1.0000 | Val: Loss=0.3766, F1=0.9408 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0193, F1=0.9949 | Val: Loss=0.3479, F1=0.9487 (Per-Sample)
Early stopping triggered after 101 epochs.
Best model restored from epoch 51 with val_f1 0.9606

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6089, F1=0.7167 | Val: Loss=0.3537, F1=0.7131 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0883, F1=0.8936 | Val: Loss=0.1874, F1=0.8618 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0523, F1=0.9500 | Val: Loss=0.1740, F1=0.9093 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0427, F1=0.9709 | Val: Loss=0.3054, F1=0.9038 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0304, F1=0.9839 | Val: Loss=0.5034, F1=0.8994 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0371, F1=0.9886 | Val: Loss=0.6925, F1=0.9057 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0150, F1=0.9962 | Val: Loss=0.6093, F1=0.9123 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0061, F1=0.9949 | Val: Loss=0.8043, F1=0.9082 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0179, F1=0.9956 | Val: Loss=0.7515, F1=0.9145 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0012, F1=0.9987 | Val: Loss=0.6693, F1=0.9062 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0003, F1=1.0000 | Val: Loss=0.7818, F1=0.9093 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0047, F1=0.9994 | Val: Loss=0.7453, F1=0.9220 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0043, F1=0.9987 | Val: Loss=0.8706, F1=0.9125 (Per-Sample)
Early stopping triggered after 129 epochs.
Best model restored from epoch 79 with val_f1 0.9470

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6091, F1=0.7233 | Val: Loss=0.3521, F1=0.7073 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0970, F1=0.9069 | Val: Loss=0.2218, F1=0.8325 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0397, F1=0.9609 | Val: Loss=0.1708, F1=0.9205 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0723, F1=0.9624 | Val: Loss=0.2349, F1=0.9133 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0500, F1=0.9711 | Val: Loss=0.4203, F1=0.8768 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0149, F1=0.9910 | Val: Loss=0.4906, F1=0.9228 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0131, F1=0.9949 | Val: Loss=0.6031, F1=0.9255 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0021, F1=0.9975 | Val: Loss=0.4630, F1=0.9394 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0161, F1=0.9975 | Val: Loss=0.5892, F1=0.9398 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5984, F1=0.9228 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0012, F1=0.9994 | Val: Loss=0.6118, F1=0.9398 (Per-Sample)
Early stopping triggered after 103 epochs.
Best model restored from epoch 53 with val_f1 0.9539

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5890, F1=0.7357 | Val: Loss=0.3864, F1=0.6517 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0949, F1=0.8978 | Val: Loss=0.1993, F1=0.8015 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0608, F1=0.9413 | Val: Loss=0.2207, F1=0.8632 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0379, F1=0.9866 | Val: Loss=0.3509, F1=0.9066 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0230, F1=0.9911 | Val: Loss=0.6262, F1=0.8922 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0247, F1=0.9930 | Val: Loss=0.4102, F1=0.9295 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0114, F1=0.9968 | Val: Loss=0.6034, F1=0.9068 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0229, F1=0.9968 | Val: Loss=0.4933, F1=0.9144 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0017, F1=0.9987 | Val: Loss=0.7102, F1=0.9222 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0032, F1=0.9987 | Val: Loss=0.6482, F1=0.9138 (Per-Sample)
Early stopping triggered after 95 epochs.
Best model restored from epoch 45 with val_f1 0.9390

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5993, F1=0.7012 | Val: Loss=0.2526, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1087, F1=0.9017 | Val: Loss=0.1737, F1=0.8732 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0398, F1=0.9632 | Val: Loss=0.2257, F1=0.9197 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0419, F1=0.9795 | Val: Loss=0.3340, F1=0.8904 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0391, F1=0.9846 | Val: Loss=0.4293, F1=0.9459 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0854, F1=0.9803 | Val: Loss=0.3922, F1=0.9229 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0149, F1=0.9981 | Val: Loss=0.4166, F1=0.9544 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0213, F1=0.9956 | Val: Loss=0.6280, F1=0.9312 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5778, F1=0.9478 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0081, F1=0.9981 | Val: Loss=0.4877, F1=0.9375 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0074, F1=0.9994 | Val: Loss=0.5784, F1=0.9214 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0046, F1=0.9975 | Val: Loss=0.7011, F1=0.9301 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0035, F1=0.9981 | Val: Loss=0.6638, F1=0.9236 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0095, F1=0.9987 | Val: Loss=0.6885, F1=0.9390 (Per-Sample)
Best trial: 21. Best value: -0.963:  82%|████████▏ | 41/50 [1:44:28<19:24, 129.36s/it]
Early stopping triggered after 134 epochs.
Best model restored from epoch 84 with val_f1 0.9602

Cross-validation score: 0.9521 ± 0.0082
  > Trial 40 Result: Mean F1 = 0.9521
[I 2025-11-10 15:26:05,103] Trial 40 finished with value: -0.952133831260535 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 41 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6264, F1=0.7148 | Val: Loss=0.3597, F1=0.7637 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1010, F1=0.8899 | Val: Loss=0.1530, F1=0.8825 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0533, F1=0.9505 | Val: Loss=0.1632, F1=0.9260 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0645, F1=0.9698 | Val: Loss=0.2416, F1=0.9080 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0279, F1=0.9846 | Val: Loss=0.2634, F1=0.9442 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0254, F1=0.9898 | Val: Loss=0.3865, F1=0.9515 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0388, F1=0.9912 | Val: Loss=0.3696, F1=0.9334 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0333, F1=0.9950 | Val: Loss=0.4715, F1=0.9220 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0049, F1=0.9981 | Val: Loss=0.6022, F1=0.9264 (Per-Sample)
Early stopping triggered after 82 epochs.
Best model restored from epoch 32 with val_f1 0.9606

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6136, F1=0.7167 | Val: Loss=0.3548, F1=0.7131 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0983, F1=0.8984 | Val: Loss=0.1930, F1=0.8805 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0397, F1=0.9639 | Val: Loss=0.2017, F1=0.9053 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0501, F1=0.9789 | Val: Loss=0.3325, F1=0.9053 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0288, F1=0.9872 | Val: Loss=0.4206, F1=0.9226 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0100, F1=0.9949 | Val: Loss=0.5254, F1=0.9241 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0089, F1=0.9981 | Val: Loss=0.6291, F1=0.9459 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0014, F1=0.9987 | Val: Loss=0.4963, F1=0.9528 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0137, F1=0.9962 | Val: Loss=0.4281, F1=0.9281 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0065, F1=0.9975 | Val: Loss=0.5476, F1=0.9215 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0133, F1=0.9975 | Val: Loss=0.6225, F1=0.9211 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0093, F1=0.9981 | Val: Loss=0.5899, F1=0.9368 (Per-Sample)
Early stopping triggered after 118 epochs.
Best model restored from epoch 68 with val_f1 0.9528

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6268, F1=0.7260 | Val: Loss=0.3585, F1=0.7073 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1084, F1=0.9013 | Val: Loss=0.1615, F1=0.8795 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0645, F1=0.9408 | Val: Loss=0.1831, F1=0.8860 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0311, F1=0.9790 | Val: Loss=0.2765, F1=0.9161 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0141, F1=0.9890 | Val: Loss=0.4401, F1=0.9303 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0546, F1=0.9880 | Val: Loss=0.3758, F1=0.9303 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0416, F1=0.9899 | Val: Loss=0.4552, F1=0.9388 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0099, F1=0.9962 | Val: Loss=0.4661, F1=0.9312 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0081, F1=0.9981 | Val: Loss=0.6036, F1=0.9234 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0131, F1=0.9994 | Val: Loss=0.5406, F1=0.9299 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0056, F1=0.9994 | Val: Loss=0.5597, F1=0.9388 (Per-Sample)
Early stopping triggered after 102 epochs.
Best model restored from epoch 52 with val_f1 0.9467

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6266, F1=0.7190 | Val: Loss=0.3930, F1=0.6530 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0878, F1=0.9079 | Val: Loss=0.1866, F1=0.8036 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0509, F1=0.9595 | Val: Loss=0.2980, F1=0.9073 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0529, F1=0.9788 | Val: Loss=0.3116, F1=0.9295 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0309, F1=0.9872 | Val: Loss=0.4555, F1=0.9301 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0211, F1=0.9911 | Val: Loss=0.5998, F1=0.9294 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0163, F1=0.9968 | Val: Loss=0.7911, F1=0.9081 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0223, F1=0.9918 | Val: Loss=0.7775, F1=0.9080 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0072, F1=0.9981 | Val: Loss=0.6161, F1=0.9002 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0181, F1=0.9975 | Val: Loss=0.6044, F1=0.9144 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.6782, F1=0.9151 (Per-Sample)
Early stopping triggered after 103 epochs.
Best model restored from epoch 53 with val_f1 0.9458

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6094, F1=0.6972 | Val: Loss=0.2609, F1=0.7785 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0901, F1=0.8903 | Val: Loss=0.1600, F1=0.8667 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0469, F1=0.9670 | Val: Loss=0.2298, F1=0.9094 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0335, F1=0.9742 | Val: Loss=0.2671, F1=0.9159 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0406, F1=0.9885 | Val: Loss=0.3940, F1=0.9365 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0329, F1=0.9880 | Val: Loss=0.4339, F1=0.9273 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0127, F1=0.9968 | Val: Loss=0.5217, F1=0.9301 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0353, F1=0.9930 | Val: Loss=0.6024, F1=0.9219 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0109, F1=0.9975 | Val: Loss=0.6098, F1=0.9304 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0183, F1=0.9975 | Val: Loss=0.6324, F1=0.9219 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0010, F1=0.9994 | Val: Loss=0.8960, F1=0.9219 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0003, F1=0.9994 | Val: Loss=0.8293, F1=0.9219 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0258, F1=0.9981 | Val: Loss=0.7580, F1=0.9161 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0080, F1=0.9975 | Val: Loss=0.6424, F1=0.9301 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0079, F1=0.9994 | Val: Loss=0.7648, F1=0.9053 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0042, F1=0.9994 | Val: Loss=0.6828, F1=0.9378 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.8202, F1=0.9212 (Per-Sample)
Best trial: 21. Best value: -0.963:  84%|████████▍ | 42/50 [1:46:33<17:04, 128.07s/it]
Early stopping triggered after 163 epochs.
Best model restored from epoch 113 with val_f1 0.9531

Cross-validation score: 0.9518 ± 0.0053
  > Trial 41 Result: Mean F1 = 0.9518
[I 2025-11-10 15:28:10,173] Trial 41 finished with value: -0.9517932793993349 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 42 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6353, F1=0.7025 | Val: Loss=0.3598, F1=0.7488 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1034, F1=0.8831 | Val: Loss=0.1446, F1=0.9182 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0588, F1=0.9458 | Val: Loss=0.0681, F1=0.9498 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0494, F1=0.9737 | Val: Loss=0.0723, F1=0.9590 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0474, F1=0.9821 | Val: Loss=0.1767, F1=0.9301 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0418, F1=0.9886 | Val: Loss=0.2936, F1=0.9431 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0245, F1=0.9937 | Val: Loss=0.2283, F1=0.9604 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0123, F1=0.9962 | Val: Loss=0.3649, F1=0.9367 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0273, F1=0.9950 | Val: Loss=0.4323, F1=0.9522 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0050, F1=0.9975 | Val: Loss=0.4754, F1=0.9333 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0059, F1=0.9975 | Val: Loss=0.2532, F1=0.9606 (Per-Sample)
Early stopping triggered after 102 epochs.
Best model restored from epoch 52 with val_f1 0.9764

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6497, F1=0.6975 | Val: Loss=0.3650, F1=0.6985 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1065, F1=0.9007 | Val: Loss=0.1902, F1=0.8592 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0656, F1=0.9494 | Val: Loss=0.2156, F1=0.9070 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0240, F1=0.9798 | Val: Loss=0.3232, F1=0.9032 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0354, F1=0.9826 | Val: Loss=0.5347, F1=0.9043 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0448, F1=0.9847 | Val: Loss=0.5011, F1=0.9384 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0144, F1=0.9956 | Val: Loss=0.5908, F1=0.9309 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0051, F1=0.9981 | Val: Loss=0.5712, F1=0.9365 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0183, F1=0.9962 | Val: Loss=0.6671, F1=0.9043 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0153, F1=0.9943 | Val: Loss=0.7718, F1=0.9207 (Per-Sample)
Early stopping triggered after 96 epochs.
Best model restored from epoch 46 with val_f1 0.9408

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6027, F1=0.7299 | Val: Loss=0.3751, F1=0.6830 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1113, F1=0.8976 | Val: Loss=0.2028, F1=0.8312 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0463, F1=0.9595 | Val: Loss=0.1941, F1=0.9133 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0449, F1=0.9767 | Val: Loss=0.2410, F1=0.9299 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0410, F1=0.9852 | Val: Loss=0.3337, F1=0.9395 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0114, F1=0.9937 | Val: Loss=0.3267, F1=0.9381 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0381, F1=0.9918 | Val: Loss=0.5273, F1=0.9403 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0616, F1=0.9847 | Val: Loss=0.3317, F1=0.9537 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0074, F1=0.9968 | Val: Loss=0.3837, F1=0.9532 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0028, F1=0.9981 | Val: Loss=0.5288, F1=0.9251 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0069, F1=0.9981 | Val: Loss=0.4845, F1=0.9366 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.4580, F1=0.9471 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0002, F1=0.9994 | Val: Loss=0.4557, F1=0.9378 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0043, F1=0.9981 | Val: Loss=0.5009, F1=0.9535 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0032, F1=0.9987 | Val: Loss=0.6279, F1=0.9548 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0013, F1=0.9994 | Val: Loss=0.5846, F1=0.9227 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5067, F1=0.9537 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5616, F1=0.9314 (Per-Sample)
Early stopping triggered after 179 epochs.
Best model restored from epoch 129 with val_f1 0.9691

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5900, F1=0.7240 | Val: Loss=0.3947, F1=0.6517 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1075, F1=0.8943 | Val: Loss=0.1709, F1=0.8317 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0487, F1=0.9544 | Val: Loss=0.1756, F1=0.9030 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0562, F1=0.9716 | Val: Loss=0.2807, F1=0.9217 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0487, F1=0.9787 | Val: Loss=0.3996, F1=0.9144 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0043, F1=0.9949 | Val: Loss=0.4752, F1=0.9217 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0290, F1=0.9943 | Val: Loss=0.4780, F1=0.9197 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0002, F1=0.9994 | Val: Loss=0.5260, F1=0.9197 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0125, F1=0.9968 | Val: Loss=0.4735, F1=0.9286 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0178, F1=0.9962 | Val: Loss=0.4660, F1=0.9286 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0133, F1=0.9943 | Val: Loss=0.4602, F1=0.9211 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0097, F1=0.9969 | Val: Loss=0.7307, F1=0.9211 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0325, F1=0.9968 | Val: Loss=0.5706, F1=0.9295 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0104, F1=0.9987 | Val: Loss=0.3959, F1=0.9284 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0101, F1=0.9987 | Val: Loss=0.6617, F1=0.9224 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0103, F1=0.9994 | Val: Loss=0.7326, F1=0.9227 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.6760, F1=0.9138 (Per-Sample)
Early stopping triggered after 168 epochs.
Best model restored from epoch 118 with val_f1 0.9383

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6396, F1=0.6965 | Val: Loss=0.2607, F1=0.7785 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0947, F1=0.8935 | Val: Loss=0.1580, F1=0.8791 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0467, F1=0.9564 | Val: Loss=0.2244, F1=0.9186 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0570, F1=0.9748 | Val: Loss=0.2276, F1=0.9437 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0566, F1=0.9814 | Val: Loss=0.3510, F1=0.9330 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0366, F1=0.9892 | Val: Loss=0.4676, F1=0.9378 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0189, F1=0.9949 | Val: Loss=0.3859, F1=0.9390 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0310, F1=0.9949 | Val: Loss=0.5237, F1=0.9373 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0094, F1=0.9981 | Val: Loss=0.5855, F1=0.9312 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0237, F1=0.9962 | Val: Loss=0.6000, F1=0.9245 (Per-Sample)
Best trial: 21. Best value: -0.963:  86%|████████▌ | 43/50 [1:48:56<15:28, 132.60s/it]
Early stopping triggered after 98 epochs.
Best model restored from epoch 48 with val_f1 0.9531

Cross-validation score: 0.9555 ± 0.0151
  > Trial 42 Result: Mean F1 = 0.9555
[I 2025-11-10 15:30:33,350] Trial 42 finished with value: -0.9555392355927286 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 43 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 32
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 64
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.3
  > l2_lambda: 0.0
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7153, F1=0.6531 | Val: Loss=0.3959, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1133, F1=0.8504 | Val: Loss=0.1212, F1=0.8923 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0812, F1=0.9271 | Val: Loss=0.1199, F1=0.9360 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0338, F1=0.9739 | Val: Loss=0.1578, F1=0.9442 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0263, F1=0.9786 | Val: Loss=0.2416, F1=0.9380 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0388, F1=0.9848 | Val: Loss=0.3925, F1=0.9407 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0236, F1=0.9930 | Val: Loss=0.4600, F1=0.9380 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0110, F1=0.9962 | Val: Loss=0.3920, F1=0.9482 (Per-Sample)
Early stopping triggered after 75 epochs.
Best model restored from epoch 25 with val_f1 0.9523

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6458, F1=0.6734 | Val: Loss=0.3963, F1=0.6807 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0814, F1=0.8814 | Val: Loss=0.1607, F1=0.8578 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0404, F1=0.9531 | Val: Loss=0.1778, F1=0.8834 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0347, F1=0.9715 | Val: Loss=0.2186, F1=0.9268 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0484, F1=0.9800 | Val: Loss=0.3098, F1=0.9310 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0656, F1=0.9815 | Val: Loss=0.3588, F1=0.9175 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0653, F1=0.9893 | Val: Loss=0.4049, F1=0.9291 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0174, F1=0.9949 | Val: Loss=0.5838, F1=0.9289 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0118, F1=0.9962 | Val: Loss=0.5681, F1=0.9291 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0165, F1=0.9969 | Val: Loss=0.4914, F1=0.9281 (Per-Sample)
Early stopping triggered after 94 epochs.
Best model restored from epoch 44 with val_f1 0.9456

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6146, F1=0.6874 | Val: Loss=0.4023, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0936, F1=0.8887 | Val: Loss=0.1694, F1=0.8331 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0434, F1=0.9539 | Val: Loss=0.1756, F1=0.8777 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0320, F1=0.9637 | Val: Loss=0.2034, F1=0.8704 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0548, F1=0.9690 | Val: Loss=0.2857, F1=0.8895 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0351, F1=0.9859 | Val: Loss=0.3732, F1=0.9134 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0087, F1=0.9917 | Val: Loss=0.4984, F1=0.9151 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0223, F1=0.9937 | Val: Loss=0.4785, F1=0.8967 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0200, F1=0.9950 | Val: Loss=0.5426, F1=0.8972 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0059, F1=0.9975 | Val: Loss=0.4355, F1=0.9094 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0027, F1=0.9994 | Val: Loss=0.5831, F1=0.8992 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0023, F1=0.9987 | Val: Loss=0.5297, F1=0.9141 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0173, F1=0.9981 | Val: Loss=0.5108, F1=0.9116 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0008, F1=1.0000 | Val: Loss=0.4647, F1=0.9093 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5703, F1=0.9145 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0153, F1=0.9981 | Val: Loss=0.6599, F1=0.9058 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0017, F1=0.9994 | Val: Loss=0.5081, F1=0.9303 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0003, F1=0.9994 | Val: Loss=0.4786, F1=0.9194 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0278, F1=0.9962 | Val: Loss=0.6540, F1=0.9309 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0056, F1=0.9987 | Val: Loss=0.5108, F1=0.9275 (Per-Sample)
Epoch 200/300 | Train: Loss=0.0307, F1=0.9962 | Val: Loss=0.6523, F1=0.9300 (Per-Sample)
Epoch 210/300 | Train: Loss=0.0067, F1=0.9987 | Val: Loss=0.6498, F1=0.9241 (Per-Sample)
Epoch 220/300 | Train: Loss=0.0005, F1=0.9994 | Val: Loss=0.5426, F1=0.9381 (Per-Sample)
Epoch 230/300 | Train: Loss=0.0295, F1=0.9968 | Val: Loss=0.6862, F1=0.9049 (Per-Sample)
Epoch 240/300 | Train: Loss=0.0099, F1=0.9968 | Val: Loss=0.4755, F1=0.9167 (Per-Sample)
Epoch 250/300 | Train: Loss=0.0109, F1=0.9981 | Val: Loss=0.4809, F1=0.9141 (Per-Sample)
Early stopping triggered after 255 epochs.
Best model restored from epoch 205 with val_f1 0.9529

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6277, F1=0.6972 | Val: Loss=0.4450, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1151, F1=0.8658 | Val: Loss=0.2276, F1=0.7872 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0427, F1=0.9459 | Val: Loss=0.2329, F1=0.8676 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0509, F1=0.9695 | Val: Loss=0.2373, F1=0.9026 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0549, F1=0.9848 | Val: Loss=0.2616, F1=0.9104 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0213, F1=0.9898 | Val: Loss=0.3408, F1=0.9460 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0264, F1=0.9931 | Val: Loss=0.3913, F1=0.9531 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0176, F1=0.9975 | Val: Loss=0.4616, F1=0.9373 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0349, F1=0.9949 | Val: Loss=0.3539, F1=0.9535 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0061, F1=0.9994 | Val: Loss=0.3907, F1=0.9293 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0009, F1=0.9987 | Val: Loss=0.4157, F1=0.9293 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0070, F1=0.9981 | Val: Loss=0.4364, F1=0.9293 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0254, F1=0.9975 | Val: Loss=0.3945, F1=0.9212 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.4296, F1=0.9293 (Per-Sample)
Early stopping triggered after 130 epochs.
Best model restored from epoch 80 with val_f1 0.9535

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5967, F1=0.6552 | Val: Loss=0.2473, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1266, F1=0.8409 | Val: Loss=0.1378, F1=0.8633 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0434, F1=0.9504 | Val: Loss=0.2032, F1=0.9159 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0343, F1=0.9692 | Val: Loss=0.2204, F1=0.9330 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0268, F1=0.9891 | Val: Loss=0.3165, F1=0.9330 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0301, F1=0.9905 | Val: Loss=0.3766, F1=0.9428 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0181, F1=0.9956 | Val: Loss=0.3441, F1=0.9440 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0047, F1=0.9962 | Val: Loss=0.4155, F1=0.9334 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0012, F1=0.9994 | Val: Loss=0.4886, F1=0.9275 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0146, F1=0.9962 | Val: Loss=0.5185, F1=0.9265 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0084, F1=0.9994 | Val: Loss=0.4386, F1=0.9241 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0302, F1=0.9969 | Val: Loss=0.5500, F1=0.9187 (Per-Sample)
Best trial: 21. Best value: -0.963:  88%|████████▊ | 44/50 [1:51:23<13:40, 136.76s/it]
Early stopping triggered after 111 epochs.
Best model restored from epoch 61 with val_f1 0.9602

Cross-validation score: 0.9529 ± 0.0046
  > Trial 43 Result: Mean F1 = 0.9529
[I 2025-11-10 15:32:59,794] Trial 43 finished with value: -0.9529076597854743 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 32, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 64, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.3, 'l2_lambda': 0.0, 'noise_std_dev': 0.1}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 44 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.01
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6630, F1=0.6775 | Val: Loss=0.4377, F1=0.7488 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0764, F1=0.9208 | Val: Loss=0.2013, F1=0.9347 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0952, F1=0.9458 | Val: Loss=0.1477, F1=0.9171 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0421, F1=0.9771 | Val: Loss=0.2218, F1=0.9341 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0333, F1=0.9801 | Val: Loss=0.2242, F1=0.9522 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0256, F1=0.9918 | Val: Loss=0.2191, F1=0.9494 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0288, F1=0.9905 | Val: Loss=0.3451, F1=0.9515 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0043, F1=0.9962 | Val: Loss=0.4951, F1=0.9258 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0375, F1=0.9931 | Val: Loss=0.2255, F1=0.9415 (Per-Sample)
Early stopping triggered after 84 epochs.
Best model restored from epoch 34 with val_f1 0.9613

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7102, F1=0.6890 | Val: Loss=0.3816, F1=0.7118 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0966, F1=0.9025 | Val: Loss=0.2977, F1=0.8695 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0388, F1=0.9696 | Val: Loss=0.4229, F1=0.9131 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0651, F1=0.9735 | Val: Loss=0.5109, F1=0.8897 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0364, F1=0.9905 | Val: Loss=0.7479, F1=0.8829 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0403, F1=0.9886 | Val: Loss=0.4514, F1=0.9127 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0234, F1=0.9930 | Val: Loss=0.6225, F1=0.9197 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0338, F1=0.9956 | Val: Loss=0.6112, F1=0.9289 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0074, F1=0.9962 | Val: Loss=0.6262, F1=0.9300 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0120, F1=0.9975 | Val: Loss=0.6178, F1=0.9014 (Per-Sample)
Early stopping triggered after 97 epochs.
Best model restored from epoch 47 with val_f1 0.9454

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6160, F1=0.7015 | Val: Loss=0.3976, F1=0.7073 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1115, F1=0.9115 | Val: Loss=0.1427, F1=0.8414 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0670, F1=0.9709 | Val: Loss=0.2438, F1=0.9146 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0263, F1=0.9892 | Val: Loss=0.4444, F1=0.9322 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0660, F1=0.9846 | Val: Loss=0.4967, F1=0.9080 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0256, F1=0.9924 | Val: Loss=0.3621, F1=0.9304 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0393, F1=0.9924 | Val: Loss=0.3796, F1=0.9379 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0106, F1=0.9949 | Val: Loss=0.5074, F1=0.8972 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0129, F1=0.9949 | Val: Loss=0.4529, F1=0.9146 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0215, F1=0.9962 | Val: Loss=0.4109, F1=0.9373 (Per-Sample)
Early stopping triggered after 94 epochs.
Best model restored from epoch 44 with val_f1 0.9539

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6481, F1=0.6890 | Val: Loss=0.4532, F1=0.6371 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0747, F1=0.9257 | Val: Loss=0.1946, F1=0.8520 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0595, F1=0.9668 | Val: Loss=0.2675, F1=0.9106 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0884, F1=0.9678 | Val: Loss=0.3745, F1=0.9216 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0306, F1=0.9918 | Val: Loss=0.3351, F1=0.9370 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0172, F1=0.9931 | Val: Loss=0.4675, F1=0.9370 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0145, F1=0.9956 | Val: Loss=0.4129, F1=0.9279 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0186, F1=0.9962 | Val: Loss=0.4055, F1=0.9369 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0049, F1=0.9987 | Val: Loss=0.4060, F1=0.9211 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0065, F1=0.9987 | Val: Loss=0.4653, F1=0.9129 (Per-Sample)
Early stopping triggered after 92 epochs.
Best model restored from epoch 42 with val_f1 0.9453

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6535, F1=0.6825 | Val: Loss=0.2394, F1=0.7928 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0629, F1=0.9412 | Val: Loss=0.1625, F1=0.9159 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0365, F1=0.9735 | Val: Loss=0.2242, F1=0.9253 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0321, F1=0.9820 | Val: Loss=0.2456, F1=0.9280 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0568, F1=0.9836 | Val: Loss=0.2343, F1=0.9460 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0249, F1=0.9937 | Val: Loss=0.2873, F1=0.9440 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0073, F1=0.9968 | Val: Loss=0.4786, F1=0.9308 (Per-Sample)
Best trial: 21. Best value: -0.963:  90%|█████████ | 45/50 [1:53:03<10:28, 125.67s/it]
Early stopping triggered after 67 epochs.
Best model restored from epoch 17 with val_f1 0.9613

Cross-validation score: 0.9534 ± 0.0071
  > Trial 44 Result: Mean F1 = 0.9534
[I 2025-11-10 15:34:39,612] Trial 44 finished with value: -0.9534430160592731 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.01, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 45 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 16
  > dropout_rate: 0.1
  > l2_lambda: 0.001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.8204, F1=0.6331 | Val: Loss=0.4011, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1084, F1=0.8509 | Val: Loss=0.0746, F1=0.9069 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0387, F1=0.9767 | Val: Loss=0.1218, F1=0.9344 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0440, F1=0.9834 | Val: Loss=0.3749, F1=0.9306 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0432, F1=0.9859 | Val: Loss=0.3396, F1=0.9442 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0136, F1=0.9930 | Val: Loss=0.3666, F1=0.9542 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0455, F1=0.9905 | Val: Loss=0.3567, F1=0.9450 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0195, F1=0.9969 | Val: Loss=0.3959, F1=0.9367 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0125, F1=0.9937 | Val: Loss=0.3712, F1=0.9456 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0057, F1=0.9975 | Val: Loss=0.4037, F1=0.9531 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0117, F1=0.9975 | Val: Loss=0.5562, F1=0.9383 (Per-Sample)
Early stopping triggered after 106 epochs.
Best model restored from epoch 56 with val_f1 0.9668

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7496, F1=0.6740 | Val: Loss=0.3871, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0894, F1=0.9267 | Val: Loss=0.2286, F1=0.9158 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0438, F1=0.9707 | Val: Loss=0.3620, F1=0.9213 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0266, F1=0.9852 | Val: Loss=0.5811, F1=0.8876 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0111, F1=0.9897 | Val: Loss=0.5200, F1=0.9307 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0074, F1=0.9949 | Val: Loss=0.5558, F1=0.9206 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0141, F1=0.9917 | Val: Loss=0.6266, F1=0.9139 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0038, F1=0.9981 | Val: Loss=0.5655, F1=0.9300 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0024, F1=0.9981 | Val: Loss=0.5262, F1=0.9296 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0034, F1=0.9968 | Val: Loss=0.5543, F1=0.9289 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0011, F1=0.9987 | Val: Loss=0.5808, F1=0.9382 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0145, F1=0.9956 | Val: Loss=0.6716, F1=0.9143 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0156, F1=0.9968 | Val: Loss=0.6181, F1=0.9009 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0035, F1=0.9975 | Val: Loss=0.5236, F1=0.9224 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0010, F1=0.9994 | Val: Loss=0.7886, F1=0.9133 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0161, F1=0.9968 | Val: Loss=0.6297, F1=0.9285 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0069, F1=0.9975 | Val: Loss=0.7578, F1=0.9285 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0303, F1=0.9918 | Val: Loss=0.7511, F1=0.9309 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0276, F1=0.9962 | Val: Loss=0.6461, F1=0.9088 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0009, F1=0.9987 | Val: Loss=0.7240, F1=0.9353 (Per-Sample)
Early stopping triggered after 193 epochs.
Best model restored from epoch 143 with val_f1 0.9461

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7244, F1=0.6864 | Val: Loss=0.3921, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0620, F1=0.9366 | Val: Loss=0.1818, F1=0.8516 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0411, F1=0.9717 | Val: Loss=0.2351, F1=0.9056 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0875, F1=0.9766 | Val: Loss=0.3690, F1=0.9168 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0185, F1=0.9917 | Val: Loss=0.3911, F1=0.9396 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0338, F1=0.9905 | Val: Loss=0.3654, F1=0.9251 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0118, F1=0.9950 | Val: Loss=0.3756, F1=0.9314 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0027, F1=0.9975 | Val: Loss=0.3690, F1=0.9388 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0026, F1=0.9981 | Val: Loss=0.3132, F1=0.9537 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0124, F1=0.9975 | Val: Loss=0.4930, F1=0.9234 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0019, F1=0.9975 | Val: Loss=0.4571, F1=0.9307 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0289, F1=0.9943 | Val: Loss=0.4210, F1=0.9383 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0204, F1=0.9962 | Val: Loss=0.3687, F1=0.9370 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0077, F1=0.9968 | Val: Loss=0.3553, F1=0.9376 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0130, F1=0.9987 | Val: Loss=0.4983, F1=0.9212 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0063, F1=0.9981 | Val: Loss=0.4180, F1=0.9371 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0074, F1=0.9968 | Val: Loss=0.3662, F1=0.9460 (Per-Sample)
Early stopping triggered after 166 epochs.
Best model restored from epoch 116 with val_f1 0.9611

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6743, F1=0.7175 | Val: Loss=0.4227, F1=0.6355 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0750, F1=0.9097 | Val: Loss=0.2426, F1=0.8159 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0484, F1=0.9735 | Val: Loss=0.3313, F1=0.9133 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0696, F1=0.9802 | Val: Loss=0.3085, F1=0.9200 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0383, F1=0.9905 | Val: Loss=0.3481, F1=0.9300 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0355, F1=0.9931 | Val: Loss=0.3827, F1=0.9217 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0148, F1=0.9943 | Val: Loss=0.4548, F1=0.9227 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0020, F1=0.9981 | Val: Loss=0.4181, F1=0.9300 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0009, F1=0.9994 | Val: Loss=0.4853, F1=0.9384 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0003, F1=0.9994 | Val: Loss=0.4915, F1=0.9222 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0150, F1=0.9956 | Val: Loss=0.4998, F1=0.9293 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0011, F1=0.9994 | Val: Loss=0.5666, F1=0.9140 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0023, F1=0.9981 | Val: Loss=0.6041, F1=0.9296 (Per-Sample)
Early stopping triggered after 124 epochs.
Best model restored from epoch 74 with val_f1 0.9463

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7558, F1=0.6654 | Val: Loss=0.2625, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1033, F1=0.8801 | Val: Loss=0.1427, F1=0.8692 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0389, F1=0.9692 | Val: Loss=0.1777, F1=0.9261 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0438, F1=0.9794 | Val: Loss=0.3568, F1=0.9312 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0412, F1=0.9879 | Val: Loss=0.5217, F1=0.9166 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0166, F1=0.9898 | Val: Loss=0.4174, F1=0.9307 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0136, F1=0.9962 | Val: Loss=0.5636, F1=0.9081 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0041, F1=0.9955 | Val: Loss=0.4226, F1=0.9329 (Per-Sample)
Best trial: 21. Best value: -0.963:  92%|█████████▏| 46/50 [1:55:35<08:54, 133.64s/it]
Early stopping triggered after 78 epochs.
Best model restored from epoch 28 with val_f1 0.9474

Cross-validation score: 0.9535 ± 0.0087
  > Trial 45 Result: Mean F1 = 0.9535
[I 2025-11-10 15:37:11,843] Trial 45 finished with value: -0.9535175028198827 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 16, 'dropout_rate': 0.1, 'l2_lambda': 0.001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 46 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 40
  > weight_ce_intensity: 0.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Best trial: 21. Best value: -0.963:  94%|█████████▍| 47/50 [1:55:35<04:41, 93.71s/it] 
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.0): [0. 0. 0.]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
NaN loss at batch 0
NaN loss at batch 1
NaN loss at batch 2
NaN loss at batch 3
NaN loss at batch 4
NaN loss at batch 5
NaN loss at batch 6
NaN loss at batch 7
NaN loss at batch 8
NaN loss at batch 9
NaN loss at batch 10
NaN loss at batch 11
NaN loss at batch 12
NaN loss at batch 13
NaN loss at batch 14
NaN loss at batch 15
NaN loss at batch 16
NaN loss at batch 17
NaN loss at batch 18
NaN loss at batch 19
NaN loss at batch 20
NaN loss at batch 21
NaN loss at batch 22
NaN loss at batch 23
NaN loss at batch 24
NaN loss at batch 25
NaN loss at batch 26
NaN loss at batch 27
NaN loss at batch 28
NaN loss at batch 29
NaN loss at batch 30
NaN loss at batch 31
NaN loss at batch 32
NaN loss at batch 33
NaN loss at batch 34
NaN loss at batch 35
NaN loss at batch 36
NaN loss at batch 37
NaN loss at batch 38
NaN loss at batch 39
NaN loss at batch 40
NaN loss at batch 41
NaN loss at batch 42
NaN loss at batch 43
NaN loss at batch 44
NaN loss at batch 45
NaN loss at batch 46
NaN loss at batch 47
NaN loss at batch 48
NaN loss at batch 49
--- ERROR in Trial 46 ---
Configuration: {'batch_size': 32, 'cross_entropy_weighting': True, 'l1_lambda': 0.0, 'learning_rate': 0.001, 'window_size': 120, 'stride': 40, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.1}
Error: need at least one array to concatenate
  > Trial 46 Result: F1 = 0.0 (due to error)
[I 2025-11-10 15:37:12,367] Trial 46 finished with value: 0.0 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 40, 'weight_ce_intensity': 0.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.1}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 47 ---
  > learning_rate: 0.001
  > window_size: 40
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 64
  > rnn_type: GRU
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.0
  > l2_lambda: 0.001
  > noise_std_dev: 0.2

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6308, F1=0.6859 | Val: Loss=0.3950, F1=0.7488 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1161, F1=0.8998 | Val: Loss=0.1553, F1=0.8938 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0573, F1=0.9604 | Val: Loss=0.1864, F1=0.9101 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0377, F1=0.9828 | Val: Loss=0.3198, F1=0.9270 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0699, F1=0.9790 | Val: Loss=0.2876, F1=0.9368 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0793, F1=0.9880 | Val: Loss=0.3816, F1=0.9360 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0247, F1=0.9930 | Val: Loss=0.3589, F1=0.9211 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0072, F1=0.9981 | Val: Loss=0.4256, F1=0.9442 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0038, F1=0.9968 | Val: Loss=0.3917, F1=0.9232 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0107, F1=0.9994 | Val: Loss=0.4371, F1=0.9367 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0067, F1=0.9987 | Val: Loss=0.6188, F1=0.9213 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0225, F1=0.9968 | Val: Loss=0.4362, F1=0.9433 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0054, F1=0.9981 | Val: Loss=0.4378, F1=0.9359 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0077, F1=0.9981 | Val: Loss=0.4114, F1=0.9327 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0027, F1=0.9981 | Val: Loss=0.3752, F1=0.9282 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0062, F1=0.9987 | Val: Loss=0.5442, F1=0.9396 (Per-Sample)
Early stopping triggered after 158 epochs.
Best model restored from epoch 108 with val_f1 0.9542

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6290, F1=0.6715 | Val: Loss=0.3866, F1=0.6807 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0955, F1=0.8862 | Val: Loss=0.2400, F1=0.8277 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0458, F1=0.9500 | Val: Loss=0.2254, F1=0.9003 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0319, F1=0.9709 | Val: Loss=0.2764, F1=0.9113 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0381, F1=0.9764 | Val: Loss=0.3603, F1=0.8871 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0147, F1=0.9859 | Val: Loss=0.4129, F1=0.9103 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0264, F1=0.9911 | Val: Loss=0.3740, F1=0.9286 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0145, F1=0.9962 | Val: Loss=0.5113, F1=0.9207 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0052, F1=0.9968 | Val: Loss=0.5232, F1=0.9057 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0206, F1=0.9975 | Val: Loss=0.5192, F1=0.9182 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0027, F1=0.9981 | Val: Loss=0.5546, F1=0.9079 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0187, F1=0.9968 | Val: Loss=0.5686, F1=0.9029 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0026, F1=0.9987 | Val: Loss=0.5382, F1=0.9206 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0091, F1=0.9987 | Val: Loss=0.5144, F1=0.9268 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5760, F1=0.9203 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.5147, F1=0.9368 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0003, F1=1.0000 | Val: Loss=0.5592, F1=0.9201 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0144, F1=0.9981 | Val: Loss=0.5570, F1=0.9196 (Per-Sample)
Early stopping triggered after 174 epochs.
Best model restored from epoch 124 with val_f1 0.9443

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6408, F1=0.7019 | Val: Loss=0.4544, F1=0.6658 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1023, F1=0.8828 | Val: Loss=0.1858, F1=0.8260 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0386, F1=0.9547 | Val: Loss=0.1802, F1=0.8871 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0611, F1=0.9783 | Val: Loss=0.3104, F1=0.9228 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0229, F1=0.9859 | Val: Loss=0.3615, F1=0.9228 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0119, F1=0.9962 | Val: Loss=0.4688, F1=0.9462 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0111, F1=0.9962 | Val: Loss=0.5935, F1=0.9237 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0279, F1=0.9931 | Val: Loss=0.6351, F1=0.9229 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0158, F1=0.9950 | Val: Loss=0.4589, F1=0.9231 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0152, F1=0.9936 | Val: Loss=0.4702, F1=0.9314 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0181, F1=0.9969 | Val: Loss=0.5491, F1=0.9309 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0018, F1=0.9994 | Val: Loss=0.5364, F1=0.9304 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0048, F1=0.9987 | Val: Loss=0.5176, F1=0.9458 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0093, F1=0.9962 | Val: Loss=0.4757, F1=0.9375 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0032, F1=0.9981 | Val: Loss=0.5699, F1=0.9222 (Per-Sample)
Early stopping triggered after 141 epochs.
Best model restored from epoch 91 with val_f1 0.9541

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5852, F1=0.6972 | Val: Loss=0.4521, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0905, F1=0.8805 | Val: Loss=0.2398, F1=0.7991 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0531, F1=0.9543 | Val: Loss=0.2444, F1=0.8660 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0331, F1=0.9765 | Val: Loss=0.3811, F1=0.8813 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0492, F1=0.9816 | Val: Loss=0.4217, F1=0.9232 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0245, F1=0.9905 | Val: Loss=0.4278, F1=0.9226 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0104, F1=0.9969 | Val: Loss=0.4101, F1=0.9301 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0153, F1=0.9943 | Val: Loss=0.4923, F1=0.9370 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0113, F1=0.9949 | Val: Loss=0.4377, F1=0.9287 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0073, F1=0.9969 | Val: Loss=0.5667, F1=0.9296 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0139, F1=0.9981 | Val: Loss=0.4233, F1=0.9464 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0063, F1=0.9987 | Val: Loss=0.5764, F1=0.9383 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0132, F1=0.9962 | Val: Loss=0.5044, F1=0.9370 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0003, F1=0.9994 | Val: Loss=0.5236, F1=0.9144 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0046, F1=0.9987 | Val: Loss=0.4228, F1=0.9375 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0052, F1=0.9994 | Val: Loss=0.5556, F1=0.9140 (Per-Sample)
Early stopping triggered after 150 epochs.
Best model restored from epoch 100 with val_f1 0.9464

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.5758, F1=0.6725 | Val: Loss=0.2504, F1=0.7785 (Per-Sample)
Epoch  10/300 | Train: Loss=0.1086, F1=0.8684 | Val: Loss=0.1427, F1=0.8791 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0757, F1=0.9463 | Val: Loss=0.1895, F1=0.9159 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0433, F1=0.9748 | Val: Loss=0.1701, F1=0.9361 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0164, F1=0.9866 | Val: Loss=0.2766, F1=0.9428 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0204, F1=0.9943 | Val: Loss=0.4079, F1=0.9199 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0220, F1=0.9956 | Val: Loss=0.4833, F1=0.9308 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0160, F1=0.9968 | Val: Loss=0.4455, F1=0.9375 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0096, F1=0.9975 | Val: Loss=0.4206, F1=0.9530 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0139, F1=0.9987 | Val: Loss=0.5025, F1=0.9346 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0192, F1=0.9987 | Val: Loss=0.5840, F1=0.9226 (Per-Sample)
Best trial: 21. Best value: -0.963:  96%|█████████▌| 48/50 [1:58:16<03:47, 113.91s/it]
Early stopping triggered after 105 epochs.
Best model restored from epoch 55 with val_f1 0.9537

Cross-validation score: 0.9506 ± 0.0043
  > Trial 47 Result: Mean F1 = 0.9506
[I 2025-11-10 15:39:53,422] Trial 47 finished with value: -0.9505525568592 and parameters: {'learning_rate': 0.001, 'window_size': 40, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 64, 'rnn_type': 'GRU', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.0, 'l2_lambda': 0.001, 'noise_std_dev': 0.2}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 48 ---
  > learning_rate: 0.001
  > window_size: 80
  > stride: 40
  > weight_ce_intensity: 2.0
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 128
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 32
  > pain_branch_bidirectional: False
  > static_hidden_size: 32
  > dropout_rate: 0.5
  > l2_lambda: 0.0001
  > noise_std_dev: 0.0

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.12719176 0.73643195 0.13637629]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6479, F1=0.6755 | Val: Loss=0.3807, F1=0.7327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0963, F1=0.9111 | Val: Loss=0.1292, F1=0.9038 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0575, F1=0.9709 | Val: Loss=0.2247, F1=0.9322 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0605, F1=0.9775 | Val: Loss=0.2025, F1=0.9524 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0398, F1=0.9860 | Val: Loss=0.2220, F1=0.9494 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0214, F1=0.9950 | Val: Loss=0.2739, F1=0.9515 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0422, F1=0.9918 | Val: Loss=0.3514, F1=0.9433 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0135, F1=0.9962 | Val: Loss=0.5645, F1=0.9367 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0162, F1=0.9969 | Val: Loss=0.3626, F1=0.9415 (Per-Sample)
Early stopping triggered after 80 epochs.
Best model restored from epoch 30 with val_f1 0.9524

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.14788678 0.72332592 0.1287873 ]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6707, F1=0.6763 | Val: Loss=0.3816, F1=0.7152 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0823, F1=0.9211 | Val: Loss=0.1848, F1=0.8786 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0323, F1=0.9739 | Val: Loss=0.3099, F1=0.9360 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0500, F1=0.9766 | Val: Loss=0.3940, F1=0.9370 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0347, F1=0.9885 | Val: Loss=0.7249, F1=0.9080 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0432, F1=0.9886 | Val: Loss=0.5759, F1=0.9136 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0154, F1=0.9962 | Val: Loss=0.5847, F1=0.9207 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0056, F1=0.9981 | Val: Loss=0.5229, F1=0.9375 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0104, F1=0.9975 | Val: Loss=0.6092, F1=0.9289 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0083, F1=0.9975 | Val: Loss=0.5667, F1=0.9205 (Per-Sample)
Early stopping triggered after 95 epochs.
Best model restored from epoch 45 with val_f1 0.9445

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19813392 0.68002195 0.12184413]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6390, F1=0.7056 | Val: Loss=0.4195, F1=0.7079 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0833, F1=0.9369 | Val: Loss=0.1800, F1=0.8616 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0580, F1=0.9604 | Val: Loss=0.1739, F1=0.9137 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0969, F1=0.9733 | Val: Loss=0.3420, F1=0.9137 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0440, F1=0.9829 | Val: Loss=0.4647, F1=0.9194 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0375, F1=0.9931 | Val: Loss=0.3497, F1=0.9383 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0163, F1=0.9975 | Val: Loss=0.3629, F1=0.9389 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0256, F1=0.9943 | Val: Loss=0.4591, F1=0.9215 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0183, F1=0.9981 | Val: Loss=0.4694, F1=0.9230 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0518, F1=0.9855 | Val: Loss=0.3392, F1=0.9458 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0190, F1=0.9968 | Val: Loss=0.4142, F1=0.9300 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0075, F1=0.9981 | Val: Loss=0.3425, F1=0.9304 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3466, F1=0.9315 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0025, F1=0.9994 | Val: Loss=0.3579, F1=0.9474 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0064, F1=0.9968 | Val: Loss=0.3340, F1=0.9465 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0022, F1=0.9987 | Val: Loss=0.3646, F1=0.9300 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.3840, F1=0.9384 (Per-Sample)
Epoch 170/300 | Train: Loss=0.0057, F1=0.9987 | Val: Loss=0.4024, F1=0.9396 (Per-Sample)
Epoch 180/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3993, F1=0.9450 (Per-Sample)
Epoch 190/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.4868, F1=0.9315 (Per-Sample)
Epoch 200/300 | Train: Loss=0.0006, F1=0.9987 | Val: Loss=0.5274, F1=0.9234 (Per-Sample)
Epoch 210/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3955, F1=0.9315 (Per-Sample)
Early stopping triggered after 215 epochs.
Best model restored from epoch 165 with val_f1 0.9618

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19903923 0.68278623 0.11817454]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6689, F1=0.6874 | Val: Loss=0.4815, F1=0.6371 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0866, F1=0.9197 | Val: Loss=0.1719, F1=0.8894 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0596, F1=0.9725 | Val: Loss=0.2901, F1=0.9217 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0561, F1=0.9861 | Val: Loss=0.2213, F1=0.9373 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0568, F1=0.9867 | Val: Loss=0.4877, F1=0.9302 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0051, F1=0.9962 | Val: Loss=0.4119, F1=0.9217 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0145, F1=0.9962 | Val: Loss=0.4121, F1=0.9227 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0096, F1=0.9956 | Val: Loss=0.3861, F1=0.9383 (Per-Sample)
Early stopping triggered after 79 epochs.
Best model restored from epoch 29 with val_f1 0.9460

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 2.0): [1.19333479 0.66943171 0.1372335 ]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6871, F1=0.6518 | Val: Loss=0.2556, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0945, F1=0.9185 | Val: Loss=0.1281, F1=0.9227 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0612, F1=0.9703 | Val: Loss=0.1448, F1=0.9412 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0559, F1=0.9720 | Val: Loss=0.2350, F1=0.9369 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0532, F1=0.9860 | Val: Loss=0.3018, F1=0.9458 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0275, F1=0.9924 | Val: Loss=0.2920, F1=0.9474 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0241, F1=0.9949 | Val: Loss=0.4479, F1=0.9399 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0053, F1=0.9962 | Val: Loss=0.4852, F1=0.9294 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0109, F1=0.9975 | Val: Loss=0.4929, F1=0.9390 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0063, F1=0.9975 | Val: Loss=0.4452, F1=0.9206 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0008, F1=0.9987 | Val: Loss=0.5410, F1=0.9459 (Per-Sample)
Best trial: 21. Best value: -0.963:  98%|█████████▊| 49/50 [2:00:26<01:58, 118.59s/it]
Early stopping triggered after 102 epochs.
Best model restored from epoch 52 with val_f1 0.9617

Cross-validation score: 0.9533 ± 0.0074
  > Trial 48 Result: Mean F1 = 0.9533
[I 2025-11-10 15:42:02,944] Trial 48 finished with value: -0.9532610208000122 and parameters: {'learning_rate': 0.001, 'window_size': 80, 'stride': 40, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 32, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}. Best is trial 21 with value: -0.9629996290867942.

--- Starting Trial 49 ---
  > learning_rate: 0.001
  > window_size: 120
  > stride: 40
  > weight_ce_intensity: 0.6
  > label_smoothing_epsilon: 0.0
  > hidden_layers: 1
  > hidden_size: 32
  > rnn_type: LSTM
  > bidirectional: True
  > pain_branch_type: GRU
  > pain_num_layers: 1
  > pain_hidden_size: 16
  > pain_branch_bidirectional: False
  > static_hidden_size: 64
  > dropout_rate: 0.3
  > l2_lambda: 0.0
  > noise_std_dev: 0.1

--- Fold 1/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.33815753 0.22092959 0.04091289]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7215, F1=0.6534 | Val: Loss=0.4028, F1=0.7153 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0858, F1=0.8570 | Val: Loss=0.1068, F1=0.8959 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0608, F1=0.9719 | Val: Loss=0.1691, F1=0.9288 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0426, F1=0.9755 | Val: Loss=0.1576, F1=0.9542 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0277, F1=0.9821 | Val: Loss=0.2748, F1=0.9354 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0411, F1=0.9767 | Val: Loss=0.3102, F1=0.9388 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0235, F1=0.9892 | Val: Loss=0.1846, F1=0.9689 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0253, F1=0.9911 | Val: Loss=0.3189, F1=0.9607 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0126, F1=0.9981 | Val: Loss=0.3985, F1=0.9523 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0096, F1=0.9968 | Val: Loss=0.4417, F1=0.9368 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0249, F1=0.9968 | Val: Loss=0.3742, F1=0.9352 (Per-Sample)
Early stopping triggered after 106 epochs.
Best model restored from epoch 56 with val_f1 0.9689

--- Fold 2/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.34436604 0.21699778 0.03863619]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6320, F1=0.6762 | Val: Loss=0.4095, F1=0.6634 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0816, F1=0.9178 | Val: Loss=0.2576, F1=0.8716 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0385, F1=0.9699 | Val: Loss=0.2598, F1=0.9048 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0370, F1=0.9800 | Val: Loss=0.3229, F1=0.9286 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0285, F1=0.9860 | Val: Loss=0.4264, F1=0.9141 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0559, F1=0.9789 | Val: Loss=0.3445, F1=0.9118 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0122, F1=0.9930 | Val: Loss=0.4675, F1=0.9119 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0161, F1=0.9943 | Val: Loss=0.6715, F1=0.9115 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0129, F1=0.9950 | Val: Loss=0.5070, F1=0.9359 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0096, F1=0.9956 | Val: Loss=0.5487, F1=0.9307 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0137, F1=0.9956 | Val: Loss=0.6267, F1=0.9203 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0192, F1=0.9962 | Val: Loss=0.5218, F1=0.9374 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0193, F1=0.9937 | Val: Loss=0.5436, F1=0.9454 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0122, F1=0.9956 | Val: Loss=0.6358, F1=0.9286 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0079, F1=0.9969 | Val: Loss=0.7152, F1=0.9212 (Per-Sample)
Epoch 150/300 | Train: Loss=0.0196, F1=0.9962 | Val: Loss=0.6741, F1=0.9289 (Per-Sample)
Epoch 160/300 | Train: Loss=0.0096, F1=0.9975 | Val: Loss=0.6348, F1=0.9202 (Per-Sample)
Early stopping triggered after 167 epochs.
Best model restored from epoch 117 with val_f1 0.9461

--- Fold 3/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35944018 0.20400659 0.03655324]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7384, F1=0.6713 | Val: Loss=0.4367, F1=0.6327 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0651, F1=0.9351 | Val: Loss=0.1621, F1=0.8565 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0419, F1=0.9672 | Val: Loss=0.1753, F1=0.9112 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0422, F1=0.9739 | Val: Loss=0.2559, F1=0.9318 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0342, F1=0.9866 | Val: Loss=0.3592, F1=0.9085 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0266, F1=0.9872 | Val: Loss=0.3397, F1=0.9304 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0215, F1=0.9904 | Val: Loss=0.3611, F1=0.9180 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0117, F1=0.9949 | Val: Loss=0.3954, F1=0.9058 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0120, F1=0.9975 | Val: Loss=0.3369, F1=0.9202 (Per-Sample)
Early stopping triggered after 80 epochs.
Best model restored from epoch 30 with val_f1 0.9318

--- Fold 4/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35971177 0.20483587 0.03545236]
  Initializing HYBRID model for CV.
  Training windows: 1587
  Validation windows: 396
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.7385, F1=0.6571 | Val: Loss=0.4312, F1=0.6024 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0548, F1=0.8941 | Val: Loss=0.1959, F1=0.7922 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0307, F1=0.9696 | Val: Loss=0.2872, F1=0.8870 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0387, F1=0.9812 | Val: Loss=0.4955, F1=0.9060 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0281, F1=0.9794 | Val: Loss=0.4551, F1=0.9222 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0247, F1=0.9905 | Val: Loss=0.3923, F1=0.9198 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0331, F1=0.9893 | Val: Loss=0.5591, F1=0.9002 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0358, F1=0.9918 | Val: Loss=0.3902, F1=0.9226 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0289, F1=0.9962 | Val: Loss=0.4481, F1=0.9304 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0083, F1=0.9975 | Val: Loss=0.4156, F1=0.9139 (Per-Sample)
Epoch 100/300 | Train: Loss=0.0058, F1=0.9987 | Val: Loss=0.4892, F1=0.9221 (Per-Sample)
Epoch 110/300 | Train: Loss=0.0012, F1=0.9987 | Val: Loss=0.4429, F1=0.9139 (Per-Sample)
Epoch 120/300 | Train: Loss=0.0046, F1=0.9987 | Val: Loss=0.4488, F1=0.9059 (Per-Sample)
Epoch 130/300 | Train: Loss=0.0060, F1=0.9981 | Val: Loss=0.4579, F1=0.9059 (Per-Sample)
Epoch 140/300 | Train: Loss=0.0058, F1=0.9981 | Val: Loss=0.4978, F1=0.9214 (Per-Sample)
Early stopping triggered after 142 epochs.
Best model restored from epoch 92 with val_f1 0.9384

--- Fold 5/5 ---
Building fixed sequences with window=80, stride=40...
Building fixed sequences with window=80, stride=40...
  Dynamic Loss Weights (Intensity 0.6): [0.35800044 0.20082951 0.04117005]
  Initializing HYBRID model for CV.
  Training windows: 1584
  Validation windows: 399
Training 300 epochs...
Epoch   1/300 | Train: Loss=0.6225, F1=0.6528 | Val: Loss=0.2893, F1=0.7593 (Per-Sample)
Epoch  10/300 | Train: Loss=0.0902, F1=0.8847 | Val: Loss=0.1382, F1=0.8791 (Per-Sample)
Epoch  20/300 | Train: Loss=0.0372, F1=0.9739 | Val: Loss=0.1907, F1=0.9226 (Per-Sample)
Epoch  30/300 | Train: Loss=0.0603, F1=0.9848 | Val: Loss=0.2823, F1=0.9301 (Per-Sample)
Epoch  40/300 | Train: Loss=0.0245, F1=0.9880 | Val: Loss=0.3394, F1=0.9382 (Per-Sample)
Epoch  50/300 | Train: Loss=0.0334, F1=0.9835 | Val: Loss=0.3226, F1=0.9545 (Per-Sample)
Epoch  60/300 | Train: Loss=0.0173, F1=0.9911 | Val: Loss=0.4311, F1=0.9324 (Per-Sample)
Epoch  70/300 | Train: Loss=0.0124, F1=0.9975 | Val: Loss=0.5151, F1=0.9330 (Per-Sample)
Epoch  80/300 | Train: Loss=0.0086, F1=0.9962 | Val: Loss=0.5851, F1=0.9173 (Per-Sample)
Epoch  90/300 | Train: Loss=0.0256, F1=0.9962 | Val: Loss=0.5875, F1=0.9323 (Per-Sample)
Best trial: 21. Best value: -0.963: 100%|██████████| 50/50 [2:02:41<00:00, 147.23s/it]
Epoch 100/300 | Train: Loss=0.0030, F1=0.9962 | Val: Loss=0.5800, F1=0.9323 (Per-Sample)
Early stopping triggered after 100 epochs.
Best model restored from epoch 50 with val_f1 0.9545

Cross-validation score: 0.9480 ± 0.0129
  > Trial 49 Result: Mean F1 = 0.9480
[I 2025-11-10 15:44:18,024] Trial 49 finished with value: -0.9479526509183476 and parameters: {'learning_rate': 0.001, 'window_size': 120, 'stride': 40, 'weight_ce_intensity': 0.6, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 1, 'hidden_size': 32, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 1, 'pain_hidden_size': 16, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.3, 'l2_lambda': 0.0, 'noise_std_dev': 0.1}. Best is trial 21 with value: -0.9629996290867942.

--- Bayesian Optimization Complete ---
Best Score (Mean F1): 0.9630
Best Config:
{'learning_rate': 0.001, 'window_size': 120, 'stride': 20, 'weight_ce_intensity': 2.0, 'label_smoothing_epsilon': 0.0, 'hidden_layers': 3, 'hidden_size': 128, 'rnn_type': 'LSTM', 'bidirectional': True, 'pain_branch_type': 'GRU', 'pain_num_layers': 2, 'pain_hidden_size': 32, 'pain_branch_bidirectional': False, 'static_hidden_size': 64, 'dropout_rate': 0.5, 'l2_lambda': 0.0001, 'noise_std_dev': 0.0}
Total Runtime: 2:02:41.562058
